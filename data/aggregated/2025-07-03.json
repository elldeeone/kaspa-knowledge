{
  "date": "2025-07-03",
  "generated_at": "2025-07-03T05:37:48.073632",
  "sources": {
    "medium_articles": [
      {
        "title": "In which we are all faceless until we have faces (Part I)*",
        "link": "https://hashdag.medium.com/in-which-we-are-all-faceless-until-we-have-faces-part-i-5f100e0555a4?source=rss-400d0e2aab3b------2",
        "summary": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XQ5qFhp8zBsSlnRbl68xAw.jpeg\" /><figcaption>PROOF OF DWORK</figcaption></figure><h3><strong>Ur of the Chaldees</strong></h3><p>I am the sole culprit behind the recent sagas surrounding “our” X account and the name of Kaspa’s atomic unit. It was my own doing; regrettably, I have no accomplices to share the credit with. I genuinely empathize with fellow Kaspers who felt confused, distressed, or deflated by my actions. Though I cannot offer a sincere apology, I can and should offer more context.</p><p>The reader should be warned in advance that my argument will be constructed rather ouroboros-ly, in a manner which makes it illogical to refute. The more the reader disagrees with it the more they are forced to recognize the validity and relevance of the argument. I learnt this little trick of constructing self-enforcing arguments from one old Chaldean family from the 19th century BC, whose story is not entirely irrelevant to ours.</p><p>Terah was a Mesopotamian idol manufacturer. The <em>Midrash of Genesis</em> (tales by Jewish sages) describes how one morning Terah left his son Abraham to watch the idol shop. Abraham seized the moment and took a stick and smashed all the idols, except the largest one. He then placed the stick in the hand of that remaining idol. When Terah returned and saw the destruction, he demanded, “What happened here?” Abraham replied, “The idols got into a quarrel, and the big one decided to smash the rest.” Terah exclaimed, “Do you think I’m a fool? These idols have no knowledge or power!” And Abraham replied, “Then let your ears hear what your mouth is saying.”</p><p>Some odd 48 hours ago someone left the largest idol alone with access to Kaspa’s official X account. I consulted no one before changing the account from “Kaspa” to “Kaspa (Unofficial)”, nor did I prepare any crisis management plan. Though, perhaps like Terah’s son, I have waited for the right moment for a long time.</p><h3>The Bourne Identity</h3><p>To the extent that this saga turns into an identity crisis for many Kaspa fans, it is a long-overdue one.</p><p>There is a choice to be made: Are we here for the cozy feeling of a cohesive community with a welcoming center of the hub, or are we committed to penetrating the crypto — thence the broader — market with a p2p electronic cash system and a trustworthy store of value?</p><p>In the past 18 months, the marketcap of bitcoin surpassed that of silver, twice. What would it take for us to be able to think of KAS in similar terms?</p><p>The technology of Kaspa is superior to Bitcoin’s many times over, largely due to elite architecture and execution by Crescendo devs. But why is Kaspa Crescendo celebrated? Because it achieves a 6000x improvement over Satoshi’s protocol whilst remaining in the confines of the same trust model — a center-less structure which delegates the control over the network to an anonymous set of miners. A set which, to be explicit, comprises egoistic entities that should never be trusted, and yet whose competing incentives align with that of securing the network.</p><p>There are dozens of crypto projects which offer much richer tech stacks, production live apps, and marketing budgets than kaspa or bitcoin. Kaspa’s value can be unlocked and recognized only only only if Kaspa is-looks-feels-smells culturally open-source, practically center-less, constitutionally principled.</p><p>The existence of an official social account simply fails the gut check.</p><h3>A public mask of the project</h3><p>The main social account, practically an official one, with announcements and press releases, is convenient and facilitative indeed, and could be argued to have been instrumental early on. It is nonetheless a headquarters of the brand, and those who populate the headquarters are inadvertently co-opting the identity of the network. The consolidation of influence becomes inevitable, whether or not the people controlling the socials would consciously choose such an outcome.</p><p>Despite the dogmatic sound of the argument, it is no less a pragmatic one; I became conscious of it through observation more than through first-principles thinking. Specifically, I observed that for a team building a product or layer atop Kaspa, the best approach at community engagement and capture is through the conviction, if not friendship, of the maintainers of the social accounts. Alignment with them is an asset, misalignment — a liability. Needless to spell out the consequences: Even if the main accounts are governed with virtue and kept far from corrupt, a hierarchy forms — one which is easily spotted by newcomers. Visitors and newcomers are the first to recognize the whos and whats of the in-group, and through their experiences I became increasingly aware how our community is increasingly losing its flat hierarchy, its mission, and its chances at fulfilling it.</p><p>To be maximally unambiguous about this — the individuals who maintained so far the socials would never opt to gain power or deliberate influence. When interacting with them, I was inspired by their firm belief and selfless devotion to Kaspa’s mission; they truly love Kaspa, resembling perhaps the love and devotion of the mother-GHOST, from CS Lewis’ The Great Divorce, to the spirit of her son Michael.</p><p>In particular, they have never undermined the authority of Kaspa Core; had they attempted to do so, that would be a breath of fresh air. The issue we are highlighting here, therefore, has to do with representation more than with alignment. Many Kaspa X’ers hold paradigms, mental models, or style that I — and other members of Core — object to. This is perfectly normal and nothing needs to be done about it. We are not, however, seen as represented by any of these accounts. In contrast, with the official X account, the interpretation is different: It is the most followed Kaspa account, it is named plainly “Kaspa”, it offers press releases and formally-styled updates, and it actively nurtures the brand. We, Kaspa members, and especially Kaspa Core, are practically represented by it; it is a forced mask on the face of the project and of Core, and no matter how good the maintainers are at their jobs, it is still a mask.</p><p>I trust the reader to not be confused by my admission that these individuals understand brand-building at a deeply higher level than I do, and that they have been undoubtedly instrumental to the growth of Kaspa community and brand. If someone asked me in the future regarding the expertise of these individuals, I would unquestionably recommend them and vouch for their brand-building-and-nurturing expertise, my lack of experience in marketing being the only reservation to a warm recommendation and a big yes.</p><p>My argument notwithstanding.</p><h3><strong>Second things first</strong></h3><p>Reflecting on the media of exchange of ancient man, and of modern primitive societies, one can argue there exists a reverse correlation between the degree of cohesiveness of a society — which can be achieved at scale only through rigid hierarchies — and the moneyness qualities of its MoE. A strictly hierarchical society, the Andean Inca for instance, was able to thrive with barter and social obligations alone; their hierarchical structure replaced many functionalities that money serves in modern civilization, such as the allocation of resources and the flow of information. More heterogeneous civilizations had to adopt more scalable MoE — cowrie, to name one notable example — with barter frequently still coexisting, as in the city-states of Yoruba. Further along the spectrum (chronology aside), the merchant-oriented Mesopotamian civilizations utilized silver for payment and accounting — a yet more scalable and practical medium of exchange for urban, complex trade.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/410/1*LX88o4dMtpgVLxwvtofjtQ.jpeg\" /><figcaption>By Published by Guillaume Rouille(1518?-1589) — “Promptuarii Iconum Insigniorum “, Public Domain, <a href=\"https://commons.wikimedia.org/w/index.php?curid=8804631\">https://commons.wikimedia.org/w/index.php?curid=8804631</a></figcaption></figure><p>Is it a good thing then to describe Kaspa community as united and conflict-free? This community is one of the friendliest and welcoming communities in crypto. Compared to Bitcoin 2014–5 it is virtually toxic-free. The atmosphere, at least that projected by the brand’s headquarters, is almost too positive and worry-free.¹</p><p>Yet, getting too attached to the friendly vibes, or over-priding ourselves on that, would be “putting second things first”, to borrow CS Lewis’ phrase. We should prioritize debates, rebuts, arguments, disputes, concerns. There are serious challenges ahead of Kaspa — security budget, decentralized governance, monolithic vs modular architecture, HF policies, open-source funding, degen vs elitism, L2 fragmentation — and if we are not fighting over those issues we are probably doing something wrong. Probably, we are delusional about their seriousness, or worse yet delusional about Terah’s ability to solve them all through the genius of his idols.</p><p>First things first, for Kaspa to become a money, a globally trusted asset in the order of today’s trillions of US dollars, the community’s culture must be conducive to becoming the cradle of money. Some degree of tension and contention must linger in the air for the moneyness properties of a society’s collectible to emerge. Our rivals, our competitors and adversaries, must feel comfortable storing their wealth herein. High net worth individuals who distrust this author — hopefully the club has grown larger in the past 48 hours — must feel Kaspa is nonetheless an absolute safe haven for their wealth. It must pass the eye test of institutional investors. Kaspa must Bitcoinize.</p><h3><strong>Ethereum, North Korea, Vitalik</strong></h3><p>Ethereum is the most important crypto platform alongside Bitcoin.² No-one in Ethereum epicenter was seriously considering a rollback after the ByBit hack, the psyops notwithstanding. Ethereum is orders-of-magnitude more mature and distributed than it was in the DAO hack days. Nonetheless, the ByBit hack can be used as a thought experiment to reflect on hierarchies and their implication to moneyness.</p><p>In spring 2016 I was invited by Andrew Miller to Cornell, where he and Elaine and Emin Gun organized an Ethereum bootcamp. Around that time, Emin voiced the DAO vulnerabilities (eg <a href=\"https://x.com/el33th4xor/status/736266834073276416\">https://x.com/el33th4xor/status/736266834073276416</a>), and a few days later the vulnerabilities were exploited by an anonymous “code-is-law” hacker. It so happened that the bootcamp took place during the intervention — technically not a rollback but a manual HF to induce an irregular state transition. The Ethereum squad attended the bootcamp as well, and we all followed closely the community’s anticipation/support/outrage, the Classic split.</p><p>While Ethereum matured since — one year later, Vitalik didn’t initiate a reversal of the Parity hack and the wallet freeze — one can argue that the effects of the DAO intervention are lasting: The primary effect of the intervention was not the violation of code-is-law rather the cementing of Vitalik’s position as the ruling arbiter. The decision not to reverse Parity was still perceived as Vitalik’s call rather than an inevitable consequence.</p><p>Whether a code-is-law view should be adopted should be a discussion for another day. Suffice it to say that a puritan stance lacks completeness in that it does not address systemic collapses — a breaking of the hash function, to provide an extreme example — or a huge 51% attack — where the social consensus will anyways violate the code’s dictation this way or another (I will post later a seemingly conflicting post regarding a WWIII-resilient finality adjustment protocol).</p><p>Code-is-law being less relevant, the discussion or rather reflection should evolve around the structure of the social-political-economic graph which shapes the social consensus, at least post catastrophic failures. And in the case of Ethereum, the network exhibits a supernode, a brilliant, reasonable, and responsible one, a single point of pressure nonetheless.</p><p>This is not to insinuate Vitalik has omnipotent power in the community; this is far from the reality of the community. Vitalik cannot pull off arbitrary code-change or impose protocol modification at his whim, he cannot act capriciously, his decisions face scrutiny, similarly to other leaders in structured organizations which are more often than not constrained by protocols and customary processes. It is not even to claim Ethereum is centralized — -this tired label is by now emptied from any tangible meaning by narrative grunts, and should be discarded. This is merely to say Ethereum is as robust as Vitalik is. It is far from being antifragile.</p><p>(Antifragility is a very useful adjective, coined by Taleb, and [GPT:] refers to a system or entity that not only withstands stress and shocks but actually benefits and grows stronger from them. Unlike robustness, which resists damage but does not improve, antifragile systems thrive in uncertainty and volatility.)</p><p>Again, this argument does not undermine Ethereum’s supremacy nor Vitalik’s leadership; the reader can acknowledge those and still comfortably remain an eth maxi. This is merely to point to the centrality of Ethereum’s social graph, to its single point of pressure, which is admittedly risky only in black swan events, but crypto is quite a hotbed for black swans:</p><p>Consider for instance a scenario where OFAC used the DAO precedent to force the hands of Vitalik and the EF to rollback the ByBit hack, lest they be deemed liable for being complicit in violating US sanctions. It is difficult to predict how would Vitalik and EF react in such a turn of events, and what would the community’s expectation and social consensus be in this scenario.</p><h3>Conquest’s second law</h3><p>Can Ethereum rearrange its social graph? I recollect Conquest’s second law of politics which states that an organization not constitutionally right-wing (or freedom maxi) will inevitably become left-wing (admin/intervention maxi), which should translate in crypto terms to: A project not constitutionally decentralized will inevitably centralize and ossify. In my head, I call this the reverse second law of cryptodynamics. (<a href=\"https://x.com/hashdag/status/1683507762204909568\">https://x.com/hashdag/status/1683507762204909568</a>).</p><p>Vitalik has historically evaded the question of Ethereum’s “moneyness”. In discussions I had with him in the past he rather dismissed this value proposition, and his public stance too was and still is non-committal. To me, this vague stance suggested not a lack of rigor but a recognition of the elusiveness of money — one can argue money is more of an emergent property of a commodity or asset rather than a defined checklist that an asset must satisfy in order to deserve the money title. Ethereum’s mindset is by and large consistent with &lt;Ethereum is tech; money is a rhetorical device; by adoption of the tech we achieve de facto a behaviour that is indistinguishable from money&gt;:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/979/1*JioJMKGxkQi6QdcDRWYZxA.png\" /></figure><p>This tech-oriented stance is quite reasonable and pragmatic, albeit shortsighted. Historically, widespread adoption has indeed upgraded many commodities into moneys, irrespective of their soundness traits (whatever exact criteria this entails). In the long term, however, these moneys were replaced by collectibles which were optimized for moneyness properties to begin with. Cf. <a href=\"https://unenumerated.blogspot.com/2016/07/artifacts-of-wealth-patterns-in_15.html\">Nick Szabo’s essay</a> on the origin of money (not coincidentally, Nick was one of the biggest supporters of the code-is-law stance towards the DAO hack.)</p><p>To complete the context, the current argument in Bitcoin circles regarding an OP_CAT plugin pertains, on a deeper level, to the question whether Bitcoin’s path from collectible to money is secured by the mere demand for it as store of value/e-gold; or rather an additional demand as a utility, e.g. a defi rails, is needed to support its moneyness, and without such demand it will remain “a Rolex”.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/624/1*sKfeVdfyhFTV13-wBwXrcA.png\" /></figure><h3><strong>Crescendo mainnet launch</strong></h3><p>Kaspa humbly suggests that a defi rails should satisfy the property of intra-round (simply: real-time) censorship resistant sequencing with subsecond confirmation (/inclusion) times, rather than inclusion times in the order of 10 minutes and censorship-resistance latency in the order of 1 hour.</p><p>There is a sense, therefore, that Crescendo feels like a second mainnet launch of Kaspa. For the reasons mentioned, a pure proof-of-work 10 BPS consensus constitutes a qualitative upgrade to Kaspa’s consensus engine, it ranks high in the ordered list of the value propositions of our money (refer further to Michael’s braindump, <a href=\"https://x.com/MichaelSuttonIL/status/1905387292853703157\">https://x.com/MichaelSuttonIL/status/1905387292853703157</a>). Whether the timing of the trouble in Terah’s family has to do with this “mainnet launch” is left to the reader as an exercise.</p><p>Alongside Michael, (and the symbolically-pseudonymous @someone235), another magician, a truly pseudonymous contributor @coderofstuff_ took ownership, and I wish him or her to remain faceless throughout our adventurous journey to silverness.</p><h3><strong>Kaspa Terah</strong></h3><p>The mainstream libertarian vision of governments expects their intervention when the need to break monopolistic entities arises, particularly those whose existence hinges on uneven grounds and structured leverage. Drawing the relevance of this axiom to the context with which this blogpost started is, too, left to the reader’s imagination.</p><p>Trust me, don’t trust me. As promised, it is very difficult to save the ouroboros creature. If I have disappointed you, perhaps you shouldn’t have appointed me in the first place. If you wished for the masks to remain, I have merely pointed at the faces overseeing the masquerade.</p><p>*<a href=\"https://www.youtube.com/watch?v=y1SeStQ1g4Y\">https://www.youtube.com/watch?v=y1SeStQ1g4Y</a></p><p>[1.] I’m not hinting that this is not unrelated to the current maintainers being Canadian and Australian.<br />[2.] For now.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5f100e0555a4\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2025-04-04",
        "ingested_at": "2025-07-03T05:36:43.630159",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "Kaspa where to (Part IV, last)",
        "link": "https://hashdag.medium.com/kaspa-where-to-part-iv-last-c68717a8d309?source=rss-400d0e2aab3b------2",
        "summary": "<h3>A Kaspa where to (Part IV, last)</h3><p>Exciting times for Kaspa!</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/949/1*mTq1rBuZXI6eY2Id2coALw.png\" /></figure><p>Kaspa is gaining traction, more eyes on us, different edges, intentions, interests. These interests may be at odds, but we are very early in our growth path, still categorically a positive-sum game. <br />In particular, future arrival of ASICs will be an overall positive for Kaspers, GPU miners included, similarly to GPU miners’ arrival being a big win for the CPU miners which they outmined.</p><p>Decentralization has more to do with the openness, permissionless, level-field nature of the market rather than the degree of heterogeneity in outcome. Fewer entities dominating mining is not inandof itself a sign of centralization, as long as they are unable to impose nonlinear rich-get-richer effects; for an example of the latter in longest-chain consensus, see Theorem 4 in <a href=\"https://www.ifaamas.org/Proceedings/aamas2015/aamas/p919.pdf\">https://www.ifaamas.org/Proceedings/aamas2015/aamas/p919.pdf</a></p><p>We are all romantically biased towards a visually egalitarian hashrate distribution pie-chart (a sentiment which leads many, in sociopolitical contexts, to wrongly expect fair systems <a href=\"https://www.aei.org/carpe-diem/thomas-sowell-on-the-fallacy-of-equal-outcomes/\">to demonstrate equal outcomes</a>). And, admittedly, capital itself is a barrier-to-entry and brings about some non-linearity to the game. However, this is the price of victory, paid by each and every ecosystem when passing the tipping point.</p><p>Moreover, Kaspa uniquely requires CapEx-heavy mining to fulfill its vision, as will be explained shortly. And so, when time comes and Kaspa shifts into “heavy” mining, we will welcome its maturity phase with great satisfaction, albeit a tinge of sadness.</p><h3>On Kaspa and CapEx</h3><p>(A previous post on this topic: <a href=\"https://hashdag.medium.com/in-which-i-have-no-patience-to-wait-til-by-and-by-b79ce53726b3\">https://hashdag.medium.com/in-which-i-have-no-patience-to-wait-til-by-and-by-b79ce53726b3</a>):</p><p>Kaspa perfects the consensus layer, primarily in terms of speed of confirmation; secondary — throughput capacity, decentralization; down the pipeline — MEV resistance.</p><p>Speed of confirmation is contingent on the mining market being illiquid, since in liquid mining environments, 51% attackers are theoretically — and, in low marketcaps, practically — feasible, and can be fended away by waiting time (or/and finality gadgets) and not by num of confirmations.</p><p>Typically, liquid vs illiquid is characterized by GPU vs ASIC, more inherently it is OpEx vs CapEx. The more CapEx will dominate mining costs, the less feasible it’d be for an attacker to rent temporary hashrate, eg via NiceHash; currently, about 5.3% of Kaspa network is NiceHash-able, and so we are seemingly still in safe illiquid territory.</p><p>CapEx-heavy mining is also more efficient (aka “energy efficient”), as a smaller fraction of the security budget is burnt with every new block. This efficiency is important both for the deflationary (no KAS minting) phase of Kaspa, but mainly for addressing the wastefulness of mining heads on, which is imperative if we are to aim at mass adoption. Notwithstanding the <a href=\"https://twitter.com/nic__carter/status/1481654012772372498?t=kxCxqpNm6q_5uui48Y5Cdg&amp;s=19\">good arguments</a> defending the energy consumption dynamics of POW, in its current form, POW is politically infeasible, and adoption considerations should supersede fundamentalism. <a href=\"https://twitter.com/musalbas/status/1359972560738406401\">https://twitter.com/musalbas/status/1359972560738406401</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*DdHvPwKpkXaj-2zfFt6Neg.jpeg\" /></figure><p>All in all, CapEx-heavy mining is essential for virtually instantaneous confirmation times in permissionless consensus, thus for Kaspa to fulfill Satoshi’s p2p electronic cash vision.</p><p>Note: Indeed PoS is pure CapEx, but the security considerations at the limit itself are non-continuous — a system with epsilon OpEx behaves materially differently than one with 0 OpEx, as the latter requires internal, state-dependent Sybil protection whereas the former hinges on external sources to distribute voting power.</p><h3>Optical POW</h3><p>Optical computation is a technology that utilizes interactions of photons, rather than electrons, to process computation. Optical POW (OPOW), envisioned by Michael Dubrovsky, is a POW-function optimized for optical machines. The low energy consumption would render OPOW extremely CapEx-heavy, and would thus be ideal for Kaspa, following above reasoning.</p><figure><img alt=\"An OPOW miner prototype; from the original OPOW paper\" src=\"https://cdn-images-1.medium.com/max/421/1*etQdhJOVXkNK91u2-vb2pQ.png\" /></figure><p>The current POW-function of Kaspa, kHeavyHash, is already friendly to optical ASICs (it is, of course, computable by CPUs, GPUs, and regular ASICs too!). This function can probably be further optimized, more R&amp;D is needed here.</p><p>For the original OPOW paper see <a href=\"https://arxiv.org/abs/1911.05193\">https://arxiv.org/abs/1911.05193</a>; for a recent Stanford lab paper on OPOW see <a href=\"https://techfinder.stanford.edu/technology_detail.php?ID=44752\">https://techfinder.stanford.edu/technology_detail.php?ID=44752</a></p><p>OPOW is a decentralizing force. It levels the mining field by centering competition around capital rather than energy, the former being order-of-magnitude easier to transport, convert, and distribute.</p><p>Aside from geographical decentralization, the low-carbon signature additionally allows for stealth mining operations, the existence of which is essential for censorship resistance (recall <a href=\"https://www.coindesk.com/business/2022/08/21/can-ethereum-fight-back-against-the-uss-sweeping-censorship-attempt/\">Ethereum’s OFAC-compliant</a> effective censorship of Tornado Cash transactions).</p><p>AFAIK optical ASICs will not enter the game in the short-to-mid term, pace depends on R&amp;D efforts and funding, which is fortunately above my paygrade (and I will obviously have no dog in this fight). Nevertheless, it is important to recollect and reaffirm the original vision we had when choosing Kaspa’s kHeavyHash, and to ensure community alignment around this.</p><p>Changes to the current version of kHeavyHash are probably necessary in order to optimize for OPOW, ideally parameter adjustments only, and with reasonable heads up to the mining community. Governance of such changes is TBD, and will depend on whether and how we can avoid centralization on the manufacturing end. This way or another, optical tech initiatives should be first citizens in Kaspa colony, as they are vital for a p2p electronic cash system to scale up while maintaining Satoshi principles, efficient security budget, geographical decentralization, political feasibility.</p><p>TLDR; Kaspa operates at the speed-of-light.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c68717a8d309\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2023-05-03",
        "ingested_at": "2025-07-03T05:36:43.630199",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "Kaspa where to (Part III)",
        "link": "https://hashdag.medium.com/kaspa-where-to-part-iii-72ddd0bbe6ae?source=rss-400d0e2aab3b------2",
        "summary": "<p>Sharing below quick thoughts on development funding models and sustainability, and on the next grant request, with the hopes that my incentive biases do not contaminate my thought process too much:</p><ol><li>DAGKNIGHT (DK) was an academic effort by Sutton and myself, conceived on the New Year of Trees,¹ and released to the open three years later, on the 14th anniversary of Satoshi’s WP release. As its birth-givers, we obviously wish to observe its impact on real world systems, and it doesn’t get more real than Kaspa. The protocol still requires some (presumably, our) attention before standing on its own feet, and we detailed the main TODOs in <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0002.md\">KIP #2</a>.</li><li>One operation mode would be to work on this for the sole sake of bringing our research into fruition, in “weekend project” mode, with no other incentive in play. This path is perhaps the default one in the gift culture of open-source, indeed one which DAGlabs RIP converged on when releasing Kaspa mainnet with neither a premine nor a heads-up on mining nor any founders’ rewards or the likes.</li><li>Another operation mode, popularized by <a href=\"https://gitcoin.co/\">Gitcoin</a>, is to receive a grant from the community in exchange for code contribution, occasionally with some milestones or/and timeline commitment. This mode is the prevailing custom in Kaspa community, and accordingly we are publishing today a grant proposal. The grant request is for 70 MKAS, which is ~0.5% of the circulating supply. For comparison, the previous funded grant was 100 MKAS, which was ~1% of the circulating supply.<br />My thought process here is to consider a hypothetical senior software engineer, Alice, with relevant domain expertise. When planning her path forward, Alice will seek either stability or opportunity, and in particular will typically not consider a short-term day-job for the same paycheck she’d receive for a long-term one. With this in mind, it makes little sense to denominate in USD deep-tech Kaspa grants (instability), rather they should be expressed in KAS terms (opportunity).<br />The same mental mode was behind the denomination of the previous grant in KAS, which was at the time greatly off the ballpark salary for the relevant devs. Hence the KAS denomination of this grant proposal too. The 0.5% ask seems reasonable to biased-me, especially when benchmarked against the previous grant.</li><li>The community accepting the grant is by no means a necessary condition for having DK implemented on Kaspa — Sutton and I will attempt at implementing it regardless, since, again, there exists already a non-materialistic incentive for us to do so. However, since we are not saints, this will be done on our spare time with non-committal or unspecified timeline.</li><li>A third operation mode, which too exists in open-source environments, is to found a for-profit enterprise around an open-source layer, which naturally incentivizes the entity to allocate resources to further development of the kernel. E.g., IBM’s symbiotic relationship with Linux. This path has the highest potential for long-term sustainability, though, of course, a for-profit entity would have its own priorities, and thus the timeline for DK execution would remain equally ambiguous.<br />I imagine this model could more realistically materialize around smart contracts, which are in arm’s reach from financial innovation opportunities. In contrast, DK is strictly an infra development, and has little to do with the app layer.</li></ol><p>[1] More on trees’ new year <a href=\"https://en.wikipedia.org/wiki/Tu_BiShvat\">here</a>. We submitted a JIP to upgrade this holiday and include DAGs, but the committee couldn’t reach consensus.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*OtAOvUQWxyHVhk_XAZg57Q.jpeg\" /></figure><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=72ddd0bbe6ae\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2022-12-14",
        "ingested_at": "2025-07-03T05:36:43.630227",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "Kaspa where to (Part II)",
        "link": "https://hashdag.medium.com/kaspa-where-to-part-ii-c080bcef2f3a?source=rss-400d0e2aab3b------2",
        "summary": "<ol><li>Crypto winters are warm for projects with character.</li><li>Last month Michal Sutton and I published the <a href=\"https://eprint.iacr.org/2022/1494\">DAGKNIGHT</a> protocol (DK), which to the best of our knowledge is the first POW consensus protocol that is responsive to the network’s actual (*adversarial) latency while being resilient to 49% byzantine attackers. DK is the culmination of nearly three years of research, a period in which we weren’t at all sure if the aforementioned property is at all possible to achieve.¹</li><li>Some work still needs to be done before considering DK for Kaspa:<br /> <br />(i) Completing several missing details in the proof section.<br /> <br />(ii) Preparing the paper for peer-review (depends on conference target).<br /> <br />(iii) Devising efficient algorithms — the current pseudocode is highly inefficient, as it was optimized for ease of reasoning rather than real world implementation.<br /> <br />(iv) Adapting the consensus algorithm to meet additional requirements of an actual cryptocurrency, e.g., the need to regulate minting, control difficulty, and enforce pruning, all of which require a responsive synchronous protocol (rather than DK’s partially synchronous operation mode).</li><li>Similarly to GHOSTDAG, DK enables high bps (blocks per second), just with much faster confirmation times. Some research needs to be done in order to suggest the optimal bps — increasing the rate indefinitely doesn’t necessarily shorten confirmation times, as it increases the higher relative latency or DAG width. The increase is both due to more blocks created per second and due to these blocks’ headers being larger. Regarding the latter factor, one can envision a scenario where confirmation times improve by reducing the number of block references inside a block (either in consensus or as a default mining rule), but whether or not this is the case is pending further research.</li><li>DK enables additional features, on which more research is needed.<br /> <br />(i) One example that comes to mind is <a href=\"https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008017.html\">flexcaps</a>, a proposal to allow miners to create blocks of different sizes and difficulties. While proposed originally for Bitcoin, at high block rates flexcaps require the DK consensus. To see the connection, observe that larger blocks → higher propagation delay of blocks → more blocks created in parallel → wider DAG; and the DK protocol uniquely does not need to bound in advance the width of the DAG, and can cope with it varying even across short timespans. One motivation for flexcap is to support, in times of peak demand, a higher throughput than that which the system can support on average. Indeed, large blocks consume both instantaneous load on the system (CPU, network congestion, etc.) and an accumulating load (larger UTXO → higher RAM and disk I/O for later block processing), which justifies a gap between the maximum limit on resource consumption and the average one.² This gap is enabled through flexcaps (or through the similar <a href=\"https://bitcointalk.org/index.php?topic=1078521\">elastic block cap proposal</a>).<br /> <br />(ii) Another potential feature is the <a href=\"https://github.com/kaspanet/research/issues/1\">stealth txns</a>, a construction which <em>utilizes</em> the asynchrony caused by high bps to protect users from MEV (relevant when smart contracts will be developed). More generally, and still in the context of MEV, the fact that many miners can create a block in the “next round”, can be utilized to facilitate richer transaction fee mechanisms, in some resemblance to Flashbots’ recent <a href=\"https://writings.flashbots.net/the-future-of-mev-is-suave/\">SUAVE</a>.</li><li>Similarly to the <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0001.md\">Rust upgrade</a>, this consensus upgrade will require a new grant request from the community. A suggested scope and raise amount will follow. I hope miners and other Kaspa whales will find this initiative as desirable and long-term profitable as the previous grant. Our community is evergrowing in size and interests, and raising large funds in the future might become harder and harder, at which point we will need to find other structures to maintain development. Hopefully we haven’t reached yet the tipping point.</li><li>Speaking of community size, a Discord moderator pressed a wrong button this week and accidentally kicked out all inactive users from the server, reducing Kaspa Discord community (~12k) by about 25%. The good news is that we learnt that ~9k members were apparently active in the last 30 days, which speaks volume of the quality of the community.</li><li>In the spirit of thanksgiving, I am grateful to all 9k of you for turning Kaspa from a sound protocol to a sound money.</li></ol><p>[1] More accurately, one of us was positive throughout that a possibility is within a hand’s reach, and the other was skeptic and believed an impossibility result was lurking in the dark. Interestingly, the roles reversed with respect to the question whether Kaspa can take off. We swore not to reveal the corresponding identities of the weak in faith, but the reader can be assured that between the two of us the truth always lies.</p><p>[2] In Bitcoin, where there’s no pruning of historical data, the gap is even larger, due to initial blockchain download that full nodes go through by default when onboarding.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c080bcef2f3a\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2022-11-26",
        "ingested_at": "2025-07-03T05:36:43.630253",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "Kaspa where to (Part I)",
        "link": "https://hashdag.medium.com/kaspa-where-to-part-i-ce6a1a2c8eb0?source=rss-400d0e2aab3b------2",
        "summary": "<p>This is a concrete version of a longer post which I started writing but had too much spare time so didn’t complete yet.</p><p>Context: One of Kaspa’s core devs, Michael Sutton, suggested a plan to order-of-magnitude enhance Kaspad full-node performance by refactoring the codebase and rewriting it in Rustlang. (<a href=\"https://discord.com/channels/599153230659846165/844142778232864809/993245032670842991\">https://discord.com/channels/599153230659846165/844142778232864809/993245032670842991</a>)</p><p>My twosats on the matter:</p><ol><li>Kaspa was initialized as a live proof of an idea, a demonstration of a novel (and very cool, but that’s beside the point) paradigm for permissionless consensus. In bootstrapping Kaspa, I was fully aware that we do not have the resources — or the manpower — or the organization machinery — required to unlock even 5% of Kaspa’s potential, and that some external strategic move will need to happen for that aspiration to come true (think Project Serum and Solana <a href=\"https://defirate.com/ftx-serum-solana/\">https://defirate.com/ftx-serum-solana/</a>).</li><li>For these reasons, as Kaspa OG’s recall, <a href=\"https://hashdag.medium.com/kaspa-launch-plan-9a63f4d754a6\">I considered launching</a> Kaspa in testnet mode, then opted for a novel (aka failed) mode, which I coined “gamenet”, and which can be thought of as “testnet with incentives”. While this attempt was apparently naïve and, in retrospect, flawed I am mentioning it to recollect the mindset with which Kaspa was released. (For the same aforementioned reasons I kept myself uninvolved with the exchange listing efforts of Kaspa; I fully appreciate their value to the community, and at the same time I preferred erring on the side of caution).</li><li>I agree with concerns voiced by some community members that, in principle, to bring real value efforts should be focused on integration, adoption, marketing, etc. In particular, at this stage, high bps and high tps are imho not a meaningful step towards building a non-hypothetical financial system.</li><li>However, our community is still far from the scale and organization necessary to reach actual adoption, if not merely for its still modest treasury size (which is contribution-based, and managed by a few volunteer OG’s). With that in mind, I believe the most correct usage of the funds donated by miners is to continue the path of demonstrating live the original DAG vision, by improving the base layer node and later on perhaps upgrade its consensus; which is precisely the proposal put forward.</li><li>The current Kaspad codebase is an adaptation of the Bitcoin client btcd, written in Golang. It enjoys a fine amount of technical debt and great code complexity, which make it difficult for new folks to contribute. The proposed refactoring explicitly aims at writing the codebase in a modular and legible manner, which is arguably even more important than performance enhancement.</li><li>For full disclosure, I am working tightly with Sutton for a few years now, and am highly biased in favour of any R&amp;D task or project to which he dedicates his rare talent (cf. Proverbs 27:14). Him availing a few months’ of his full capacity to Kaspa is exceptional, and I hope the community (and esp. miners) will match this generosity. I believe a ballpark of 1% of the circulating supply would be very reasonable.</li></ol><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ce6a1a2c8eb0\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2022-07-05",
        "ingested_at": "2025-07-03T05:36:43.630279",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "Kaspa (Black Tuesday)",
        "link": "https://hashdag.medium.com/kaspa-black-tuesday-8c7f4fa35834?source=rss-400d0e2aab3b------2",
        "summary": "<p>This post assumes reader context on the crash of the Kaspa network in the course of the last 48 hours, and provides some additional notes and perspective.</p><p>(1) To simplify logic and debugging, and since the gamenet concept didn’t really catch air, I removed the random block reward and replaced the average block reward of 500 with a deterministic block reward of 500. Thus, if so far we mined <strong>on average</strong> 86400x500=43200000 Kaspa per day, we will henceforth mine <strong>deterministically </strong>86400x500=43200000 Kaspa per day.</p><p>(2) Many ppl expressed their concern about the future rebase that will take place soon. I want to reiterate that the rebase is cosmetic only, it’s a rename, an altering of representation. If you mined so far, say, 10% of the (total or of the circulating) supply of Kaspa, you will posses 10% after the rebase as well.</p><p>(3) The deflationary monetary policy HF that I mentioned here (https://discord.com/channels/599153230659846165/909907923084382218/911015904144420895 or <a href=\"https://hashdag.medium.com/kaspa-launch-plan-responding-to-reality-6b4bec449037\">https://hashdag.medium.com/kaspa-launch-plan-responding-to-reality-6b4bec449037</a>) will be specified next week, after syncing and mining resume for a few days, the network remains fully operational and confident, and the voluntary Kaspa magicians (Ori, Michael, Elichai) get some sleep and catch up with their own research and ventures.</p><p>(4) The community by and large reacted solidly to the crash. Thank you! No-one took it lightheartedly, and at the same time most focus was on providing datadirs and other useful info, getting instructions, funny memes. Let’s hope we won’t need to test ourselves again in similar circumstance.</p><p>(5) There was some genuine misunderstanding regarding the approaches we were looking into. Specifically, we were never considering a rollback in the sense of pointing at an early state which we were satisfied with and wanted to revert to, and discarding blocks and transactions appended to it later. Rather, we were searching for the <em>latest</em> state for which we have a <em>certainly-valid</em> UTXO commitment. While many users shared with us up-to-date datadirs, and while we had our own datadirs, we had to spend effort and time to ensure that the UTXO commitment builds correctly. Thus, we (read: aforementioned devs) had to rebuild the state afresh, feed it with such datadirs, and compare the commitments. Fortunately, the UTXO that we built hashed into the same UTXO commitment string embedded in the latest block in the datadir, producing 710f27df423e63aa6cdb72b89ea5a06cffa399d66f167704455b5af59def8e20, which proved that the DAG UTXO algebra was not erroneous, but “merely” a victim of the memory problem. This is not to say the architecture of this module should not be revised and improved — -a more correct architecture would protect it from DB failures.</p><p>(6) Kaspa is here to stay, in case you were wondering.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8c7f4fa35834\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2021-11-23",
        "ingested_at": "2025-07-03T05:36:43.630304",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "Kaspa launch plan (responding to reality)",
        "link": "https://hashdag.medium.com/kaspa-launch-plan-responding-to-reality-6b4bec449037?source=rss-400d0e2aab3b------2",
        "summary": "<p>First and foremost I wanted to thank you all for joining and forming this community, for the interest, excitement, and involvement around the project. Seeing my PhD obsession — POW DAG consensus — realize itself into a live network and a spontaneous community is thrilling yet humbling. Thank you, <em>Todah</em>!</p><p>I’m definitely going to start valuing members-count over citation-count, so please bring more crypto friends to the party!</p><p>Every few years a new fair-launched POW cryptocurrency captures the excitement of the community — Litecoin, Monero, Grin, and now Kaspa. May the force be with us.</p><p>Since we didn’t anticipate this rapid growth, I didn’t prepare accessible answers to several FAQs. I hope to write a longer post about all this in the coming days, but for now here are some answers:</p><ol><li>Monetary policy will be deflationary. Halving will be more aggressive than Bitcoin’s since market conditions are different (order-of-magnitude faster market discovery). When the deflationary scheduling will be activated, and what would be the initial block reward (compared to the current avg of 500) — TBD. We will try to seal this next week or so. These numbers will imply the finite cap on supply. BTW we should rebase the term Kaspa to refer to today’s 1000 Kaspa, say; the current representation feels not so scarce :)</li><li>Our proof-of-work is a Kaspa variant of heavy-hash, let’s call it k-heavy-hash. My goal here was to create a CapEx heavy POW function, since I believe this concept is both energy-efficient and provides more miner-commitment (stronger than ASIC since less OpEx burnt). Whether k-heavy-hash is actually CapEx heavy, and whether a different POW function will better serve this goal for Kaspa — is a question I’m open to discuss.</li><li>The project is maintained by a few devs, all of which have other full time dealings, and some of which are funded by DAGlabs (but totally self managed). In particular, there’s no company or entity behind the project that is responsible for your wallet, full node, funds, miners. We are here during our spare time. I, for one, am a full time postdoc at Harvard university, and while this project is my PhD baby, I am at the same time dedicated to my postdoc baby. So this is a purely community project, please take that into consideration. What can you do to help? Arrange for more dev-power to learn the codebase and join the efforts; DAGlabs can potentially fund additional devs, as long as they have the ability to manage themselves, open issues, fix bugs, manage PRs, etc.</li><li>Roadmap. There is no official roadmap as there’s no organized development. I can write down what devs should be working on IMO, post bug fixing and version updates (HFs). In short, IMO priorities should be accelerating <em>gradually</em> to 10 blocks per second, then implementing an amazing upgrade to the consensus protocol, pending theoretical research results of Michael Sutton. In parallel, if someone can promote a privacy gadget (e.g., bulletproofs) and implementation for Kaspa that would definitely leap us forward.</li><li>Next week there will be a hard fork (HF) in order to fix a bug and decrease header size. Tune in on this, especially if you are running a mining full node. A few weeks later there will be a HF to embed said monetary policy.</li><li>Will we list the coin on exchanges? There is no “we”, there’s “you”. And I suggest waiting for the community to grow more organically before bringing retail.</li><li>When is it recommended to stress test for utxo throughput? You are free to stress test as you wish. However, note that even if bugs are discovered, some time will pass before the devs can make themselves available to fix them. Therefore, I suppose better wait for the current system to prove itself stable for more than two weeks, say.</li><li>What about block explorer? I believe next week devs will deploy one.</li><li>Feel free to follow me here or on Twitter <a href=\"https://twitter.com/hashdag\">https://twitter.com/hashdag</a> where I hope to post more Kaspa related material in the coming weeks.</li></ol><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6b4bec449037\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2021-11-18",
        "ingested_at": "2025-07-03T05:36:43.630329",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "Kaspa launch plan",
        "link": "https://hashdag.medium.com/kaspa-launch-plan-9a63f4d754a6?source=rss-400d0e2aab3b------2",
        "summary": "<h3>Kaspa launch plan (proposal)</h3><p>tldr</p><ul><li>launch Kaspa in gamenet mode, a research oriented experimental network</li><li>inject deliberate fragility into Kaspa launch via random semi-scarce monetary policy</li><li>construct battlefield for reward-based and MEV-based reorgs</li><li>as community matures and hashrate grows, go full scarcity mode, transition from game- to main- net mode, rendering early (gamenet) stage miners profitable in retrospect</li><li>gamenet 2.0 requires developing Ethereum bridge to simulate and practice MEV-reorgs, in order to test and demonstrate DAG consensus’ antifragility</li></ul><h3>Why Kaspa shouldn’t launch as an ordinary cryptocurrency</h3><p>[L1, POW, lack of EVM]</p><p>There seems to be little room now for a new L1, especially one powered by PoW such as Kaspa. The market has matured, and with it the scope of attacks and manipulations has expanded from direct attacks on block ordering (eg double spends via reorgs, liveness attacks) to attacks that regard txn content — typically in the zero-to-one-confirmations phase — by miners or bots (aka flashbots). Moreover, Kaspa lacks as of now EVM support, rendering it significantly less relevant to the current market.</p><h3>Gamenet: A proposal to launch Kaspa in a novel experimental mode</h3><p>Kaspa may be launched as a research-oriented consensus engine that is focused on experimentation, novel testing of dynamics, and a vibrant battlefield for real world cryptoeconomics attacks. I call it gamenet mode.</p><h4>Cryptoeconomics phase 1</h4><p>[CPU/GPU mining, uncertain scarcity, low hashrate and security, non-commercializable]</p><p>The platform should be CPU/GPU-mineable, to facilitate the base activity; I believe Ethash is a neutral candidate that fits our needs. However, its token should be deliberately unfit for commercialization, in order to penetrate hardcore communities, individuals, and zones that refrain from cooperating with non-BTC or non-ETH tokens. Accordingly, the token’s supply should challenge the ordinary notion of scarcity, and should be unfit for exchange listing. This implies that the platform will obtain low hashrate and low security, at the initial stages.</p><h4>Gamenet activity</h4><p>[battle-field, simulation, real world game, selfish mining, reorgs, MEV]</p><p>The theme of gamenet’s activity can be thought of as a continuous hackathon over a live network which serves as a battle-test field for simulating real world attacks, manipulations, and dynamics of a multi-player network. The block rewards will incentivize occasional reorg/selfish-mining attacks by strategic and sophisticated miners of the gamenet, whereas transactions in the network will implicitly or directly reflect MEV exploits from real world DeFi systems such as Ethereum.</p><h4>Community</h4><p>[R&amp;D groups and individuals, testbed for innovation, recognition by broad crypto community]</p><p>The goal is to attract research and dev groups (e.g., flashbots fans) to play, compete and/or collude over the live system, and to extract insights on the real-world dynamics of other live systems such as Ethereum. Further, commercial L2 projects that propose solutions to certain exploits, such as using cryptographic primitives for MEV, can implement those over Kaspa gamenet and prove the robustness of their solution, while others may attempt to refute it. I hope Kaspa will become a center of a vibrant R&amp;D community, and that the general community will look to Kaspa gamenet as a credible source of insights regarding cryptoeconomic dynamics in the wild.</p><p>An example for the community’s interest around the topic may be found in this recent summit <a href=\"http://reorg.wtf/\">http://reorg.wtf/</a></p><h4>Cryptoeconomics phase 2</h4><p>[monetary policy solidification, recover scarcity, compensate early community]</p><p>As the community and the platform matures, we will want to transition to a solid monetary policy, regulate the supply, and trade over exchanges. To this end, we may set an automatic trigger in consensus that will eliminate the randomness in the monetary policy, and the uncertainty of the supply, once a certain hashrate is reached. This will automatically compensate early participants — the early miners of the network — by rendering the mined tokens scarce in retrospect. See below for specifics. See <a href=\"https://fc21.ifca.ai/papers/222.pdf\">https://fc21.ifca.ai/papers/222.pdf</a> by Lucianna Kiffer for a related design.</p><h3>Kaspa development and support</h3><p>[community+product management, further development and support]</p><p>As a continuous real-world hackathon, Kaspa gamenet can significantly enjoy some product and community management, conveying and demonstrating to the community the rules of the game and example dynamics. Community members are welcome to take the lead on these fronts.</p><p>On the development side, and closer to the backend, explicit MEV activity can be simulated via a bridge from Ethereum. While not necessary for gamenet, I believe gamenet+MEV will make the platform much more attractive a battle-field.</p><p><em>Individuals capable and willing to take upon themselves some of these efforts, and who require funding, may DM me.</em></p><h3>Timeline</h3><p>[Kaspad ready mid October, Kaspa launch end of October, full gamenet activity TBD]</p><p>Kaspad — the core consensus component of Kaspa — will be production ready (though untested) by mid October. The remaining features are (1) implementation of the monetary policy described below, (2) plugging in the chosen PoW function.</p><p>The execution of components that enhance gamenet activity depends on community engagement.</p><h3>Monetary policy and block rewards</h3><h4>Requirements</h4><p>The block reward should incorporate randomness so as to:</p><ol><li>Test selfish mining and reorg attacks,</li><li>Introduce uncertainty regarding the supply,</li><li>Incentivize extending the main chain, 95% of the time (say),</li><li>Incentivize forking the chain when the reward — or the MEV opportunity — is exceptionally high.</li></ol><h4>Concrete proposal</h4><p>Have each block mint a random amount of Kaspa, where the randomness is a function of the last $M$ blocks. The result of the randomness, the “sampling”, should depend on the block’s hash, to ensure that it cannot be gamed. The randomness should not be a function of the previous block only ($M=1$) because that will lend itself to frequent forking attacks. At the same time, it should be responsive to recent blocks, to ensure a sufficient degree of uncertainty with respect to the supply (so $M=10¹²$ won’t do, as well). I propose $M=~100$ blocks.</p><h4>Formal description</h4><p>Given a DAG $G$, GHOSTDAG outputs a chain $C(G) \\subseteq G$. For each block $B ∈ G$, let $merging(B)$ be the earliest block in $C(G)$ that contains $B$ in its past. For each block $B ∈ G$, $B.selectedParent$ is the tip of the chain $C(past(B))$. For each block $B ∈ G$, $mergeSet(B,G)$ is defined as $past(B)∖ past(B.selectedParent)$ if $B$ is in $C(G)$, and the emptyset otherwise.</p><p>The reward of blocks in G, $rew(*)$, is defined by:</p><ul><li>$rew(genesis) = const_1$</li><li>$rew(B) = const_2*avg_{D \\in prvs M blocks of B}rew(D)*4^x + const_3*sum_{D\\in mergeSet(B)}(rew(D))$</li></ul><p>where $x$ is a random variable drawn from the normal distribution (mean 0, std 1), and which is uncontrollable by the miner (it is the result of the block hash). $const_1=1$, and $0&lt;const_2,const_3&lt;1$ should be in the order of $0.9$ and $0.1$ respectively; $M$ should be in the order of 100 as mentioned.</p><h4>Hashrate trigger</h4><p>Once the network reaches a meaningful hashrate, one that corresponds to operational expenses of $I=250k USD$ per week (say), the randomness should be eliminated, and then each block should mine a block reward of $rew(B) = const_3$ (const_3=1, say). This could be triggered automatically in consensus and requires no soft/hardfork.</p><h4>Finality parameter</h4><p>In phase 1, due to the expected low security, the finality parameter of the system should probably be set to be in the order of minutes rather than hours. In case of netsplits, the conflict should be resolved manually, which is a legitimate practice in a non-mainnet platform.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9a63f4d754a6\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2021-09-22",
        "ingested_at": "2025-07-03T05:36:43.630354",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "In which mayday mayday we are syncing about*",
        "link": "https://hashdag.medium.com/in-which-mayday-mayday-we-are-syncing-about-bf05ad58957a?source=rss-400d0e2aab3b------2",
        "summary": "<h3>Don’t trust, terrify!</h3><p>The “don’t trust, verify!” slogan is beyond my comprehension. I board airplanes without verifying anything about the pilot or the aircraft; I visit restaurants and foolishly eat — without verifying what will be transmitted to my blood; I take medicines without verifying the supply chain. Why would I protect my money with measures I am not taking to protect my life?!</p><p>This reminds me of Jacob, the forefather of Israel, who crossed the Jordan river some 35 hundred years ago, back to his homeland. Albeit, he forgot a few small pitches (think of dust UTXOs), so we are told by later Talmudic Rabbis, and so he went back to the eastbank, in the middle of the night, alone, to fetch the pitches. This was apparently unsafe, and he met a mysterious figure who wrestled with him, a battle from which he came out with a limp, a divine blessing, and a new name — Israel. The Talmud concludes:</p><p>“ <em>From here it is derived that the possessions of the righteous are dearer to them than their bodies. And why do they care so much about their possessions? It is because they do not stretch out their hands to partake of stolen property.</em> “</p><p>I hope the connection between this story and SPV sync mode based on multiplicative-hash UTXO-commitments is self explanatory but, just in case, do note that verifying by yourself the proper functioning of all external systems which you rely on is infeasible, unscalable, and anticivil — civilization is the scaling up of function and trust — whereas doing so only with respect to money is peculiar and will cause you to limp.</p><h3>Why not fiat?</h3><p>Because fiat supply is inflated unpredictably; because the political printing of money is corrupting the democratic sphere, positing the political debate on allocation of money instead of creation of wealth; because regulators are attempting to control financial transfers, eliminate cash, restrict economic freedom.</p><p>We reiterate cryptocurrencies’ golden traits — predictable issuance and censorship resistance. In contrast, the property “no fraudulent transaction ever occurred in this currency system” is not the something (or at least: main thing) users should/care about. Consequently, when joining a cryptocurrency system, checking its issuance and its decentralization is far more important than checking the historical validity of transactions.</p><p>Yes, we go so far as to say that users do/should not care about historical corruption. For the user is interested in knowing that the state of her node is the one that is most likely to prevail by the economic majority, not that the state of her node is the valid one in some abstract sense. <em>The primary goal of consensus systems is facilitating</em> agreement, <em>not enforcing</em> consistency. And if the economic majority happens to follow and maintain a ledger to which an invalid transaction entered at some point in history, so be it, the system can still serve its purpose of facilitating agreement even if its constitution was breached at some point in time. Indeed, if code rules, the constitution, are violated time and again, or even not extremely rarely, the wise user should definitely refrain from joining said network. Users are therefore protected from unsafe networks as long as they can safely assume the existence of efficient ways to communicate and diffuse information on such mishaps. This novel way to diffuse information is called The Internet, or its manual predecessor — Civilization.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gVeVc_JLtgKMJjQUnviwlA.png\" /></figure><p>As with airplanes and restaurants, users rightly assume that they arrived at a functioning system, and that information on past security breaches would have reached them in the conventional information channels. These information channels are typically reliable thanks to highly-staked, ideological, or philanthropic whistleblowers that continuously verify the ledger — or by previous users that have been harmed by the system’s malfunctioning — granting the system its herd immunity. Running full nodes are thus very important for the ledger’s society, however, the marginal utility is rapidly decreasing.</p><p>To summarize: If you are going to join IOTA network, google it first.</p><h3>Why not Bitcoin?</h3><p>Initial Blockchain Download (IBD) is the process by which new nodes join the network. Since Bitcoin core devs’ ethos is “don’t trust, verify!”, the default behaviour of the new node is to download and verify the entire history of the Bitcoin ledger. Consequently, Bitcoin’s throughput is deliberately limited to enable fast IBD-indeed, processing too many transactions per second today would make it difficult for a user joining in 2040 to verify that today’s transactions were valid. I kid you not.</p><p>Back in 2013 my advisor sent me a paper titled “Bitcoin: a p2p electronic cash system”. The new narrative seems to have changed into “Bitcoin: an electronic store of value” coupled with “the attempt to create a p2p electronic cash coin is a scam”. An interesting development that is.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/623/1*mdRaVUEqfc6uxohppCEKZQ.jpeg\" /></figure><h3>Why Kaspa?</h3><p>First, to make Satoshi great again. PHANTOM is really a neat generalization of Nakamoto Consensus (when k=0 PHANTOM coincides with the longest chain rule), it follows the same principles just with support for concurrency. It is Satoshi at its best, and the only path to fulfil His electronic cash vision. We make DAGs because we know how to and because no-one else does. We implement PHANTOM because we want to ping with “send txn” and be ponged “txn mined” in the same manner we get the results of a google search or send an email. We picked up this challenge in the same way Bitcoin core devs chose to work on Taproot — it is cool and not entirely useless.</p><p>Secondly, to make myself rich.</p><p>Third, because we need a base layer whose tradeoffs are centered around the crypto-informed user’s needs and asks. This implies, in particular, and according to the worldview I suggested above — implementing the default node to skip historical verification, making it an optional operation, one that relies on the fewer archival nodes that some entities choose to maintain and make available (this is the same situation as in Bitcoin, since most Bitcoin full nodes are pruning the data necessary for syncing newcomers, but be advised to not share this info with Bitcoiners, it’s gonna ruin their day and by implication your week). Importantly, these archival nodes maintain the ability to prove to newcomers that a certain transaction was fraudulent, by providing the Merkle witnesses for inclusion in the blocks involved in the claimed collusion.</p><p>Accordingly, Kaspa nodes prune block data by default, and new nodes by default do not request historical data, rather, they sync in SPV mode, i.e., by downloading and verifying only block headers. I reiterate that this is not a stronger trust assumption than a history-verifying node, rather a different requirement. The node then requests the UTXO set from untrusted peers in the network, and verifies it against the UTXO commitment embedded inside the latest received header (technically, this is done against the latest pruning point). If those do not match, the node bans the sending peers, requests the UTXO set from new untrusted peers, and repeats the process. If those match, the node verifies that no unexpected inflation occurred by comparing the sum of UTXOs to the specified minting schedule, a comparison for which block headers suffice.</p><h3>Make Satoshi Great Again</h3><p>My not-novel suggestion to scale up Nakamoto Consensus:</p><ul><li>Latency constraint on Consensus — move from longest-chain to PHANTOM, which is tolerant to and compatible with any predetermined upper bound on the latency.</li><li>CPU consumption — process few transactions per second on L1, while supporting large payloads, which are cheap CPU-wise, and which enable easy and healthy L2 (e.g., SN/TARK proofs for ZKRUs).</li><li>Bandwidth consumption — design sharding of data and data availability proofs, similar to Eth 2.0 stopped after phase 1 (see Ethereum’s <a href=\"https://ethereum-magicians.org/t/a-rollup-centric-ethereum-roadmap/4698\">rollup-centric revisited roadmap</a>); this is an open research question, since PoW has no native identities to serve as the basis for sharding.</li><li>Memory consumption and disk I/O — implement <a href=\"https://github.com/cambrian/accumulator\">class group based accumulators</a> that require no trusted setup, and which allow to prune the UTXO set and run as a stateless client. Challenges include: UX of storing and updating the witnesses; weighing memory save against higher CPU consumption.</li><li>Storage — prune block data, reducing storage requirement from <em>O(block header size)*O(num of blocks in history) + O(block size)*O(size of history)</em> to <em>O(block header size)*O(num of blocks in history) + O(block size)* O(size of pruning window)</em>; additionally, consider pruning block headers, further reducing the requirement to <em>O(block size)*O(size of pruning window)</em>. Pruning block headers is an open research question, since it is then unclear how a new node will be guaranteed it is syncing to the consensus state and not to a stale or malicious branch. However, arguably, any system with (deterministic) finality enjoys/suffers/relies on weak subjectivity, and therefore reading the entire history of PoW might be redundant.</li><li>IBD time — implement DAG-adapted version of <a href=\"https://ieeexplore.ieee.org/document/9152680\">FlyClient</a> to reduce the cost of syncing a new node from O(num of blocks in history) to O(log(num of blocks in history)). This does not reduce the storage at the syncer, but does allow the syncee to sync w/o downloading the entire history of block headers.</li></ul><h3>What are you syncing about?</h3><p>Concluding today’s topic. Kaspa is PoW on steroids. It is optimized for the informed users, not for the ideologs. Its throughput ought be constrained by real time performance considerations, not by the performance of downloading and verifying the historical ledger, which is an auxiliary trust gateway, but not the primary pillar of trust in the system.</p><p>Kaspa should reach 100 blocks per second, and to that end, Kaspa nodes should be pruning data by default. When new nodes join the network, they should sync against the heaviest PoW DAG, as it is the one with max likelihood to represent the current consensus view. Node operators should take measures to connect to sufficiently many peers so as not to be eclipsed from the network (as in Bitcoin), so that they hear about historical mishaps, e.g., invalid blocks, finality violations, etc., which are recorded and propagated by full nodes to ensure that new nodes know what they are syncing about.</p><p>I’m reminded of the very best ad in the history of ads. Pardon my associative memory. The ad is picturing a coast guard trainee on his first day on the job. He’s apparently not too well versed English-wise, which is unfortunate for the desperate pilot trying to communicate with him that the plane is …</p><p>See the ad in the title reference below.</p><p>* <a href=\"https://www.youtube.com/watch?v=yR0lWICH3rY\">Title reference</a></p><p><em>Originally published at </em><a href=\"https://blocklivenessmatters.com/blog/syncing\"><em>https://blocklivenessmatters.com</em></a><em> on May 5, 2021.</em></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=bf05ad58957a\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2021-05-05",
        "ingested_at": "2025-07-03T05:36:43.630378",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "title": "In which I have no patience to wait ’til by and by*",
        "link": "https://hashdag.medium.com/in-which-i-have-no-patience-to-wait-til-by-and-by-b79ce53726b3?source=rss-400d0e2aab3b------2",
        "summary": "<h3>A personal take</h3><p>Fun fact: Many Bitcoiners believe that Bitcoin’s 10-minute block time is not too slow and that having a fast block rate is not useful.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/596/0*IlBLIc35wJApcYSq.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/598/0*0ORtD7OGWk8ULTUF.png\" /></figure><p>This is a very disturbing stance, since I’ve invested lots of conceptual and practical efforts into accelerating block times based on the premise that fast confirmation times-or “Liveness” in consensus terminology-matters, and it will break my spirit if this proves in vain. It is with the noble objective of making sure I’m needed that I present to you below the case for fast Liveness. I believe it matters.</p><h3>The case against fast confirmation times</h3><p>The elephant in the room: No one needs Bitcoin to be user-friendly; it is here to be HODLed, not used. Bitcoiners rarely pay with their bitcoins, a true Bitcoiner never sells them, and we can all just hope to be as sacred as He <a href=\"https://bitslog.com/2013/04/17/the-well-deserved-fortune-of-satoshi-nakamoto/\">who never even touched his bitcoins</a>.¹</p><p>While obnoxious, this position is not ridiculous. The most difficult part solved by Bitcoin was the creation of a stateless algorithmically-controlled money system. Of course, this money should be transferable and, ideally, conveniently so; but the main achievement is not “good UX money.” And if improving Bitcoin’s UX would compromise its resilience, decentralization, or social scalability, it’s not worth it.</p><p>This position does imply, though, that <a href=\"https://github.com/kaspanet\">a different cryptocurrency</a> will prevail as a cryptocurrency that’s intended to be used by the masses.</p><h3>The case against fast block times</h3><p>If you ever wondered why you are not invited to VIP Bitcoin parties, it’s probably because you let it slip that you think fast blocks are cool, which proves your deep lack of understanding of our cult’s core principles. This is where continuing reading this post can become very helpful for you.</p><p>In order to understand what system parameters improve confirmation times, and when, we need to describe the flow of a crypto payment authorization.</p><p>If you are a merchant receiving payments in crypto, you must wait until transactions are sufficiently irreversible before confirming them to the payers. There are two paradigms on how this <em>waiting</em> is done. In the first paradigm, you wait for a sufficient amount of blocks to pile atop the transaction in the ledger; here, accelerating block liveness will shorten your waiting, namly, the confirmation time. In the second paradigm, you wait for a sufficient amount of (absolute) time to pass before confirming; accelerating block times will not shorten your waiting.</p><p>Essentially, in the first paradigm you wait until enough <em>blocks</em> are mined so that the [hypothetical or invisible] attacker’s chain is shorter than the ledger’s main branch, with sufficiently high degree of certainty; in the second paradigm you wait until enough <em>time</em> passes so that the attacker’s budget has drained. The two paradigms correspond to two threat models.</p><h3>Illiquid vs. liquid mining</h3><p>Bitcoin miners spend only a few 10 5USD/hour on protecting our transactions. Their revenue from mining-a.k.a. the <em>security budget</em>-is on the order of a few 10 6USD/hour. Contrast this with Bitcoin’s market cap of a few 10 11USD, and you’ve got yourself wondering how 10 6can protect a 10 11asset-surely there are ways to make much more than a few millions by attacking the Bitcoin giant, by reversing the ledger after an hour or two. For instance, if you are a financial whale that has the ability to short Bitcoin,² or if your name is Vitalik and you want to double spend bitcoins to keep Bitcoiners from being obsessed with something other than Ethereum’s success. Definitely worth a few billions.</p><p>Well, since the Bitcoin mining market is highly illiquid, an attacker cannot get hold of temporary hashrate even if he has a few spare 10 6USD. Instead, the attacker will need to invest in manufacturing or purchasing mining ASICs, which are priced for long term mining, and are indeed on the order of magnitude of 10 11USD. It is the capital expenditure of Bitcoin miners (CapEx), therefore, that protects our Bitcoin transactions, not their operational expenditure (OpEx). The high CapEx is what lends credence to our assumption that the majority of hashrate is held by honest miners (on which I’ll elaborate below), whereas the OpEx seems too low relative to the potential gain from an attack.</p><p>But many cryptocurrencies operate in liquid mining³ environments wherein significant hashpower is available for temporary rent on <a href=\"https://www.nicehash.com/\">NiceHash</a>; NiceHash is a marketplace for renting hashpower, typically relating to GPU-dominated coins. Now, if attackers can temporarily rent large amounts of hashrate, there’s no reason to assume that they do not control a majority of it, during the attack window. Indeed, most of the 51% attacks on small cryptocurrencies have occurred in GPU-dominated mining environments. In these environments, one needs to think about the security of the system in different terms: Instead of the honest majority assumption, the security assumption underlying liquid mining environments is that a 51%-attacker’s budget is limited and therefore he is unable to spend OpEx-by mining with GPUs or with rented hashpower-for long attack windows.</p><p>The following table compares illiquid and liquid mining environments.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8FvApr7C_6akuW51wW5ZvA.png\" /></figure><h3>Payment authorization flows</h3><p>In the liquid setup, since the attacker’s budget is affected by the <em>duration</em> of the attack, payments can be confirmed safely after enough <em>time</em> has passed. In the illiquid setup, since the block race between the honest majority and the attacking minority is governed by block creation events, payments can be confirmed safely after enough <em>block creation events</em> occurred. We conclude that block liveness matters in illiquid ASIC-dominated environments. QED.</p><p>Here is a payment authorization flow comparison between Bitcoin and Grin:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/643/0*q0hVhhlU-uClaNA3.png\" /></figure><p>The above compares the payment authorization flows in illiquid, or Bitcoin-like, vs. liquid, or Grin-like, mining environments.⁵ Observe that the attack cost-hence the waiting time for confirmation-does not depend on the block rate, in the liquid setup. In contrast, in an illiquid environment, <em>func(num_of_confs, attacker_rhashrate)</em> does depend on block creation events; so accelerating block times does in fact shorten confirmation times.</p><h3>On the altruistic majority assumption</h3><p>It is useful to recall Satoshi’s original security analysis wherein neither honest miners nor the attacker regard their own incentives: The honest 51% follow the honest mining strategy altruistically, and the malicious 49% are irrational (aka <em>byzantine</em>) and attempt to maximize the damage inflicted or the success probability of the attack (and not the profit!⁶). This is not to say incentives are not underlying the core logic of the system, rather that questions about why a miner would mine honestly or maliciously are discussed outside the security model; they are macro level questions, whereas a client accepting transactions should concern herself with micro level dynamics. It is these macro level arguments that may justify our honest majority assumption. Beware! My thought process here makes “the dumbest assumption of all,” <a href=\"https://twitter.com/NickSzabo4/status/1097020377778122752\">according to Nick Szabo</a> -namely, that Bitcoin’s security strengthens as more CapEx is invested into it. I admit I don’t get it.</p><p>I’m emphasizing Satoshi’s <em>altruistic majority</em> assumption because some people argue that OpEx is essential to security and that without it an attacker could use the same mining resource to mine both good and bad blocks simultaneously (similar to the <a href=\"https://blog.ethereum.org/2014/11/25/proof-stake-learned-love-weak-subjectivity/\">nothing-at-stake</a> criticism of PoS). My counterargument is that the costless simulation phenomenon is noncontinuous and holds only in the absolute zero OpEx case. But either way, this micro level reasoning with respect to mining requires a different-and, in fact, weaker-security model, namely, one which treats malicious (and honest?) miners as rational profit-maximizing agents participating in a game. Understanding these mining dynamics requires thorough analysis of optimal mining strategies, and an attempt at that was started <a href=\"https://arxiv.org/abs/1605.09193\">here</a>; also see <a href=\"https://dl.acm.org/doi/10.1145/2976749.2978341\">this paper</a>. These results show that a rational attacker can in fact maintain long-term profitable attacks, which questions the assumption that attacks are costly and which intrigues rephrasing or rethinking the security model.</p><p>In other words, I disagree with <a href=\"https://twitter.com/BobMcElrath/status/1115945915137908736\">Bob</a>.</p><h3>ASICs are energy-efficient PoW</h3><p>I’m not a fan of liquid mining systems. Yes, they are more democratic; anyone can plug in their GPUs, opt in, plug out, opt out. No barrier-to-entry, no barrier-to-exit. But this is a double-edged sword; “OpEx-heavy” means <em>you burn the security budget by using it</em>. It means the security-by implication, the ability to accelerate confirmation times-can only scale linearly with the amount of resources burnt on the system. To be very very very secure, we need lots and lots and lots of computational resources burnt for us. It works, but it’s inefficient.</p><p>In contrast, “CapEx-heavy” means you are protected by the miners’ investment, which locks them into the system and which is not burnt with every new mined block (outside of a negligible hardware wear).</p><p>It’s like the difference between buying a house and renting it. Your monthly mortgage payment might be similar to the rent, but you never enjoy a month’s rental payment beyond living in the residence for that specific month.</p><p>An ironic implication: Green PoW attempts which use available commodity hardware, such as <a href=\"https://www.chia.net/assets/proof_of_space.pdf\">Chia’s</a> and <a href=\"https://spacemesh.io/post-asic/\">Spacemesh’s proof-of-space</a>, are in fact more wasteful than ASIC-able PoW, because they constantly burn the security budget.</p><p>Pro tip: When dating a Bitcoiner, never tell her you care about energy-efficient PoW. An immediate irreparable turn off.</p><h3>Optimizing for the happy path</h3><p>I would like to finish the blog by making a fool of myself and claiming that the most important threat model is when a no-attacker assumption is applied, formally, <em>attacker_budget</em> = $0 and <em>attacker_rhashrate</em> = 0%.</p><p>In many use cases the payee is not really concerned by the payer carrying out a double spend attack. E.g., in cup-of-coffee-type purchases, or when the merchant knows and trusts the payer-remittance, in-person payments, etc. In such instances, good UX implies a very fast <em>first</em> confirmation time, that is, a short time span between the user broadcasting the payment and the receiver observing it has been mined into the ledger.</p><p>This category includes also cases where the good or service paid for will only be delivered to the buyer at a later date. Such is the case in e-commerce which takes several days to ship, thus providing enough time for the seller before real harm can be done. And such is the case when trading crypto against a centralized exchange, or a crypto-to-fiat gateway, which settles the IOU at a later date, off-chain.</p><p>This <em>time-to-first-confirmation</em> metric is almost always overlooked, as researchers and devs tend to analyze the usability of respective blockchains in a principled methodology, and treat the first confirmation as merely one step towards sufficient (probabilistic) irreversibility of the state. However, while the system is only usable due to its robust worst-case guarantees, real-world usage frequently requires, simply, speedy inclusion in the ledger, which establishes the importance of the <em>time-to-first-confirmation</em> metric.</p><p>Importantly, note that in <a href=\"https://eprint.iacr.org/2018/104.pdf\">PHANTOM</a> and many other DAG-based protocols, valid transactions that entered the DAG and that do not admit double spends will be accepted and admitted to the state regardless of the final ordering; in database language, they are <em>commutative</em> with respect to other transactions co-mined in the same level in the DAG. This implies that an attacker can reverse a payment only if he engaged directly with the victim, i.e., paid the merchant in exchange for another commodity or service and then went on to reverse the transaction. This is in contrast to a single-chain-based protocol where overriding the selected chain automatically overrides all transactions in it.⁷ When we refer to an attacker with <em>attacker_rhashrate</em> = 0 we do <em>not</em> assume, therefore, that 100% of the mining is done by honest nodes, rather that no attacker node engaged with the merchant directly in the to-be-confirmed transaction (or in very recent transactions on which this one depends).</p><p>Finally, and perhaps most importantly, a fast first confirmation is crucial for a good UX for the <em>payer</em>, regardless of the merchant’s waiting and authorization policy. Paying with crypto is an anxiety-inducing process, and an ordinary end-user wants to see the result of her attempts as fast as possible. Bitcoin’s ten minutes is just way too slow, it’s not the way money goes.</p><h3>That’s the way the money goes</h3><p>Impatience is a core trait of the Kaspa community. The main reason we are obsessed with DAGs is we want to see our transactions confirmed at Internet speed, similar to other web-based services. For use cases that require more than one confirmation we know to revisit the app later to check for the final status. Nevertheless, we need this speedy responsiveness out of the crypto app; we want a cryptocurrency to ping and be ponged, and we want it now.</p><p>Impatience will also motivate the smart contract layer design I will propose here [WIP], where I will suggest optimizing for speed rather than for decentralization. Stay tuned, and start getting used to instant confirmations.</p><p>*<a href=\"https://www.youtube.com/watch?v=fOg6JkGa2ic\">Title reference</a></p><p>[1] Or maybe he just kept losing his private keys out of sheer cumbrousness, and he’s now very upset about this whole Bitcoin thing.</p><p>[2] One could challenge this claim if one could provide evidence that there aren’t enough financial instruments and liquidity to profit, in the mentioned order of magnitude, from an hours-long attack on Bitcoin; then one would need to provide a model for confirming transactions based on the existing financial liquidity for such manipulations. Until then, we’re good.</p><p>[3] This term has nothing to do with liquidity mining in DeFi.</p><p>[4] CapEx dominates in the initial days of the ASIC. Of course, as time develops the miner pays more and more OpEx. A mining-informed friend told me that a very rough approximation of the CapEx to OpEx ratio, at the end of the machine’s lifetime, is 1:1. A PoW system which goes more extreme on the CapEx side would use an <a href=\"https://arxiv.org/pdf/1911.05193.pdf\">optical PoW</a> function, which uses cutting-edge optical computing chips. These chips run the central part of the computation on photons, whose interaction does not emit heat, in contrast to electrons running on wire. Also relevant is a paper by Ganesh et. al., suggesting virtual ASICs, a protocol that mimics any point on the CapEx-OpEx spectrum.</p><p>[5] (*) <em>elapsed_time</em> is the time that passed between the transaction being included in the block and now. (**) A more nuanced approach to the <em>attack_cost</em> paradigm would consider additionally the <em>attack_success_probability</em>, for the case of an attacker renting less than 50% of the hashrate. This does not change the qualitative differences between the paradigms, and so to keep things simple I assumed instead that the attacker is &gt;50%, during the attack window, when attacking a liquid mining system. And vice versa: <em>elapsed_time</em> can be used in illiquid mining systems as well, to shorten confirmation times, if the client applies more sophisticated confirmation policies; see <a href=\"https://eprint.iacr.org/2018/040.pdf\">this paper</a>. Still, accelerating block times would accelerate confirmation times in these policies as well.</p><p>[6] For instance, the bounds given by Satoshi on the attacker’s probability of successfully overriding the longest chain are calculated for an attacker that never abandons his one-shot attack, even if many blocks behind. Such an attacker is clearly not concerned with cost and profit.</p><p>[7] Admittedly, transactions removed due to chain reorgs can and in many systems are pushed back to the mempool. However, these transactions are not guaranteed to enter the blockchain in a relevant timeframe, either due to insufficient fees in the post-reorg congestion, or due to users keeping on transacting and innocently double spending previous payments.</p><p><em>Originally published at </em><a href=\"https://blocklivenessmatters.com/blog/patience\"><em>https://blocklivenessmatters.com</em></a><em> on December 28, 2020.</em></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b79ce53726b3\" width=\"1\" />",
        "author": "Yonatan Sompolinsky",
        "published": "2020-12-28",
        "ingested_at": "2025-07-03T05:36:43.630401",
        "source_type": "medium_rss",
        "rss_url": "https://hashdag.medium.com/feed",
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      }
    ],
    "telegram_messages": [],
    "github_activity": [],
    "discord_messages": [],
    "forum_posts": [
      {
        "post_id": 508,
        "post_number": 1,
        "topic_id": 295,
        "topic_title": "A proposal towards elastic throughput",
        "topic_slug": "a-proposal-towards-elastic-throughput",
        "content": "<p>In the following post I’m discussing transient storage as one type of resource that is capped per block. The proposed method can be applied to other resources as well, e.g., persistent_storage_mass, compute_mass.</p>\n<h2><a name=\"p-508-prepaid-transient-storage-a-proposal-1\" class=\"anchor\" href=\"#p-508-prepaid-transient-storage-a-proposal-1\"></a>Prepaid Transient Storage: A Proposal</h2>\n<p>Fullnodes on commodity hardware require strict transient storage limits. Currently, a 200GB cap per pruning epoch (~42 hours) is enforced. The enforcement mechanism uniformly distributes this cap across blocks, allocating an equal fraction of storage per block. This approach limits flexibility and prevents occasional large transactions from utilizing excess available storage, even when the global transient storage cap is not exceeded.</p>\n<h3><a name=\"p-508-problem-with-the-current-enforcement-mechanism-2\" class=\"anchor\" href=\"#p-508-problem-with-the-current-enforcement-mechanism-2\"></a>Problem with the current enforcement mechanism</h3>\n<p>The current mechanism enforces a fixed per-block limit:</p>\n<pre><code class=\"lang-auto\">transient_storage_mass_epoch_cap / total_blocks_per_epoch\n</code></pre>\n<p>This method prevents miners from accommodating natural fluctuations in transaction size. If some blocks use less than their allocated storage, the excess cannot be transferred to other blocks. As a result, transactions requiring more storage than a single block’s allocation are not feasible, even when total transient storage remains within the global cap.</p>\n<h3><a name=\"p-508-example-usecase-1-supporting-high-peak-txn-demand-aka-elastic-throughput-3\" class=\"anchor\" href=\"#p-508-example-usecase-1-supporting-high-peak-txn-demand-aka-elastic-throughput-3\"></a>Example usecase 1: supporting high peak txn demand (aka elastic throughput)</h3>\n<p>If a miner expects high transaction demand within an epoch, such as during peak hours, or needs to guarantee block space for users who have prepaid for transaction approval, they can mine underutilized blocks at the beginning of an epoch. This reserves transient storage for future blocks within the same epoch, ensuring that miners can handle anticipated transaction surges efficiently and allocate block space as needed. (Implicitly, I’m assuming here the method is applied to compute mass as well.)</p>\n<p>The idea to support elastic throughput came to me from Gregory Maxwell and Meni Rosenfeld’s elastic block cap ideas (around the big block debates), <a href=\"https://bitcointalk.org/index.php?topic=1078521.msg11517847#msg11517847\">https://bitcointalk.org/index.php?topic=1078521.msg11517847#msg11517847</a>. The gap between max capacity and peak capacity is of greater importance in Bitcoin vs Kaspa, since the former (supposedly) employs no pruning. Practically though, Kaspa’s pruning epoch of ~42 hours corresponds to more than 4 months’ worth of data growth in Bitcoin. In short, the peak vs avg gap is relevant to Kaspa in-pruning-epoch as well.</p>\n<h3><a name=\"p-508-example-usecase-2-supporting-native-stark-rollups-4\" class=\"anchor\" href=\"#p-508-example-usecase-2-supporting-native-stark-rollups-4\"></a>Example usecase 2: supporting native STARK rollups</h3>\n<p>A zk rollup entity may seek to implement native STARK verification using arithmetic field operations. Unlike SNARKs, STARKs do not require a trusted setup and offer quantum resistance, making them especially attractive for zk rollup infra. However, STARK proofs and the verifier size are significantly larger than SNARK proofs and verification scripts, potentially exceeding a few hundreds of KBs. Under the current enforcement mechanism, such proofs may not fit within a single block, making STARK-based rollups cumbersome or requiring them to go through a SNARK reduction, which is a legit construction, though slightly compromises the trustless property (it’s not too bad, since PLONK SNARK setup is universal updatable).</p>\n<h1><a name=\"p-508-proposed-solution-prepaid-transient-storage-5\" class=\"anchor\" href=\"#p-508-proposed-solution-prepaid-transient-storage-5\"></a>Proposed Solution: prepaid transient storage</h1>\n<p>To enable occasional publication of large blocks while maintaining the global transient storage cap, miners should be able to accumulate transient storage credits by underutilizing previous blocks. The credits here are used metaphorically, as a conceptual concept within the consensus/fullnode.</p>\n<p>When a miner produces a block <code>B</code>, the transient storage consumed is recorded as <code>transient_storage_mass(B)</code>. If <code>transient_storage_mass(B) &lt; transient_storage_mass_cap</code>, the difference is stored as credit. In a future block <code>X</code>, the miner may prove via digital signature that it is the miner of block B, and utilize:</p>\n<pre><code class=\"lang-auto\">transient_storage_mass_cap + (transient_storage_mass_cap - transient_storage_mass(B))\n</code></pre>\n<p>Generalizing this across multiple previously mined blocks <code>B_1, ..., B_n</code>, the total allowable transient storage in block <code>X</code> is:</p>\n<pre><code class=\"lang-auto\">C = (n+1) * transient_storage_mass_cap - Σ transient_storage_mass(B_i)\n</code></pre>\n<p>The full node then deducts the usage proportionally from the previously mined blocks:</p>\n<pre><code class=\"lang-auto\">transient_storage_mass(B_i) -= C/n\n</code></pre>\n<p>This mechanism enables miners to accumulate storage credits and later use them for transactions requiring more storage in a single block, ensuring better resource allocation while adhering to the global cap.</p>\n<h3><a name=\"p-508-elastic-throughput-and-dagknight-dk-6\" class=\"anchor\" href=\"#p-508-elastic-throughput-and-dagknight-dk-6\"></a>Elastic throughput and DAGKNIGHT (DK)</h3>\n<p>Recall that larger blocks propagate slower, which widens the DAG. Fortunately, the DK protocol can handle dynamic DAG widths, by readjusting the parameter <code>k</code> in real time. Even with DK, some hard cap on individual blocks’ sizes must be applied, e.g., each block shouldn’t exceed 2 MB.</p>\n<h3><a name=\"p-508-solo-miners-and-the-prepaid-approach-7\" class=\"anchor\" href=\"#p-508-solo-miners-and-the-prepaid-approach-7\"></a>Solo miners and the prepaid approach</h3>\n<p>One not accustomed to Kaspa’s high block creation rate – upcoming 10 per second – might find this whole approach of prepaid block space awkward. However, with 10 bps and north, it is likely that the mining market will change and adjust. In particular, some service providers – e.g., wallets, rollup/prover teams – may find it profitable to either mine only or primarily their own users’ txns or to engage in some agreement with existing miners. This gives rise to the notion that mined blocks – the economics of mining txns – will sometimes reflect the needs of specific entities and sectors, alongside ordinary generic mining nodes. (All of the latter rambling describes offchain economics; no entity will receive privileged treatment from consensus’ POV.)</p>\n<h3><a name=\"p-508-notes-8\" class=\"anchor\" href=\"#p-508-notes-8\"></a>Notes</h3>\n<ul>\n<li>Emphasizing again that this approach can be applied to other resource constraints as well, such as persistent storage and compute mass limits. Though, it seems particularly relevant for transient storage.</li>\n<li>One caveat of this approach is that, by providing proof of mining of previous blocks, miners link their mined blocks, thereby reducing their anonymity. However, most miners seem to not actively conceal their identity, so this is unlikely to be a significant issue.</li>\n<li>Post pruning all blocks’ credits must be zeroed, b/c the global transient storage cap is relevant for and enforced per pruning epochs (thank you <a class=\"mention\" href=\"/u/coderofstuff\">@coderofstuff</a> for this comment and general proofreading)</li>\n</ul>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2025-02-05T23:08:47.503Z",
        "updated_at": "2025-02-06T16:32:04.139Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/1",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 584,
        "post_number": 3,
        "topic_id": 295,
        "topic_title": "A proposal towards elastic throughput",
        "topic_slug": "a-proposal-towards-elastic-throughput",
        "content": "<p>Re persistent_storage_mass, it seems no different than transient_storage_mass, in that we don’t care about the peak usage just about its regulated growth in time, be it abrupt or gradual (ofc apart from the CPU cost of the storing operation). Applying credit to persistent_storage_mass would imply the credit is never expiring.</p>\n<p>Re cpu_mass, I was alluding to a credit that plays on the margin between peak and avg within the scope of an epoch, but better leave that complex optimization aside and admit that in the CPU context, as you said, we are interested in peak and not only aggregate consumption over time.</p>\n<p>BTW I’lll mention here another type of mechanism, which I am not necessarily advocating but which we might draw inspiration from: There’s an EVM feature where operations on zero-led addresses enjoy discounted gas costs. <a href=\"https://blockwithanand.hashnode.dev/the-impact-of-leading-zeros-in-ethereum-addresses-on-transaction-costs\">https://blockwithanand.hashnode.dev/the-impact-of-leading-zeros-in-ethereum-addresses-on-transaction-costs</a>. At the time Eth teams used this hack to reduce their gas costs, by investing POW in creating zero-led addresses, in order to save on future costs. If this address-POW mechanism simplifies implementation, we can consider using it in order to regulate transient storage by allowing txn issuers eg provers that use such addresses to publish large txns/proofs that exceed the usual block size; the address or POW would need to be renewed every pruning period.</p>\n<p>The benefit would be avoiding the need to couple identity of miner and prover/txn issuer, and no need to compromise miner anonymity through block linkage. If you view this path as superior to credit, we can discuss details eg whether this POW needs adjustment.</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2025-06-03T17:01:44.773Z",
        "updated_at": "2025-06-03T17:01:44.773Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/3",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 400,
        "post_number": 8,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p>AFAIK Andrew’s post deals with UTXO commitments (more accurately: accumulator), not sure what connection to this post you are suggesting here (?)</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2024-12-22T09:31:12.200Z",
        "updated_at": "2024-12-22T09:31:12.200Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/8",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 359,
        "post_number": 2,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Are any of the zk-friendly hash functions considered sufficiently safe by cryptanalysts? And is there a reasonable chance that some new zkp system variant would require choosing a different hash function?</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2024-12-13T04:54:10.344Z",
        "updated_at": "2024-12-13T04:54:10.344Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/2",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 399,
        "post_number": 5,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Thanks for this info <a class=\"mention\" href=\"/u/proof\">@proof</a>! Optimizing for execution of L1 ops should be primary over prover efficiency, so let’s aim for Poseidon, unless someone provides new arguments. Poseidon over what field do you recommend we use?</p>\n<p>(Elichai, any chance to get your input on Poseidon safety?)</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2024-12-22T08:37:28.784Z",
        "updated_at": "2024-12-22T08:37:28.784Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/5",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 417,
        "post_number": 1,
        "topic_id": 247,
        "topic_title": "Fees and throughput regulation dynamics",
        "topic_slug": "fees-and-throughput-regulation-dynamics",
        "content": "<p>Context: <a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208\">based rollups</a>.</p>\n<p>Since L2’s are permissionless, they too need a mechanism to cap their throughput. As per the custom, let’s refer to the computational load on L2 nodes in gas units. This load applies to any L2 operator – entities parsing the state, eg L2 explorers, wallets, etc. – but more importantly to provers. The latter’s load is 2-3 order of magnitude heavier than entities merely executing the state.</p>\n<p>What entities should regulate L2 throughput? One could imagine a setup where an L2 instance caps its own throughput by encoding a gas limit rule in its program, but this could lead to an undesirable outcome where users’ transactions are included in L1 blocks but ignored by the corresponding L2. If we want to avoid such horrible UX, then L1 sequencers are the only relevant entities to take on the responsibility of regulating L2 throughput.</p>\n<p>Let’s start by defining the requirements for a regulation mechanism:</p>\n<ol>\n<li>L2 throughput must be capped.</li>\n<li>L1 shouldn’t typically include transactions that exceed the throughput cap.</li>\n<li>L2 quality of service (QoS) should be preserved: Users with higher urgency should be able to prioritize their txns. (This crucial property is often overlooked, ignoring this requirement can lead to practical DoS for urgent transactions, similarly to the havoc on KAS, June '24, due to the lack of RBF to deal with txn congestion.)</li>\n<li>The mechanism shouldn’t rely on users’ honesty or miners’ goodwill.</li>\n<li>The mechanism shouldn’t require L1 actors executing L2 logic.</li>\n<li>L2 provers should be compensated for sustainability.</li>\n</ol>\n<p>One simple approach we propose (<a class=\"mention\" href=\"/u/freshair08\">@FreshAir08</a> +the usual suspects) is:<br>\ni. Transaction issuers declare the maximum gas their transactions consume.<br>\nii. Miners ensure no more than X gas units are confirmed per block per L2 instance (enforced in L1 consensus).<br>\niii. Transactions that exceed their declared maximum gas are reverted but still pay their fees, similar to Ethereum L1 rules. Note: Wallets can typically provide reasonable gas estimates. Even for complex transactions, users can often give tight bounds, so they won’t usually suffer from gross misestimates.<br>\niv. Transactions that don’t exhaust their declared gas still pay the full gas fee to L2 <em>from the perspective of L1</em>. Whether this is refunded to the user within L2 logic is outside this discussion. [A riddle for the reader: If users are refunded the gas change, can they overestimate gas just to be lazy? Why not?]</p>\n<p>It is easy to verify that this design satisfies all aforementioned requirements.</p>\n<p>Observe an important implication of this design: When L2 demand is high, revenue and fees go to L1 sequencers, not to L2 provers. I argue this is not a problem, and the preservation of L1’s security budget outweighs L2 layer biz model concerns. An alternative proposal will be be described at  the end of the post.</p>\n<p>Another observation is that introducing gas units and limits per block (point ii above) adds complexity for sequencers (miners) optimizing their blocks. It effectively creates a multi-knapsack problem for sequencers, which is NP-hard. However, this computation isn’t done on-chain or in consensus, and simple heuristics can achieve a sufficient approximation of the optimal solution. Importantly, this challenge isn’t unique to gas units; similar considerations apply to storage-mass (KIP9) units. See KIP13 by <a class=\"mention\" href=\"/u/coderofstuff\">@coderofstuff</a>.</p>\n<p>Importantly, each L2 instance can define its own interpretation and scale for gas—there’s no implication in L1 consensus about what gas specifically means. If one instance computes gas differently — whether in a simple or complex manner — that’s fine. L2 instances can also dynamically adjust gas scales based on load (<a class=\"mention\" href=\"/u/proof\">@proof</a> note this). However, this dynamic adjustment should be carefully designed to avoid situations where users’ transactions are included by miners but ignored by L2 programs, causing fee loss and poor UX.</p>\n<p>Finally, since we proposed that all transaction fees — regardless of rising demand — flow to sequencers, one might ask how L2 provers are paid and what guarantees their compensation and sustainability. One alternative advocated by <a class=\"mention\" href=\"/u/someone235\">@someone235</a> is that a fraction of L1 fees associated with a certain L2 instance flow to its P2SH address and be distributed to provers via its PROG. Hopefully others can evaluate this direction or provide alternative L2 sustainability schemes</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2025-01-07T17:09:52.136Z",
        "updated_at": "2025-01-08T21:10:55.183Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/1",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 474,
        "post_number": 4,
        "topic_id": 247,
        "topic_title": "Fees and throughput regulation dynamics",
        "topic_slug": "fees-and-throughput-regulation-dynamics",
        "content": "<p>Even if L2 txns incur no special costs to miners, no added complexity etc., still the miners are inevitably the entity to receive users’ txn fees and to enjoy surge in demand for L2 txns, b/c they are the (only) gatekeepers to the entire system, so the user must incentivize them – bribe, if you may – in order to be sequenced sooner rather than later. The more pressure and demand for L2 txns –  the more impatient users are willing to pay to enter the sequencing in a timely manner – and if this willingness to pay does not transfer to the miners themselves in the form of greater revenue from these impatient txns then the miner would ignore these high priority txns, which destroys the QoS and practically leads to a DoS for L2.</p>\n<p>Whether you need another L2-dictated form of payment or mechanism to satisfy the economics or sustainability of provers – I did not form a strong opinion on that, there are several options that come to mind. One could argue, as you<br>\nmentioned, that provers’ load is const and therefore they should require no extra compensation, others can argue that a healthy macro dynamic is one where service providers receive higher compensation in case of higher interest/demand in their services. L2’s can decide their own mechanism here, as long as they comprehend that fees for prioritization under demand must inherently flow to the miners.</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2025-01-22T22:04:59.535Z",
        "updated_at": "2025-01-22T22:04:59.535Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/4",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 404,
        "post_number": 1,
        "topic_id": 237,
        "topic_title": "Updateable list of L1<>L2 topics to flesh out before finalizing design",
        "topic_slug": "updateable-list-of-l1-l2-topics-to-flesh-out-before-finalizing-design",
        "content": "<p>For the sake of visibility and coordination of research core, dumping here an updateable list of research topics that must be fleshed out before we present an initial holistic design for L1&lt;&gt;L2. All topics and terms will be defined and explained clearly in separate independent posts.</p>\n<p>Published</p>\n<ul>\n<li>high level op_codes; L1&lt;&gt;L2 conceptual protocol: see <a class=\"mention\" href=\"/u/michaelsutton\">@michaelsutton</a>’s <a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208\">post</a>.</li>\n<li>cryptographic primitives, low-level op_codes: see <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a>’s <a href=\"https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219\">post</a>.</li>\n</ul>\n<p>Needs to be written/published:</p>\n<ul>\n<li>Benchmarks towards final decisions on zk-friendly hash; see <a href=\"https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/7\">comments</a> to <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a>’s post</li>\n<li><strong>State commitments:</strong>\n<ul>\n<li>uniqueness of the state commitment utxo. <a href=\"https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258#p-429-key-based-state-commitment-utxo-authentication-7\"><em>Done</em></a></li>\n<li>proving race rules; liveness attacks (proof construction to prevent ramifications). <a href=\"https://research.kas.pa/t/conflicting-proofs-policy/251\"><em>Done</em></a></li>\n<li>multiple state commitments per rollup (related to intra subnet parallelism)</li>\n</ul>\n</li>\n<li><strong>Sequencing commitments</strong>; hierarchical data structure (linear; consider usecase for log). <a href=\"https://research.kas.pa/t/subnets-sequencing-commitments/274\"><em>Done</em></a></li>\n<li><strong>Entry/exit mechanisms;</strong> virtual wallet management. <a href=\"https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258\"><em>Done</em></a></li>\n<li><strong>Throughput regulation</strong>; fee market; gas vs fees; global vs per subnet  gas limit; (related: multidimensional knapsack problem). <em><a href=\"https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247\">Done</a></em></li>\n<li><strong>L2’s interop:</strong> messaging protocol; design for sync atomic composability</li>\n</ul>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2024-12-23T13:49:35.720Z",
        "updated_at": "2025-01-20T11:31:42.541Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/updateable-list-of-l1-l2-topics-to-flesh-out-before-finalizing-design/237/1",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 90,
        "post_number": 1,
        "topic_id": 62,
        "topic_title": "Including transactions of red blocks",
        "topic_slug": "including-transactions-of-red-blocks",
        "content": "<p>Red blocks are “the orphans of DAG”, they are blocks that arrived too late and therefore their POW is not counted as contributing to the weight of their respective branches. This is formally defined by the PHANTOM/GHOSTDAG procedure. I propose to include their txns in the UTXO set as well.</p>\n<p>The motivation is that it allows for very fast 1 confirmation: In many use cases there is no real threat of a double spend, and the user only wants to know that her txn entered the ledger and is accepted for sure (assuming no double spend). This gives protection, e.g., against temporary 51% attacks, where an attacker gets hold of a majority of the hashrate for some limited time, and is able to gain full control over the set of blue blocks and to reject all txns included by the honest miners and users, which are now inside red blocks. (A counterargument is that valid txns that were rejected by this temp attack will be reintroduced to the mempool and mined again, post the attack. I still argue that this proposal greatly simplifies the UX for users/integration points, and that counting on the mempool process adds risks and uncertainty - mempool may bloat due to the attack, fees can temporarily skyrocket, etc.)</p>\n<p>This can be achieved by simply iterating over txns in the set of blocks <span class=\"math\">B.past \\setminus B.selectedParent.past</span> according to some deterministic topological ordering, and for each txn adding it to the UTXO set if it is consistent with the current UTXO.</p>\n<p>From a DAST point of view, there is no need to keep in memory the information on red blocks, it can be simply part of the UTXO handling around the accepting (blue) block.</p>\n<p>Note that txn fees should not be paid to the miner of red blocks. Note further that even red blocks must respect the mass and gas caps, and can’t introduce more txns than allowed.</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2020-04-07T16:03:47.235Z",
        "updated_at": "2020-05-11T16:24:44.945Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/1",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 349,
        "post_number": 4,
        "topic_id": 62,
        "topic_title": "Including transactions of red blocks",
        "topic_slug": "including-transactions-of-red-blocks",
        "content": "<p>IIRC red blocks receive no special treatment within the merge set–the rules of inclusion and the ordering do not explicitly take into account the colour of a block (re inclusion, it so happens that merge_depth violating blocks will be red, since merge_depth&gt;&gt;k). From the “local” merge-set pov, the only different treatment they receive is that their rewards are sent to the merging block, in order to impose a clear cost on double-spending attempts (similarly to Bitcoin orphans not receiving rewards; also related: the uncle mining attack on Ethereum 1.0).</p>\n<p>The real impact of blue-red discrimination is the contribution to a block’s score  (aka blue score), which impacts the chain selection rule, and in turn the entire ordering. Since only blue blocks contribute to a block’s ability to compete for chain membership, red blocks do not contribute to the security of the DAG’s ordering.</p>\n<p>BTW, I’ll try to explain in a later comment the answer to another question (which i believe you once raised): if red blocks do not contribute to security, why are they still counted in the DAA window</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2024-12-11T05:44:48.783Z",
        "updated_at": "2024-12-11T05:44:48.783Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/4",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 322,
        "post_number": 1,
        "topic_id": 193,
        "topic_title": "Atomic Composability and other considerations for L1/L2 support",
        "topic_slug": "atomic-composability-and-other-considerations-for-l1-l2-support",
        "content": "<h1><a name=\"h-1\" class=\"anchor\" href=\"#h-1\"></a></h1>\n<p>This post is based on extensive conversations and brainstorming with Sutton, Roma, and Ilia@Starkware.</p>\n<h2><a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h2>\n<p>We are putting efforts into designing Kaspa’s L1 support for L2. Our design follows the principles of zk-based rollups (ZK RUs): all smart contract activity is recorded in L1 as payloads (blobs in Eth jargon), and the state of all logic zones (smart contracts or rollups, I use them deliberately interchangeably, for reasons I’ll explain later or in a separate post) is committed to in block headers.</p>\n<p>The term rollups originally required on-chain sequencing, but nowadays on-chain sequenced rollups became the exception, not the rule, and are referred to as based rollups. To anchor a new state of a certain logic zone to the base layer, after one or multiple state transitions, some prover node must send to the base layer a transaction that provides a ZKP. The latter is verified in the base consensus through a new <code>op_code</code>, and the (commitment to the) state of the logic zone is thence updated. Crucially, the proofs may be submitted with non-negligible delays, notwithstanding users can get instant (~100 ms) confirmation by any node following this logic zone, executing its (L1-sequenced) transactions, and parsing its state. In short: proof latency does not affect finality latency; it affects the state-sync time of new L2 nodes, the L1-pruning interval, and the computational load of cross-logic zone transactions.</p>\n<h2><a name=\"atomic-composability-3\" class=\"anchor\" href=\"#atomic-composability-3\"></a>Atomic Composability</h2>\n<h3><a name=\"sync-vs-async-composability-4\" class=\"anchor\" href=\"#sync-vs-async-composability-4\"></a>Sync vs Async Composability</h3>\n<p>The best smart contract layers are designed for atomic or sync composability, where a smart contract can allow functions from other smart contracts to call it and interact with its variables (read and write) on the spot, atomically, in the scope of the triggering transactions. This cross-smart contract functionality describes the on-chain activity during Ethereum’s early years and arguably facilitated the incredible growth of its ecosystem and dev community. Unfortunately, Ethereum’s rollup-centric roadmap is working against this atomic composability feature and settles for async composability (which is still much better than TradFi UX, which requires manual composing).</p>\n<p>In async composability, smart contracts can still interact with and send messages to one another, yet this is done through some layer that carries or bridges these messages – typically the base layer – which suffers from some latency. Consequently, read/write actions are not treated in the scope of the originating transaction, atomicity is not guaranteed, and the effect of a composable transaction (the contracts’ variables and the issuer’s account post the transaction) cannot be guaranteed in advance.</p>\n<blockquote>\n<p><strong>Note</strong>: The unavoidable lack of predictability due to multiple users acting simultaneously is a separate issue; transaction issuers can enforce behaviour through several techniques, e.g., slippage specification, explicit conditions in the transaction, or intent specification. The issue with async composability is that the effect of only parts of the transaction can be fully enforced.</p>\n</blockquote>\n<h3><a name=\"based-rollups-are-atomically-composable-5\" class=\"anchor\" href=\"#based-rollups-are-atomically-composable-5\"></a>Based rollups are atomically composable</h3>\n<p>There are arguments for why async composability is good enough, but I will not get into them here. Let’s just assume we don’t want to settle on that. The good news is that we don’t need to since we are going full zk-based mode: Since all data is on-chain, the state of each logic zone is fully reconstructable from on-chain data (until the pruning point). Consequently, the effect of a multi-logic zone transaction occurs at its very sequencing, with no latency, and conditions on its effect across all logic zones can be simultaneously enforced. Contrast this dynamic with the non-based rollup-centric Ethereum, wherein the semi-off-chain sequencing of logic zone I transactions may be inaccessible to L1 (and to logic zone II provers), hence not provable to it. I reiterate that the sync atomicity of transactions is irrespective of prover latency: Proof arrival frequency does not affect confirmation times in L1 or L2.</p>\n<h3><a name=\"logic-zones-interactions-and-dependencies-6\" class=\"anchor\" href=\"#logic-zones-interactions-and-dependencies-6\"></a>Logic zones interactions and dependencies</h3>\n<p>Now, consider a composable transaction txn that not only acts on two logic zones but also triggers an interaction between them; e.g., the transaction calls a function inside logic zone I and uses the output of this interaction as an argument for a function call to logic zone II. Observe that to verify the correct execution of logic zone II, the base layer must see a proof of the correct state transition of logic zone II and that of logic zone I, since the output or intermediate state of the latter served as input to the state transition of the former. Similarly, the operators (read: provers) of logic zone II that wish to follow and parse their state must follow and execute logic zone I as well.</p>\n<p>This dependency seems problematic at first sight—if the existence of composable transactions implies that all provers need to execute all transactions of all logic zones, then the architecture supposedly collapses back to one grand logic zone that suffers from the same scalability hindrances as a computation-oriented L1—each txn consumes the same computation load that serves all other txns. But this is not really the case because:</p>\n<ol>\n<li>Executing transactions of other logic zones needs to occur only when logic zones interact.</li>\n<li>Logic zones can define permissions (in their program code) for specific logic zones to interact with them in sync mode and require other logic zones to interact in async mode through the base layer’s messaging protocol.</li>\n<li>Transaction execution is cheaper than proof generation by 2 or 3 orders of magnitude. Observe that provers of logic zone II need to execute but not to prove the (intermediate) state of logic zone I, which is the computationally intense part.</li>\n<li>Running a prover needs to be permissionless but not necessarily decentralized in the sense of optimizing for commodity hardware being able to run system-wide provers.</li>\n</ol>\n<h3><a name=\"minimizing-cross-logic-zone-dependencies-7\" class=\"anchor\" href=\"#minimizing-cross-logic-zone-dependencies-7\"></a>Minimizing cross logic-zone dependencies</h3>\n<p>These considerations imply that an ideal ecosystem would minimize the scope of logic zones, which as a byproduct would minimize cross-logic zone dependencies as well as the implications (e.g., the aforementioned execution burden) of dependencies. I strongly encourage L2/SC projects building on Kaspa to follow this design principle and avoid aggregating many separable logic zones (smart contracts) under one overarching logic zone (rollup).</p>\n<h3><a name=\"latency-in-async-composability-mode-8\" class=\"anchor\" href=\"#latency-in-async-composability-mode-8\"></a>Latency in async composability mode</h3>\n<p>It is important to note that when logic zones are not supporting atomic composability and instead use async composability through L1’s messaging feature, they suffer the latency of provers and not (only) the latency of the base layer. Thus, even when Kaspa implements 10 (Sutton, read: 100) BPS, if the prover of logic zone I provides a proof every 10 minutes, then that is the latency that async-composable transactions would suffer; and for many applications, 10 minutes = infinity (which is why Bitcoin can’t realistically serve as a base layer for zk-based rollups). This is why I think we should insist on atomic composability.</p>\n<h2><a name=\"ensuring-state-commitments-9\" class=\"anchor\" href=\"#ensuring-state-commitments-9\"></a>Ensuring State Commitments</h2>\n<p>A final comment on this construction: Recall that the composable txn described above forces provers of logic zone II to execute the state of logic zone I after txn’s first part. Now, a proof by the former operators can appear on-chain only after the latter submitted one; let’s denote this state by <code>state_I_pre</code>. However, logic zone I provers might submit a proof that batches a series of state transitions of I, of which <code>state_I_pre</code> is only an intermediate member. To allow II’s provers to build their proof utilizing the (chunk) proof of I, we must ensure L1 has access to the intermediate states that have been proven by I’s provers. In other words, we need all proofs to commit to all intermediate states in an accumulator (e.g., Merkle tree), and then II’s provers can add a witness to that commitment alongside the proof of their execution.</p>\n<h2><a name=\"reflection-10\" class=\"anchor\" href=\"#reflection-10\"></a>Reflection</h2>\n<p>At the risk of overselling the feature discussed in this post, I think that the construction I’m proposing here extracts the best of all architectures: Bitcoin, Ethereum (rollup-centric roadmap), Solana—an internet-speed version of Nakamoto base layer (verification-oriented), a zk-based computation layer, and a Solana-like unified defragmented state.</p>\n<h2><a name=\"l2-throughput-regulation-and-transaction-fee-market-11\" class=\"anchor\" href=\"#l2-throughput-regulation-and-transaction-fee-market-11\"></a>L2 throughput regulation and transaction fee market</h2>\n<h3><a name=\"controlling-l2-throughput-12\" class=\"anchor\" href=\"#controlling-l2-throughput-12\"></a>Controlling L2 Throughput</h3>\n<p>How to control the throughput of L2? Let’s denominate computation with the familiar “gas” unit. How should the gas limit be enforced? Since L1’s sequencers—miners, in Kaspa—are the only entity capable of selecting and prioritizing transactions entering the system, the gas regulation mechanism too must be employed at the L1 layer. The most simplistic design is to convert gas units to mass units, and since the latter are capped per block, so will gas per block be. Such a unidimensional restriction pools together resources of different natures—L1 mass (script computation and storage) and L2 proving loads—and this is economically inefficient: It implies, for instance, that a user issuing a gas-heavy, storage-light transaction may be outcompeted by users issuing storage-heavy transactions, despite the fact that she imposes no externality on them and can be co-approved without consuming the same resource. We should thus keep the mass and gas throughput constraints decoupled, namely, to provide a two-dimensional throughput restriction on blocks in the form: <code>mass(block) &lt; mass_limit</code> AND <code>gas(block) &lt; gas_limit</code>. This proposal implies miners will face a two-dimensional knapsack problem when selecting transactions from the mempool.</p>\n<blockquote>\n<p><strong>Note</strong>: The very same discussion applies to our coupling of L1 script computation mass and KIP-9 storage mass. We opted to couple them nonetheless under the same mass function and accept the theoretical economic inefficiency.</p>\n</blockquote>\n<p>Notice that a surge in demand for L2 operations will not translate to higher revenue for provers—miners, the selectors of the L1 club, will collect the profits This profit flow seems okay, since the gas limit implies provers’ load does not increase (beyond the limit) in times of higher demand; need to think about this more.</p>\n<p>Another proposal that came up is to run a variant of the EIP-1559 mechanism on the gas units, which (i) would flow base fee profits to provers, and which (ii) would remove the complexity of running a dual knapsack, as it provides an in-consensus sufficiently precise price per gas unit.</p>\n<h3><a name=\"avg-vs-peak-gas-limit-13\" class=\"anchor\" href=\"#avg-vs-peak-gas-limit-13\"></a>Avg vs peak gas Limit</h3>\n<p>Either way, when setting <code>gas_limit</code>, one consideration to put in mind is the gap between the average and the peak load on a prover: While logic zone I may typically comprise 10% of the gas capacity in blocks, its relative demand can reach 100% in peak demand, when users want to interact with it more than anything else. Here too, an economically-efficient design would restrict the block’s gas per logic zone. This, however, would result in an n-dimensional knapsack problem for constructing a block out of the mempool, and so we are currently opting for a simpler design with one <code>gas_limit</code> per block, acknowledging the economic suboptimality.</p>\n<h3><a name=\"funding-schemes-for-provers-14\" class=\"anchor\" href=\"#funding-schemes-for-provers-14\"></a>Funding Schemes for Provers</h3>\n<p>With either of the above mechanisms, L2 projects may conceive of additional funding schemes for provers. From an ecosystem vantage point, it is imperative that these fees be given in KAS, contribute to the flywheel of Kaspa, and align all players.</p>\n<p>Again, I emphasize that I’ve summarized here ideas and considerations raised by aforementioned co-contributors. Some of the topics discussed here are still WIP and open for discussion, hence the research post format.</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2024-11-12T23:32:23.476Z",
        "updated_at": "2024-12-03T08:16:09.382Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/atomic-composability-and-other-considerations-for-l1-l2-support/193/1",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 188,
        "post_number": 3,
        "topic_id": 122,
        "topic_title": "Miner funds consolidation",
        "topic_slug": "miner-funds-consolidation",
        "content": "<blockquote>\n<p>and the output value is slightly greater than the sum of the inputs</p>\n</blockquote>\n<p>But this effects the total emission of coins, and also gives more value to larger miners, which is undesired from decentralization standpoint</p>",
        "raw_content": "",
        "author": "msutton",
        "created_at": "2022-01-05T13:46:09.529Z",
        "updated_at": "2022-01-05T14:00:37.346Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/miner-funds-consolidation/122/3",
        "category_id": 7,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": true,
          "is_founder": false
        }
      },
      {
        "post_id": 189,
        "post_number": 4,
        "topic_id": 122,
        "topic_title": "Miner funds consolidation",
        "topic_slug": "miner-funds-consolidation",
        "content": "<p>In general, one can think of allowing coinbase transactions to have a UTXO input (thus constantly aggregating), however this introduces the following tradeoff/dillema:</p>\n<ul>\n<li>\n<p>Spending a UTXO usually requires a signature, thus complicating miner logic and requiring interaction with miner’s wallet</p>\n</li>\n<li>\n<p>Allowing the usage of a UTXO which belongs to the same mining address without a signature, might have many side-effects which are usually avoided in the UTXO model, such as replay attacks and double-spending on behalf of someone by mining to his address</p>\n</li>\n</ul>",
        "raw_content": "",
        "author": "msutton",
        "created_at": "2022-01-05T14:39:53.253Z",
        "updated_at": "2022-01-05T14:39:53.253Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/miner-funds-consolidation/122/4",
        "category_id": 7,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": true,
          "is_founder": false
        }
      },
      {
        "post_id": 205,
        "post_number": 1,
        "topic_id": 127,
        "topic_title": "Pruning point heuristic formula",
        "topic_slug": "pruning-point-heuristic-formula",
        "content": "<p>This is not an actual research post, but rather a technical mathematical one for clarifying a point. I’m using this platform mainly due to latex support.</p>\n<p>I might be using basic notation from <a href=\"https://github.com/kaspanet/docs/blob/main/Reference/prunality/Prunality.pdf\">the prunality proof</a>. However note that pruning point calculation in the actual Kaspa implementation was modified a bit to be non-sliding, as described below.</p>\n<p><strong>Definitions</strong></p>\n<p>Denote <span class=\"math\">P, F</span> to be the pruning and finality depths respectively.</p>\n<p>Define the finality score of a block to be:</p>\n<p><span class=\"math\">\nFinalityScore(B) := \\lfloor BlueScore(B) / F \\rfloor\n</span></p>\n<p>The formula for calculating the pruning point of a block <span class=\"math\">B</span> is as follows:</p>\n<p><span class=\"math\">PruningPoint(B) := \\max C \\in Chain(B)  \\text{  s.t.  } BlueScore(B) - BlueScore(C) \\ge P </span> AND  <span class=\"math\">FinalityScore(C) &gt; FinalityScore(PruningPoint(SelectedParent(B)))</span></p>\n<p>In words: we only progress the pruning point of a block if we find a pruning point candidate which has larger finality score than the previous pruning point (i.e. the one we calculated for <span class=\"math\">SelectedParent(B)</span>).</p>\n<p>The idea is that the usage of finality score makes the pruning point move in steps of <span class=\"math\">F</span>.</p>\n<p><strong>Heuristic</strong></p>\n<p>Now, we want to define a heuristic for easily determining that the pruning point did not move, and to do so with out iterating a <span class=\"math\">F</span>–length chain, at least in most cases.</p>\n<p>Claim: if <span class=\"math\">FinalityScore(BlueScore(B) - P) \\le FinalityScore(PruningPoint(SelectedParent(B)))</span> then<br>\n<span class=\"math\">PruningPoint(B) = PruningPoint(SelectedParent(B))</span></p>\n<p>Proof: assume the contrary and denote <span class=\"math\">\\pi</span> the pruning point of <span class=\"math\">B</span>. By definition we know that <span class=\"math\">BlueScore(\\pi) \\le BlueScore(B) - P</span>. So <span class=\"math\">FinalityScore(\\pi) \\le FinalityScore(BlueScore(B) - P),</span> contradicting the requirement that <span class=\"math\">FinalityScore</span> progressed.</p>\n<p>(<em>note:</em> some abuse of notation here by using the block or the blue score of a block or a sum combination in calls to <span class=\"math\">FinalityScore</span>)</p>\n<p>However, the current codebase seems to have a wrong formula for this heuristic (see <a href=\"https://github.com/kaspanet/kaspad/blob/ce4f5fcc33bf985bd78b110170b15ce6d74d5f83/domain/consensus/processes/pruningmanager/pruningmanager.go#L998\">here</a>), with the following condition:</p>\n<p><span class=\"math\">FinalityScore(BlueScore(B)) \\le FinalityScore(PruningPoint(SelectedParent(B))+P)</span></p>\n<p>This is not an equal definition due to the floor function in the definition of <span class=\"math\">FinalityScore</span>.</p>\n<p>The question for discussion is how safe is it to fix this condition w/o breaking consensus. It seems like there are theoretical cases where the wrong formula returns <span class=\"math\">true</span> while the correct formula returns <span class=\"math\">false</span>, meaning that the current code would skip searching for the new pruning point although it should have performed the search.</p>",
        "raw_content": "",
        "author": "msutton",
        "created_at": "2022-03-03T11:34:35.180Z",
        "updated_at": "2022-03-03T12:19:46.968Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/pruning-point-heuristic-formula/127/1",
        "category_id": 10,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": true,
          "is_founder": false
        }
      },
      {
        "post_id": 168,
        "post_number": 1,
        "topic_id": 106,
        "topic_title": "Visualization of DAGs",
        "topic_slug": "visualization-of-dags",
        "content": "<p>This post describes some wishes I have regarding visualization features for DAG that could bring more intuition and understanding of DAG tadeoff to any new DAGer:</p>\n<ol>\n<li>\n<p>A simulated visualizer that is configurable, allowing the user to play with the block rate, the block mass limit(s), the assumed delay in the network (more advanced: the topology of the mining network), etc., and view a DAG growing according to the specified configuration.</p>\n</li>\n<li>\n<p>A rich Rothchild (=Kaspa testnet txgen): The primary effect of the mass limit(s) is on whether the DAG is processing many regular small txns, or few txns with big payload. The latter is more interesting imo, as it is more in line with the current needs of the community (cf. Rollups), and because it keeps the requirement for full node more reasonable, as it only needs to process few (10, say) txns per second. Bottom line is: visualizing the different setups and capabilities requires implementing a txgen with rich features.</p>\n</li>\n<li>\n<p>As a next step, I’d wish to be able to see what the resource consumption would be with such configurations – some rough estimations on the required CPU, bandwidth, disk space needed to run such a DAG in a real node. The estimation could originate from some extrapolation from several real data points (real performance of Kaspad under different configurations).</p>\n</li>\n<li>\n<p>A separate tool that allows for a step-by-step algorithm visualizer of various DAG protocols, e.g., longest chain. Inclusive-GHOST, SPECTRE, PHANTOM. Similar to <a href=\"https://rosulek.github.io/vamonos/demos/index.html\">https://rosulek.github.io/vamonos/demos/index.html</a></p>\n</li>\n<li>\n<p>A dashboard for the actual Kaspa (test/main) network, monitoring various metrices. Personally I’m interested in (i) the end-to-end time-to-inclusion, namely, the time that passes from the user pressing “send txn” to the user observing the txn included inside some block in the DAG; and in visualizing (ii) the frequency of block creation by small miners, i.e., how frequent a miner with x% of the hashrate would mine a block, and the resulting time-to-inclusion for a user that sends a txn directly to the miner, without going through the mempool. More motivation on this later. The expected value of these items can be provided with over-the-envelope calculations, but we want to see that they admit to their expected value in the actual network.</p>\n</li>\n</ol>\n<p>Other metrics are mentioned in <a href=\"https://research.kas.pa/t/metrics-to-test-dag-performance/88\">this post</a>. If you have more ideas pls comment below.</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2021-02-24T12:16:30.512Z",
        "updated_at": "2021-02-24T12:30:22.348Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/visualization-of-dags/106/1",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 145,
        "post_number": 1,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<p>Q1: What quantity acts as a (rough) time measurement in a POW-powered DAG?<br>\nA1: The number of blocks whose timestamps were considered by the difficulty adjustment algorithm (DAA), when adjusting the difficulty.</p>\n<p>Q2: Why is time measurement important?<br>\nA2: It is (and has to be) used for regulating coin minting and for enforcing relative time-lock scripts.</p>\n<p>======</p>\n<p>A1 is quite tautological but still I want to point out why the blue score, or the pastSize (roughly the number of blocks in the DAG) do not act as such measurements.</p>\n<p>The problem with blueScore is that an attacker can withhold its blocks, publish them late so that they are not part of the blue set, and manipulate thus time measurement. This would lead to us underestimating the time that has elapsed.</p>\n<p>The problem with pastSize is that post a network split (which did not last longer than the pruning window) both branches will have already adjusted their block rates (i.e., mine with lower difficulty) which would lead to us overestimating the time that has elapsed, as pointed out by <a class=\"mention\" href=\"/u/deshe\">@Deshe</a>.</p>\n<p>======</p>\n<p>Notes and reminders:</p>\n<ul>\n<li>\n<p>Recall that Kaspa DAA uses a sliding window, in the line of algorithms developed and investigated by Zawy12 in <a href=\"https://github.com/zawy12/difficulty-algorithms\">https://github.com/zawy12/difficulty-algorithms</a>. Roughly, the idea is to consider the last <span class=\"math\">N</span> blocks (in Kaspa: the <span class=\"math\">N</span> blocks with highest blue score in the DAG) – aka the DAA window – and use their average and min and max timestamps to adjust the blockrate, in a manner similar to that done in Bitcoin.</p>\n</li>\n<li>\n<p>Technically, A1 translates to maintaining, for each block, the number of blocks that are members of its merged set and that were used in the DAA (i.e., that were part of its DAA window). I emphasize “blocks that are members of the merged set” because, as <a class=\"mention\" href=\"/u/elichai2\">@elichai2</a> points out, we select the highest blue score blocks in a block’s past when calculating the DAA, and every block inherits the blue-red colouring of its selected parent, which in turn implies that a block will not consider in its DAA window blocks in its selected parent’s past that were not already considered by its selected parent.</p>\n</li>\n<li>\n<p>While every block will have its own time measurement – i.e., its own count of the number of blocks that participated in the DAA windows throughout its past set – the dictating one is of course the block that won the consensus battlefield, i.e., the selected tip of the virtual.</p>\n</li>\n</ul>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2020-12-09T11:12:49.703Z",
        "updated_at": "2020-12-09T11:23:35.418Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/1",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 147,
        "post_number": 3,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<p>Good point. Do you then suggest we modify the pruning/finality choices according to the time measurement proposed herein?</p>\n<p>BTW, a bad (though not dire) consequence of decreasing clock speed is the deviation from minting/inflation schedule.</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2020-12-09T17:35:25.452Z",
        "updated_at": "2020-12-09T17:35:25.452Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/3",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 152,
        "post_number": 7,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<p>Lower bound is sufficient. If I understand correctly, an attacker can manipulate these points by a factor of 2 at most (assuming an attacker &lt; 50%)</p>",
        "raw_content": "",
        "author": "hashdag",
        "created_at": "2020-12-10T11:44:02.348Z",
        "updated_at": "2020-12-10T11:44:02.348Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/7",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "founder_researcher",
          "is_lead": false,
          "is_founder": true
        }
      },
      {
        "post_id": 583,
        "post_number": 2,
        "topic_id": 295,
        "topic_title": "A proposal towards elastic throughput",
        "topic_slug": "a-proposal-towards-elastic-throughput",
        "content": "<aside class=\"quote no-group\" data-username=\"hashdag\" data-post=\"1\" data-topic=\"295\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/hashdag/48/45_2.png\" class=\"avatar\"> hashdag:</div>\n<blockquote>\n<p>The proposed method can be applied to other resources as well, e.g., persistent_storage_mass, compute_mass.</p>\n</blockquote>\n</aside>\n<p>Applicability to transient storage is unique in that the epoch with which it is relevant is clearly defined (the pruning period). After such an epoch, the transient storage credits are reset and at any time the expected maximum bound for storage use within the pruning period is maintained even if the transient storage limits are made elastic.</p>\n<p>However, given your claim above I’m curious what your thoughts are for how this credit system this would be applied to persistent storage mass (which KIP9 applies to; and relates to UTXO storage and is boundless) and to compute mass (related to CPU usage, bounded to 100% cpu capacity but is also technically a resource that can’t be “stored for use later” easily)</p>",
        "raw_content": "",
        "author": "coderofstuff",
        "created_at": "2025-06-02T20:11:11.781Z",
        "updated_at": "2025-06-02T20:11:11.781Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/2",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 577,
        "post_number": 3,
        "topic_id": 347,
        "topic_title": "On the inherent tension between multileader consensus and inclusion-time proving",
        "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
        "content": "<ol>\n<li>\n<p>Async interop should generally be discouraged, but the issue there is in my opinion similar only in cosmetics - it would ressemble more a locking of outboxes and such, and of the top of my head as long as all parties know what they are into, it has no effect on others in the system and they can set their rules as they see fit. The difficulty with sync interop is that until the cross tx is settled all other tx of these logic zones cannot settle.</p>\n</li>\n<li>\n<p>for complex composition cases - I’d kind of say the opposite. The less the probability a tx is proven eventually,  the less leeway I think it should get in terms of proving time. Allowing people to create complex transactions that could potentially fail to settle is an attack vector.</p>\n</li>\n<li>\n<p>Generally speaking, different logic zones can have different timeouts, or even different tx of the same logic zones can have different timeouts, but it’s still imperative that these be globally known - everyone involved directly or indirectly must agree on whether a tx succeeded or failed.</p>\n</li>\n<li>\n<p>The world I personally dream of is a permissionless world where anyone can participate and open a new zkapp (possibly constrained to standard code infras to maintain security), but their ability to interact with other zkapps is automatically in correlation to their credibility in supplying proofs. i.e. if your zkapp was unable to provide a proof for its part in a cross tx, you would get less and less leeway (and possibly higher fees) every time it happens henceforth - and the opposite.</p>\n</li>\n</ol>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2025-05-27T11:08:29.570Z",
        "updated_at": "2025-05-27T11:10:19.251Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/3",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 566,
        "post_number": 1,
        "topic_id": 346,
        "topic_title": "KIP Draft: Threshold Address Format for KIP-10 Mutual Transactions",
        "topic_slug": "kip-draft-threshold-address-format-for-kip-10-mutual-transactions",
        "content": "<h2><a name=\"p-566-introduction-1\" class=\"anchor\" href=\"#p-566-introduction-1\"></a>Introduction</h2>\n<p>This proposal introduces a new address type for Kaspa that leverages a compact floating-point representation for threshold values. The Float24 format enables efficient encoding of threshold values within P2SH scripts, borrowing functionality while maintaining compatibility with existing address infrastructure.</p>\n<h2><a name=\"p-566-motivation-2\" class=\"anchor\" href=\"#p-566-motivation-2\"></a>Motivation</h2>\n<p>Current address formats don’t efficiently support threshold-based spending conditions as described in KIP-10. By encoding threshold values directly in addresses, we can:</p>\n<ul>\n<li>Support borrowing functionality where UTXOs can only be spent by increasing their value</li>\n<li>Provide a compact representation for a wide range of threshold values</li>\n<li>Maintain compatibility with existing P2SH infrastructure</li>\n</ul>\n<h2><a name=\"p-566-specification-3\" class=\"anchor\" href=\"#p-566-specification-3\"></a>Specification</h2>\n<h3><a name=\"p-566-address-version-4\" class=\"anchor\" href=\"#p-566-address-version-4\"></a>Address Version</h3>\n<p>We propose a new address version for threshold-based scripts:</p>\n<pre><code class=\"lang-auto\">Version::ThresholdScript = 9\n</code></pre>\n<h3><a name=\"p-566-address-structure-5\" class=\"anchor\" href=\"#p-566-address-structure-5\"></a>Address Structure</h3>\n<p>A threshold address consists of:</p>\n<ul>\n<li>Network prefix (e.g., “kaspa”, “kaspatest”)</li>\n<li>Version byte (9 for ThresholdScript)</li>\n<li>Address payload (35 bytes for ECDSA, 35 bytes for Schnorr)</li>\n</ul>\n<h3><a name=\"p-566-address-payload-format-6\" class=\"anchor\" href=\"#p-566-address-payload-format-6\"></a>Address Payload Format</h3>\n<p>The address payload consists of:</p>\n<ul>\n<li>Public key X coordinate (32 bytes)</li>\n<li>Float24 threshold value (3 bytes)</li>\n</ul>\n<p>The Float24 format uses 3 bytes (24 bits) with the following structure:</p>\n<pre><code class=\"lang-auto\">[1 bit: signature type] [1 bit: Y coordinate parity (ECDSA) or reserved (Schnorr)] [17 bits: mantissa] [5 bits: exponent]\n</code></pre>\n<p>Where:</p>\n<ul>\n<li><strong>Signature type bit</strong>: 0 for ECDSA, 1 for Schnorr</li>\n<li><strong>Y coordinate parity bit</strong>:\n<ul>\n<li>For ECDSA: 0 for even Y, 1 for odd Y</li>\n<li>For Schnorr: Always 0 (reserved for future use)</li>\n</ul>\n</li>\n<li><strong>Mantissa</strong>: 17-bit unsigned integer (0-131,071)</li>\n<li><strong>Exponent</strong>: 5-bit unsigned integer (0-31)</li>\n</ul>\n<p>The threshold value is calculated as:</p>\n<pre><code class=\"lang-auto\">threshold = mantissa * (2^exponent)\n</code></pre>\n<h3><a name=\"p-566-range-of-representable-values-7\" class=\"anchor\" href=\"#p-566-range-of-representable-values-7\"></a>Range of Representable Values</h3>\n<p>With this format, we can represent:</p>\n<ul>\n<li><strong>Minimum value</strong>: 0 * 2^0 = 0 sompi (allowing zero-threshold conditions)</li>\n<li><strong>Maximum value</strong>: 131,071 * 2^31 ≈ 281.4 trillion sompi</li>\n</ul>\n<p>This range easily covers most practical threshold values, though it falls short of the maximum possible Kaspa supply of 29 billion KAS (2.9 * 10^18 sompi).</p>\n<h2><a name=\"p-566-script-template-8\" class=\"anchor\" href=\"#p-566-script-template-8\"></a>Script Template</h2>\n<p>The address automatically generates a P2SH script with the following pattern:</p>\n<h3><a name=\"p-566-for-non-zero-thresholds-9\" class=\"anchor\" href=\"#p-566-for-non-zero-thresholds-9\"></a>For non-zero thresholds:</h3>\n<pre><code class=\"lang-auto\">OP_IF\n   &lt;pubkey&gt; OP_CHECKSIG[_ECDSA]\nOP_ELSE\n   OP_TXINPUTINDEX OP_TXINPUTSPK OP_TXINPUTINDEX OP_TXOUTPUTSPK OP_EQUALVERIFY\n   OP_TXINPUTINDEX OP_TXOUTPUTAMOUNT\n   &lt;threshold_value&gt; OP_SUB\n   OP_TXINPUTINDEX OP_TXINPUTAMOUNT\n   OP_GREATERTHANOREQUAL\nOP_ENDIF\n</code></pre>\n<h2><a name=\"p-566-examples-10\" class=\"anchor\" href=\"#p-566-examples-10\"></a>Examples</h2>\n<h3><a name=\"p-566-example-1-zero-threshold-with-schnorr-11\" class=\"anchor\" href=\"#p-566-example-1-zero-threshold-with-schnorr-11\"></a>Example 1: Zero Threshold with Schnorr</h3>\n<pre><code class=\"lang-auto\">Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes)\nSignature type: Schnorr (1)\nY coordinate bit: 0 (reserved for Schnorr)\nThreshold: 0 sompi\nFloat24: [1][0][00000000000000000][00000] (mantissa=0, exponent=0)\n</code></pre>\n<h3><a name=\"p-566-example-2-small-threshold-1024-sompi-with-schnorr-12\" class=\"anchor\" href=\"#p-566-example-2-small-threshold-1024-sompi-with-schnorr-12\"></a>Example 2: Small Threshold (1,024 sompi) with Schnorr</h3>\n<pre><code class=\"lang-auto\">Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes)\nSignature type: Schnorr (1)\nY coordinate bit: 0 (reserved for Schnorr)\nThreshold: 1,024 sompi\nFloat24: [1][0][00000000000000001][00010] (mantissa=1, exponent=10)\n</code></pre>\n<h3><a name=\"p-566-example-3-medium-threshold-100000-sompi-with-ecdsa-even-y-13\" class=\"anchor\" href=\"#p-566-example-3-medium-threshold-100000-sompi-with-ecdsa-even-y-13\"></a>Example 3: Medium Threshold (100,000 sompi) with ECDSA (Even Y)</h3>\n<pre><code class=\"lang-auto\">Public key X coordinate: ba01fc5f4e9d9879599c69a3dafdb835a7255e5f2e934e9322ecd3af190ab0f6 (32 bytes)\nSignature type: ECDSA (0)\nY coordinate bit: 0 (even Y)\nThreshold: 100,000 sompi\nFloat24: [0][0][00000000001100100][00000] (mantissa=100000, exponent=0)\n</code></pre>\n<h3><a name=\"p-566-example-4-medium-threshold-5242880-sompi-with-ecdsa-odd-y-14\" class=\"anchor\" href=\"#p-566-example-4-medium-threshold-5242880-sompi-with-ecdsa-odd-y-14\"></a>Example 4: Medium Threshold (5,242,880 sompi) with ECDSA (Odd Y)</h3>\n<pre><code class=\"lang-auto\">Public key X coordinate: ba01fc5f4e9d9879599c69a3dafdb835a7255e5f2e934e9322ecd3af190ab0f6 (32 bytes)\nSignature type: ECDSA (0)\nY coordinate bit: 1 (odd Y)\nThreshold: 5,242,880 sompi\nFloat24: [0][1][00000000000000101][00020] (mantissa=5, exponent=20)\n</code></pre>\n<h3><a name=\"p-566-example-5-large-threshold-1-million-kas-with-schnorr-15\" class=\"anchor\" href=\"#p-566-example-5-large-threshold-1-million-kas-with-schnorr-15\"></a>Example 5:  Large Threshold (~1 million KAS) with Schnorr</h3>\n<pre><code class=\"lang-auto\">Public key X coordinate: Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes)\nSignature type: Schnorr (1)\nY coordinate bit: 0 (reserved for Schnorr)\nThreshold: 99,999,547,392 sompi (~999,995 KAS)\nFloat24: [1][0][10111010010000111][10100] (mantissa=95367, exponent=20)\n</code></pre>\n<h2><a name=\"p-566-alternative-approaches-16\" class=\"anchor\" href=\"#p-566-alternative-approaches-16\"></a>Alternative Approaches</h2>\n<h3><a name=\"p-566-alternative-1-adjusting-bit-allocation-for-full-range-coverage-17\" class=\"anchor\" href=\"#p-566-alternative-1-adjusting-bit-allocation-for-full-range-coverage-17\"></a>Alternative 1: Adjusting Bit Allocation for Full Range Coverage</h3>\n<p>One alternative approach would be to adjust the bit allocation to provide coverage for the full range of possible Kaspa values. By moving 1 bit from the mantissa to the exponent, we could use:</p>\n<pre><code class=\"lang-auto\">[1 bit: signature type] [1 bit: Y coordinate parity] [16 bits: mantissa] [6 bits: exponent]\n</code></pre>\n<p>With this adjustment:</p>\n<ul>\n<li><strong>Mantissa</strong>: 16-bit unsigned integer (0-65,535)</li>\n<li><strong>Exponent</strong>: 6-bit unsigned integer (0-63)</li>\n</ul>\n<p>The maximum representable value would be:<br>\n65,535 * 2^63 ≈ 6.04 * 10^23 sompi</p>\n<p>This would easily cover the maximum Kaspa supply of 2.9 * 10^18 sompi with significant headroom. The tradeoff would be slightly less precision in the mantissa (16 bits instead of 17 bits), but 65,535 distinct mantissa values should still provide sufficient granularity for practical threshold purposes.</p>\n<h3><a name=\"p-566-alternative-2-non-power-of-2-exponent-base-18\" class=\"anchor\" href=\"#p-566-alternative-2-non-power-of-2-exponent-base-18\"></a>Alternative 2: Non-Power-of-2 Exponent Base</h3>\n<p>Another alternative would be to use a non-power-of-2 base for the exponent, such as 10:</p>\n<pre><code class=\"lang-auto\">threshold = mantissa * (10^exponent)\n</code></pre>\n<p>This would make the values more human-readable and potentially allow for more intuitive threshold settings. However, this approach has significant drawbacks:</p>\n<ul>\n<li><strong>Performance Penalties</strong>: Computing powers of 10 is more computationally expensive than powers of 2, which can be implemented as simple bit shifts.</li>\n<li><strong>Precision Issues</strong>: Powers of 10 cannot be represented exactly in binary, potentially leading to rounding errors.</li>\n</ul>\n<p>For these reasons, the power-of-2 approach is preferred for the Float24 format.</p>\n<h2><a name=\"p-566-benefits-19\" class=\"anchor\" href=\"#p-566-benefits-19\"></a>Benefits</h2>\n<ul>\n<li><strong>Practical Range</strong>: Float24 can represent values from 0 sompi to well beyond most practical threshold needs</li>\n<li><strong>Zero Threshold Support</strong>: Enables simple same-address spending without value constraints</li>\n<li><strong>Signature Type Choice</strong>: Allows selection between ECDSA and Schnorr signatures</li>\n<li><strong>Complete Public Key Information</strong>: Preserves Y coordinate parity for ECDSA keys</li>\n</ul>\n<h2><a name=\"p-566-implementation-considerations-20\" class=\"anchor\" href=\"#p-566-implementation-considerations-20\"></a>Implementation Considerations</h2>\n<ul>\n<li><strong>Bech32 Encoding</strong>: The complete 35-byte payload would be encoded using bech32</li>\n<li><strong>Script Generation</strong>: Wallet software would need to generate the appropriate script based on the Float24 value and signature type</li>\n<li><strong>Validation</strong>: Address validation would need to check the Float24 format is valid</li>\n<li><strong>Compatibility</strong>: Existing software would need updates to recognize and handle the new address type</li>\n</ul>\n<h2><a name=\"p-566-summary-21\" class=\"anchor\" href=\"#p-566-summary-21\"></a>Summary</h2>\n<p>The Float24 Threshold Address format provides an efficient way to encode threshold values directly in Kaspa addresses, enabling powerful compounding and borrowing functionality without requiring consensus changes. This proposal builds on the transaction introspection capabilities introduced in KIP-10 while maintaining compatibility with existing P2SH infrastructure.</p>\n<p>By standardizing this address format, we can enable a new class of applications that leverage threshold-based spending conditions, particularly for mining pools and other services that benefit from auto-compounding UTXOs. The support for both ECDSA and Schnorr signatures, along with a wide range of threshold values, provides flexibility for various financial applications.</p>",
        "raw_content": "",
        "author": "biryukovmaxim",
        "created_at": "2025-05-22T08:23:49.222Z",
        "updated_at": "2025-05-22T09:25:10.878Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/kip-draft-threshold-address-format-for-kip-10-mutual-transactions/346/1",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 567,
        "post_number": 2,
        "topic_id": 346,
        "topic_title": "KIP Draft: Threshold Address Format for KIP-10 Mutual Transactions",
        "topic_slug": "kip-draft-threshold-address-format-for-kip-10-mutual-transactions",
        "content": "<p>It’s important to note that the conversion from an address to a script public key (SPK) is a one-way process. Once an address is converted to an SPK, the original script cannot be recovered from the SPK hash alone unless all placeholder values are known.</p>",
        "raw_content": "",
        "author": "biryukovmaxim",
        "created_at": "2025-05-22T08:39:04.245Z",
        "updated_at": "2025-05-22T08:39:04.245Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/kip-draft-threshold-address-format-for-kip-10-mutual-transactions/346/2",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 543,
        "post_number": 1,
        "topic_id": 323,
        "topic_title": "A Basic Framework For Proofs Stitching",
        "topic_slug": "a-basic-framework-for-proofs-stitching",
        "content": "<h1><a name=\"p-543-introduction-1\" class=\"anchor\" href=\"#p-543-introduction-1\"></a>Introduction</h1>\n<p>In an atomic synchronous composability model, a single transaction may invoke execution in several logic zones (conceptually, smart contracts or rollups). A priori, synchronous composability ambitions appear to be at odds with <span class=\"math\">ZK</span> execution scaling—if provers and verifiers are forced to <span class=\"math\">ZK</span> prove and verify such involved transactions as a whole, the separation to distinct and independent logic zones will effectively collapse to a single monolithic design.</p>\n<p>This post outlines a proof of concept  for how provers of various logic zones can cumulatively create proofs for multi logic zone transactions, while each is only required to provide proofs limited to executions of code in their respective logic zone. This consists an initial but essential step to potentially prevent the aforementioned collapse.</p>\n<p>The main idea is stitching in a coherent manner several heterogenous <span class=\"math\">zk</span> proofs. In that regard, the framework bears some resemblance to works like <a href=\"https://eprint.iacr.org/2019/142\">legosnarks</a>, but unlike those, merely uses <span class=\"math\">Snark</span> in a black box manner.</p>\n<h1><a name=\"p-543-subtransactions-2\" class=\"anchor\" href=\"#p-543-subtransactions-2\"></a>Subtransactions</h1>\n<p>In our setting, multiple logic zones execute within a single transaction, sequentially calling on one another, where the initial logic zone and its input arguments are determined by the transaction’s contents. Each zone maintains its own internal state, isolated from others, which can neither read from nor write to it.</p>\n<p>The execution trace derived from the above can be broken into continuous segments I refer to as <em>subtransactions</em>. Each subtransaction represents a continuous block of execution in one logic zone and is executed in a strict sequence, with a call stack managing the flow between them.  To account for atomicity, the call stack is extended with a special element <span class=\"math\">\\bot</span>, that when pushed to the stack denotes that the transaction as a whole has failed.</p>\n<p>A subtransaction can be categorized by three elements:</p>\n<p><strong>Inputs:</strong></p>\n<ul>\n<li>The internal state of the logic zone executed, at the outset of the subtransaction.</li>\n<li>The input call stack, with the executed logic zone and the current local variables (possibly including a program counter) stored as a pair at the stack’s top. This call stack allows managing proper context switching between the subtransactions.</li>\n</ul>\n<p><strong>Outputs:</strong></p>\n<ul>\n<li>The updated internal state of the executed logic zone, after execution of the subtransaction.</li>\n<li>The output call stack, capturing any changes made to the call stack as a result of a call to a new logic zone (update of the top’s local variables, and a push of a new pair to the stack consisting of the called logic zone, and the local variables initialized by the call’s arguments) or a return from one (A pop of the current element of the stack, and an extension of the new top’s local variables with a new variable denoting the return value).</li>\n</ul>\n<p><strong>Ordinal Number:</strong></p>\n<p>A subtransaction is assigned an index representing its position in the overall sequence of execution. This explicit ordering will be useful for efficiently stitching subtransactions together.</p>\n<h1><a name=\"p-543-zero-knowledge-proof-messages-3\" class=\"anchor\" href=\"#p-543-zero-knowledge-proof-messages-3\"></a>Zero-Knowledge Proof Messages</h1>\n<p>A proof message is constructed as a tuple</p>\n<p><span class=\"math\">(index, inputs, outputs, \\pi)</span>,</p>\n<p>conceptually representing a subtransaction’s execution.</p>\n<p>Namely:</p>\n<ul>\n<li><strong>index:</strong> ordinal number <em>i</em> of the subtransaction.</li>\n<li><strong>inputs:</strong> The starting inputs, including the executed logic zone’s internal state and the input call stack.</li>\n<li><strong>outputs:</strong> The resulting outputs, including the updated internal state and the output call stack.</li>\n<li><strong><span class=\"math\">\\pi</span>:</strong> a zero-knowledge proof demonstrating that, given these inputs, a subtransaction  with index <span class=\"math\">i</span> correctly produces the specified outputs.</li>\n</ul>\n<p>The logic zone responsible for executing the subtransaction, as well as the next logic zone in line, are implicitly identified within the call stack data.</p>\n<p>Proofs can be  internally valid (i.e. <span class=\"math\">\\pi</span> verifies the subtransaction as stated correctly) even if they do not represent a computation that occurs at the transaction’s proper execution trace. Proofs hence need be treated as conditional, and not finalized until they are “stitched” (see below) together with others up to the transaction’s conclusion.</p>\n<h1><a name=\"p-543-proving-conditional-proofs-4\" class=\"anchor\" href=\"#p-543-proving-conditional-proofs-4\"></a>Proving Conditional Proofs</h1>\n<p>Provers each specialize in a designated logic zone and produce proofs for  its corresponding subtransactions.</p>\n<p>To generate a proof of a certain subtransaction’s execution, a prover needs to have the inputs of that subtransaction. These intermediary values may depend on the results of subtransactions in other logic zones. Provers could acquire the results of these intermediary computations, in one of two ways:</p>\n<ul>\n<li>Communicate intermediary results of subtransactions with each other via an offchain provers’ network.</li>\n<li>Execute (but not prove) subtransactions of other logic zones in order to derive the intermediary results they require by themselves.</li>\n</ul>\n<h1><a name=\"p-543-proofs-stitching-5\" class=\"anchor\" href=\"#p-543-proofs-stitching-5\"></a>Proofs Stitching</h1>\n<p>All proof messages, once submitted and verified, are stored in a public database. They are assumed to be submitted in an unordered manner. To finalize a transaction, a sequence of proofs need to be sequentially stitched with each other, in a manner such that the outputs and inputs are consistent, from the initial subtransaction and the global state on all associated logic zones at its inception, until either the stack is empty, or alternatively, a <span class=\"math\">\\bot</span> has been pushed to it.</p>\n<p>An implementation for stitching is given below.</p>\n<pre data-code-wrap=\"python\"><code class=\"lang-python\">class TransactionStitcher:\n    FAILURE_SIGN = '⊥'\n\n    class StitchedChain:\n        def __init__(self, initial_call_stack, initial_states):\n            \"\"\"\n            :param initial_call_stack: The initial call stack object. It must include a field\n                                       'currentLogicZone' that identifies the active logic zone.\n            :param initial_states: A dict mapping each logic zone to its internal state at transaction inception.\n            \"\"\"\n            self.proof_list = []  # List of proofs in forward order.\n            self.latest_writes = dict(initial_states)  # zone -&gt; internal state (latest updates)\n            self.current_call_stack = initial_call_stack  # The current call stack.\n\n    def __init__(self, initial_stack, initial_states):\n        \"\"\"\n        :param initial_stack: The initial call stack (an object with a field 'currentLogicZone').\n        :param initial_states: A dict mapping each logic zone to its initial internal state.\n        \"\"\"\n        # proofs_by_index maps an index to a dict:\n        # key: (internalState, callStack) from the proof's inputs, value: the proof.\n        self.proofs_by_index = {}\n        self.stitched_chain = self.StitchedChain(initial_stack, initial_states)\n        self.initial_states = dict(initial_states)  # For failure case.\n\n # --- Main Method ---\n\n    def submit_proof_message(self, proof):\n        \"\"\"\n        Processes a new publicly verified proof  by storing it by index and attempting\n        to extend the active forward-growing chain.\n        Outputs the result of the transaction if it can be fully stitched together\n\n        :param proof: A dictionary representing a proof with keys:\n            - 'index': int, the subtransaction's ordinal index.\n            - 'inputs': dict with keys:\n                  'callStack': an object with a field 'currentLogicZone',\n                  'internalState': the state of the active logic zone at the start.\n            - 'outputs': dict with keys:\n                  'callStack': an object (same structure as inputs['callStack']),\n                  'internalState': the updated state after execution.\n            - 'π': the zero-knowledge proof data (not used in stitching logic).\n            - 'callstack': an object representing the call stack; it must include a field\n                           'currentLogicZone'.\n        \"\"\"\n        self.store_proof(proof)\n\n        while True:\n            candidate = self.lookup_candidate()\n            if candidate:\n                self.extend_chain_with_candidate(candidate)\n                if self.is_candidate_ending(candidate):\n                    if self.is_failure(candidate['outputs']['callStack']):\n                        return self.initial_states  # Failure: return initial state.\n                    else:\n                        return self.stitched_chain.latest_writes  # Success: return latest writes.\n            else:\n                break\n        return None\n    # --- Helper Functions ---\n\n    def is_empty(self, call_stack):\n        \"\"\"\n        Determine if the call stack is empty.\n        (Implementation left out – should return True if call_stack represents an empty stack.)\n        \"\"\"\n        pass\n\n    def is_failure(self, call_stack):\n        \"\"\"\n        Determine if the call stack signals failure (i.e. contains FAILURE_SIGN as the logic zone at the top).\n        (Implementation left out.)\n        \"\"\"\n        pass\n    def is_candidate_ending(self, proof):\n        \"\"\"\n        Returns True if the proof's output call stack indicates termination:\n        either an empty call stack (success) or a failure signal.\n        \"\"\"\n        cs = proof['outputs']['callStack']\n        return self.is_empty(cs) or self.is_failure(cs)\n\n    def get_candidate_key(self, proof):\n        \"\"\"\n        Returns a tuple (internalState, callStack) from the proof's inputs for dictionary lookup.\n        \"\"\"\n        return (proof['inputs']['internalState'], proof['inputs']['callStack'])\n\n    def store_proof(self, proof):\n        \"\"\"\n        Stores the proof in the proofs_by_index dictionary.\n        \"\"\"\n        index = proof['index']\n        key = self.get_candidate_key(proof)\n        self.proofs_by_index.setdefault(index, {})[key] = proof\n\n    def lookup_candidate(self):\n        \"\"\"\n        Looks up and returns a candidate proof for the next index using the expected inputs,\n        or returns None if no candidate exists.\n        \"\"\"\n        active_zone = self.stitched_chain.current_call_stack.currentLogicZone\n        expected_state = self.stitched_chain.latest_writes.get(active_zone)\n        expected_stack = self.stitched_chain.current_call_stack\n        next_index = len(self.stitched_chain.proof_list) + 1\n        candidate_key = (expected_state, expected_stack)\n        return self.proofs_by_index.get(next_index, {}).get(candidate_key, None)\n\n    def extend_chain_with_candidate(self, candidate):\n        \"\"\"\n        Extends the stitched chain with the given candidate proof and updates the chain's state.\n        \"\"\"\n        self.stitched_chain.proof_list.append(candidate)\n        self.stitched_chain.current_call_stack = candidate['outputs']['callStack']\n        active_zone = self.stitched_chain.current_call_stack.currentLogicZone\n        self.stitched_chain.latest_writes[active_zone] = candidate['outputs']['internalState']\n\n   \n\n</code></pre>\n<h1><a name=\"p-543-final-notes-6\" class=\"anchor\" href=\"#p-543-final-notes-6\"></a>Final Notes</h1>\n<ol>\n<li>The precise identity of TransactionStitcher in the ecosystem remains to be finalized. It could be invoked within each logic zone separately, at a single stitching specialized logic zone, or even delegated to  the <span class=\"math\">L1</span>. Regardless, it must be aware of the standard used to encode data (inputs and outputs) in proof messages.</li>\n<li>Whole transactions could be proven ahead of time and stitched together in a similar manner. Transactions notably form a dependency DAG between themselves  according to their associated logic zones. Two branches of this DAG could potentially advance themselves independently of the other.</li>\n<li>Proof messages of the same logic zones will likely be submitted by provers in batches (likely even including messages from several transactions). Parsimonious encoding of these batches, and correspondingly decoding their contents, warrants more detailed discussion.</li>\n<li>A more general setting could consider parallelizable calls within the transaction. Such intra-transaction parallelism opens up various questions on data races prevention, determinism, and read and write permissions, which at the moment do not appear justified from a practical perspective.</li>\n<li>Challenge for the reader: does a proof message truly must commit to the index of the subtransaction? does it need the state of the stack in its entirety?</li>\n</ol>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2025-04-02T12:08:08.794Z",
        "updated_at": "2025-04-21T12:39:50.272Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/a-basic-framework-for-proofs-stitching/323/1",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 506,
        "post_number": 2,
        "topic_id": 279,
        "topic_title": "Crescendo Hardfork discussion thread",
        "topic_slug": "crescendo-hardfork-discussion-thread",
        "content": "<p>RE</p>\n<blockquote>\n<ul>\n<li>\n<ul>\n<li>The pruning depth formula provides a lower bound, yet the actual pruning period can be set longer. Plugging in the scaled parameters, the lower bound is calculated to be 627,258 blocks, representing approximately ~17.4238 hours. We suggest rounding this up to 30 hours for simplicity and practical application. A 30-hour period is closer to the current mainnet pruning period (~51 hours) and aligns closely with the value used and benchmarked throughout TN11 (~31 hours).</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p>I think setting it as some (integer) multiplication of finality depth (something like 3ϕ) can simplify pruning point calculation, since the difference between pruning points is at least the finality depth.</p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2025-02-03T16:41:51.158Z",
        "updated_at": "2025-02-03T16:41:51.158Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/2",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 520,
        "post_number": 4,
        "topic_id": 279,
        "topic_title": "Crescendo Hardfork discussion thread",
        "topic_slug": "crescendo-hardfork-discussion-thread",
        "content": "<p>Dear community - I propose to consider including <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0015.md\">KIP-15</a> to the Crescendo HF.</p>\n<p>The change is tiny (literally few lines of code) but still IMHO very powerful.</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-02-23T12:54:08.596Z",
        "updated_at": "2025-02-23T12:54:08.596Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/4",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 530,
        "post_number": 6,
        "topic_id": 279,
        "topic_title": "Crescendo Hardfork discussion thread",
        "topic_slug": "crescendo-hardfork-discussion-thread",
        "content": "<p>Being tested … Hope it will be ok</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-03-05T19:42:14.189Z",
        "updated_at": "2025-03-05T19:42:14.189Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/6",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 354,
        "post_number": 4,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p>This has been an interesting read so far. I’m still trying to catch up to be able to properly parse and understand the content fully. For now, I do have some preliminary questions based on my first reading here:</p>\n<ol>\n<li>\n<p>Entry/exit funds - will these use p2sh exclusively? Or p2pk for entry and exit but p2sh for state transitions?</p>\n</li>\n<li>\n<p>What does a state_commitment look like?</p>\n</li>\n<li>\n<p>Regarding the use of p2sh here referencing a single state commitment - does this mean that each state transition will necessarily use a different p2sh address each time? I’m wondering about the impact of this on services like the explorer who will maintain several address entries that will keep getting used just the one time.</p>\n</li>\n<li>\n<p>With the statement: “The proof is validated by L1, and the new state-commitment UTXO replaces the previous one in the UTXO set, recording the L2 state transition on L1.” - what happens during a re-org in L1? This will regularly happen around the tips. How is the L2 tolerant to such re-orgs?</p>\n</li>\n<li>\n<p>The post focuses on state transitions, but I’m curious about how much funds would actually move between each transaction from p2sh to new p2sh? Each tx will incur a fee also. What are your thoughts on these amounts moved/paid on the base layer as state transitions in the L2?</p>\n</li>\n</ol>",
        "raw_content": "",
        "author": "coderofstuff",
        "created_at": "2024-12-12T06:56:20.319Z",
        "updated_at": "2024-12-12T06:56:20.319Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/4",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 521,
        "post_number": 9,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<aside class=\"quote no-group\" data-username=\"michaelsutton\" data-post=\"1\" data-topic=\"208\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/michaelsutton/48/130_2.png\" class=\"avatar\"> michaelsutton:</div>\n<blockquote>\n<p><code>OpChainBlockHistoryRoot</code>: An opcode that provides access to the <code>ordered_history_merkle_root</code> field of a previous chain block. This opcode expects a block hash as an argument and fails if the block has been pruned (i.e., its depth has passed a threshold) or is not a chain-block from the perspective of the merging block executing the script. It will be used to supply valid anchor points to which the ZKP can prove execution.</p>\n</blockquote>\n</aside>\n<p>I want to emphasize the fact that this opcode is stateful, which breaks the assumption that a script validity is dependent only on the transaction itself and its previous UTXOs. This means that a transaction that spends a UTXO with such opcode might be invalidated without an intentional double spend, which might harm the UX of the recipient.</p>\n<p>This can be dealt by introducing a concept similar to coinbase maturity for such transactions, or developing an off-chain mechanism to help users identify such transactions (and dependent transactions) so they can set a higher confirmation time.</p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2025-02-23T15:17:32.886Z",
        "updated_at": "2025-02-23T15:17:32.886Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/9",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 519,
        "post_number": 1,
        "topic_id": 303,
        "topic_title": "KIP 15 discussion thread",
        "topic_slug": "kip-15-discussion-thread",
        "content": "<p>This is the discussion thread for <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0015.md\">KIP 15</a></p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2025-02-23T09:07:49.125Z",
        "updated_at": "2025-02-23T09:07:49.125Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/kip-15-discussion-thread/303/1",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 356,
        "post_number": 1,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>This post was deeply inspired by ideas shared with me by <a href=\"https://research.kas.pa/u/hashdag\">@hashdag</a>, <a href=\"https://research.kas.pa/u/michaelsutton\">@michaelsutton</a>, and  <a href=\"https://research.kas.pa/u/proof\">@proof</a>. It assumes a strong foundational understanding of ZK concepts and the workings of ZK-protocols. This blog post is far from being a detailed design proposal, but the core concept is accurately outlined.</p>\n<p>A previous <a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208\">post</a> by <a class=\"mention\" href=\"/u/michaelsutton\">@michaelsutton</a> outlined the basic L1&lt;–&gt;L2 interaction. Here I dive in further into the technical bits:</p>\n<ol>\n<li>About Proving System Choice for L1 zk verification</li>\n<li>L1 Enhancements / Additions<br>\na. Low level elliptic curve opcodes<br>\nb. L1 zk-verify function pseudo code as it should be implemented in Kaspa script<br>\nc. ZK-friendly hash function choice (partly finished, require separate blogpost)<br>\nd. UTXO ( input / output ) Anchoring via KIP10 (so-called canonical bridge)</li>\n<li>Possible L2 design examples<br>\na. Single proving system<br>\nb. zkVM base solution<br>\nc. Pseudocode for zk-SNARK Verification for Groth16</li>\n</ol>\n<p>Additionally, the general guideline for implementing L2 solutions on top of Kaspa L1 should prioritize <strong>minimizing</strong> L1 modifications, ensuring support for a wide range of L2 solutions, and maintaining a <strong>strong guarantee of bounded performance impact</strong> for zk-verification executed by L1 nodes.</p>\n<p>This post serves as an attempt and example of how zk-L2s can be supported with minimal changes to the BTC-like Kaspa script language and L1 architecture, while still offering flexibility for diverse implementation options.</p>\n<p>Terminology and concepts adopted from <a href=\"https://research.kas.pa/u/michaelsutton\">@michaelsutton</a> <a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208\">post</a><strong>:</strong></p>\n<ul>\n<li>Recursive DAG ordering commitments Merkle root</li>\n<li>State UTXO / State commitment UTXO</li>\n<li>L1 based rollups (zk)</li>\n<li>ZK-related opcodes (OpZkVerify)</li>\n</ul>\n<hr>\n<p><strong>1. About Proving System Choice for L1 zk verification</strong></p>\n<p><strong>Groth16</strong> / <strong>Plonk</strong> proving systems are proposed to be utilized due to their <em><strong>small proof size</strong></em> and <em><strong>constant-time verification</strong></em> complexity, which are <strong>critical</strong> for maintaining L1 performance.</p>\n<p><strong>Groth16/ Plonk</strong> proving systems are highly stable, with thoroughly debugged technology stacks and rigorously audited, well-established mathematics. Despite not being recent developments, they remain <strong>state-of-the-art</strong> in terms of <strong>proof size</strong>, <strong>verifier efficiency</strong>, and overall stability and security. Moreover, these proving systems are the only ones natively supported by major blockchain networks such as Ethereum.</p>\n<p><strong>Proving System zk-stacks &amp; stability:</strong> Codebases for zk-proving systems like <strong>Groth16</strong> and <strong>Plonk</strong> are mature, extensively tested, and battle-hardened across various blockchain ecosystems. This reduces the risk of bugs or implementation errors, ensuring a stable and secure integration.</p>\n<p><strong>Elliptic Curve Choice: Groth16</strong> / <strong>Plonk</strong> zk-proving systems are based on the <strong>BN254</strong> elliptic curve (a Barreto-Naehrig curve with a 254-bit prime field). While these systems can operate with other pairing-friendly elliptic curves, <strong>BN254</strong> offers a security level of approximately <strong>100 bits</strong>, which is considered sufficient for many practical applications. Additionally, it is already supported by Ethereum-like chains, making it a strong candidate for adoption.</p>\n<hr>\n<p><strong>2. L1 Enhancements / Additions:</strong></p>\n<p>To enable zk-proof verification and interaction between L1 and L2, the following enhancements are proposed:</p>\n<p><strong>2.a. Low level EC operations:</strong></p>\n<p><strong>Additions to BTC like Kaspa-script language:</strong></p>\n<ol>\n<li><strong>ADD</strong>: Modular addition for elliptic curve field arithmetic.</li>\n<li><strong>MUL</strong>: Modular multiplication for elliptic curve field arithmetic and scalar multiplications.</li>\n<li><strong>PAIRING</strong>: Executes elliptic curve pairing operations, required for verifying Groth16 / Plonk proofs.</li>\n</ol>\n<p><strong>Source of Opcode Implementations:</strong></p>\n<ol>\n<li>Implementations for `ADD`, `MUL`, and `PAIRING` opcodes will be derived from the stable and widely adopted blockchains, such as Ethereum. Most stable implementations are <strong>zCash</strong>, <strong>Geth</strong> (and maybe <strong>reth</strong>)</li>\n<li>Ethereum’s opcode implementations for <strong>BN254</strong> are well-tested and optimized for performance, providing a reliable foundation.</li>\n</ol>\n<p><strong>2.b. Details about Groth16 / Plonk ZK-Verify-Function (executed by L1 nodes)</strong>:</p>\n<p>The verify function will be implemented directly in terms of the BTC-like Kaspa scripting language, along with the verification key and state hash. The exact definition of public parameters can vary based on the application, but this design ensures flexibility and trustless execution across use cases.</p>\n<p>The verification process requires only two public inputs <strong>***</strong> enabling support for practically any desired logic. The verify function can be defined with four arguments:</p>\n<ul>\n<li>Verification Key: Less than 1kB.</li>\n<li>Public Parameter 1: 256-bit.</li>\n<li>Public Parameter 2: 256-bit.</li>\n<li>zk-Proof: Less than 1kB.</li>\n</ul>\n<p>The verification itself involves dozens of elliptic curve (EC) operations (ADD, MUL, and PAIR) and can be efficiently implemented within a Bitcoin-like Kaspa script if these operations are supported as was proposed earlier in this post.</p>\n<p><strong>***</strong> In general, more than two public inputs can be supported; however, the number should always be bounded, as the verification complexity increases linearly with the size of the public inputs.</p>\n<p><strong>2.c. ZK-friendly hash function choice (partly finished, require separate blogpost)</strong></p>\n<p>L1 must provide additional support for zk-based L2 systems to prove transaction ordering as observed by L1. This ensures consistency and enables verifiable state updates. zk-L2s will leverage Merkle Inclusion Proofs, using Merkle Roots computed by L1 to verify transaction inclusion and ordering without requiring direct access to the complete transaction history during L1 verification.</p>\n<p>L1 will construct a Merkle Tree, where each leaf corresponds to an individual transaction. The Merkle Root will represent the set of all transactions in a block, and will be computed by L1 with a zk-friendly hash function (e.g., Poseidon2, Blake2s, or Blake3) ensuring compatibility with zk systems. This structure is referred to as <strong>Recursive DAG ordering commitments</strong> in a previous <a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208\">post</a> by <a href=\"https://research.kas.pa/u/michaelsutton\">@michaelsutton</a></p>\n<p><strong>Hash Function Considerations</strong></p>\n<ul>\n<li><strong>Efficiency</strong>: L1 requires a highly efficient hash function optimized for standard CPUs, as it must be executed by all Kaspa nodes.</li>\n<li><strong>zk-Friendliness</strong>: L2 requires zk-friendly hash functions to compute inclusion proofs for each L2 transaction during proof generation.</li>\n<li><strong>Security</strong>: The selected hash function must ensure robust security across both L1 and L2.</li>\n</ul>\n<p>For example, while the Blake2s/Blake3 family of hashes is field-agnostic and suitable for L1 (cpu friendly), it is significantly less performant than zk-friendly options like Poseidon2, which offers better compatibility with zk-proofs but is finate field dependent. Blake2s / Blake3 zk performance currently about at least 20 times slower than this of Poseidon2, moreover major zkVMs should be extended with Blake hash as its precompile circuit that is not a big problem but still should be done accurately and properly audited ( which can delay mainnet L2s )</p>\n<p>Most major zkVMs (such as SP1, CairoVM, Risc0, Jolt, and Nexus) would / should be compatible with the proposed choice of hash function. “Compatible” means prove generation performance or number of circuit constraints per hash per byte of hashed data etc …</p>\n<p>An alternative approach could involve selecting a hash function optimized for Groth16 / Plonk, but this comes with at least two significant drawbacks:</p>\n<ol>\n<li><strong>Fixed Tree Size</strong>: Groth16 and Plonk require a fixed tree size. While this limitation is manageable, it imposes a strict proving interval, meaning the number of transactions in the tree must remain constant for single zk-proof.</li>\n<li><strong>Performance Impact</strong>: A more critical issue arises because transaction order and inclusion would not be proven by the zkVM itself. Instead, these transactions would become public inputs for the second-stage proving system (i.e., the zkVM verifier implemented using Groth16/Plonk). This approach results in increased zk-proof generation times, negatively impacting performance.</li>\n</ol>\n<p>The relationship between hash function choice, cpu vs zk performance and the dependency on finite field will be explored in a future blog post.</p>\n<p><strong>Sub-Nets Support</strong></p>\n<p>To enhance scalability and parallel processing, transactions can be categorized into sub-nets. The Merkle Tree’s leftmost leaf will store the Merkle Root of the previous block and its merge-set (all transactions), ensuring consistent chain linkage.</p>\n<p><strong>2.d. UTXO ( input / output ) Anchoring via KIP10 (so-called canonical bridge)</strong></p>\n<p>By utilizing KIP10 introspection opcodes, UTXO-input and UTXO-output anchoring can be incorporated into the spending script. This mechanism enables any party to provide a valid zk-proof to spend a State UTXO, ensuring that the newly created UTXO complies fully with the rules defined by the L2 state transition logic. It guarantees verifiability and security of L2 state transitions in a fully trustless manner, as KIP10 enforces the creation of specific UTXO-outputs when spending UTXO-inputs.</p>\n<p>This topic will be explored in greater detail in a separate blog post. For now, it suffices to say that KIP10 facilitates the linking of UTXO-inputs and UTXO-outputs within the same transaction through commitments.</p>\n<p><strong>Summary for L1 additions:</strong></p>\n<ol>\n<li>Math OpCodes: <strong>ADD</strong>, <strong>MUL</strong>, <strong>PAIR</strong> - allow implementation of zk-verifier (aka <strong>OpZkVerify</strong>)</li>\n<li>Merkle-Tree structure with zk-friendly hash function - allow TX inclusion proofs and ordering during state transaction (<strong>open question:</strong> hash function choice)</li>\n<li><strong>Open Question:</strong> A dedicated opcode for constructing a Merkle root from public inputs may be required—this will be addressed separately. In any case, the aim is to minimize additions to L1. The current Bitcoin-like Kaspa script capabilities are sufficient to support basic hash composition of public inputs. However, for more advanced schemes, L1 may need to provide additional support in the future.</li>\n</ol>\n<hr>\n<p><strong>3. Possible L2 Implementation examples</strong></p>\n<p><strong>How to Store L2 State in L1 UTXO</strong></p>\n<p>The ScriptPubKey will store a hash representing a specific verification function, including its verification key and the L2 state hash. Zk-proof public parameters act as commitments to the previous and next states, dag recursive commitments and output UTXO (and maybe input UTXO), ensuring that the spending process is fully trustless.</p>\n<p>The <strong>ScriptPubKey</strong> field in L1 UTXOs will store the following information related to L2 state:</p>\n<ol>\n<li><strong>Recursive DAG ordering commitments Merkle root</strong></li>\n<li><strong>State-Hash</strong>: Represents the Merkle root of the L2 state</li>\n<li><strong>Program-Hash</strong>: Specifies the zk-circuit or state transition program.</li>\n<li><strong>Verification-Key</strong>: Enables zk-proof verification on L1</li>\n<li><strong>ZK-Verification code</strong></li>\n</ol>\n<p><strong>3.a. Example of a Single zk-proof system:</strong></p>\n<p>State transition programs can be implemented directly using Groth16 or Plonk proving systems. This approach is well-suited for smaller, predefined, or existing solutions, for example <strong>zCash</strong> ZK core is implemented in terms of this proving system and at least theoretically can be deployed as is.<br>\nIn such cases, a single zk-proof is generated and verified on-chain.</p>\n<p><strong>3.b Example of a zkVM based solution:</strong></p>\n<p>Another potential implementation for L2 is a zkVM-generated proof of state transitions. A zkVM enables the efficient execution of arbitrary smart contracts or virtual machine code (e.g., EVM, SVM) with state transitions verified through zk-proofs. This approach ensures program correctness while maintaining efficiency in proof generation and verification.<br>\nLeveraging zkVM technology offers significant development flexibility and numerous essential features. Currently, zkVMs represent the dominant approach in the industry. They are continually improving, becoming more user- and developer-friendly, as well as increasingly efficient. Furthermore, two-stage proving can be seamlessly extended to N-stage proving, as zkVMs are capable of supporting additional zk-schema implementations on top of their existing framework.</p>\n<p><strong>zkVM Codebase Compatibility:</strong> This approach enables integration with major <strong>zkVM</strong> codebases, including <strong>Risc0</strong>, <strong>SP1</strong>, and <strong>CairoVM</strong> and others, ensuring broad interoperability and ecosystem growth.</p>\n<p><strong>Example of using 2 zk-proof systems (zkVM→Groth16/Plonk):</strong></p>\n<ol>\n<li><strong>zkVM (e.g., Risc0, CairoVM, SP1)</strong>: This system will prove the execution of a state transition program (e.g., EVM) alongside transaction ordering and inclusion proofs.</li>\n<li><strong>Groth16 / Plonk</strong>: This proving system will implement a zk-verifier for the zkVM. zkVM proof will be feeded inside Groth16 / Plonk circuit that implement zkVM verifier. This will generate another zk-proof that going to be verified on-chain by L1. It will generate a zk-proof that is sent on-chain and verified by all Kaspa L1 nodes.</li>\n</ol>\n<p><strong>3.c. Pseudocode for zk-SNARK Verification for Groth16.</strong></p>\n<pre><code class=\"lang-auto\">Constants\n\n- PRIME_Q (256 bits, 32 bytes): Prime field size for alt_bn128 (aka BN254) elliptic curve.\nData Structures \n\n1. G1Point (512 bits, 64 bytes) - Represents a point on the curve with:\n     - x (256 bits, 32 bytes).\n     - y (256 bits, 32 bytes).\n2. G2Point (1024 bits, 128 bytes):\n   - Represents a point on the extension field with:\n     - x[0], x[1] (each 256 bits, 32 bytes).\n     - y[0], y[1] (each 256 bits, 32 bytes).\n3. VerifyingKey (640 bytes):\n   - alfa1 (G1Point, 64 bytes).\n   - beta2 (G2Point, 128 bytes).\n   - gamma2 (G2Point, 128 bytes).\n   - delta2 (G2Point, 128 bytes).\n   - ic (array of G1Points, size depends on the number of public inputs + 1; each G1Point = 64 bytes).\n4. SnarkProof (192 bytes):\n   - a (G1Point, 64 bytes).\n   - b (G2Point, 128 bytes).\n   - c (G1Point, 64 bytes).\nElliptic Curve Math\n\n1. P1 and P2:\n   - P1 (512 bits, 64 bytes): Generator point for G1.\n   - P2 (1024 bits, 128 bytes): Generator point for G2.\n2. Negate Point:\n   - Input: G1Point (64 bytes).\n   - Output: Negated G1Point (64 bytes).\n3. Addition:\n   - Inputs: Two G1Point values (each 64 bytes).\n   - Output: Resultant G1Point (64 bytes).\n4. Scalar Multiplication:\n   - Inputs: G1Point (64 bytes), scalar s (256 bits, 32 bytes).\n   - Output: Scaled G1Point (64 bytes).\n5. Pairing:\n   - Inputs: Arrays of G1Point and G2Point (each G1Point = 64 bytes, each G2Point = 128 bytes).\n   - Output: Boolean (1 byte).\n6. Pairing Check:\n   - Input: Four pairs of G1Point and G2Point (4 x 64 bytes + 4 x 128 bytes = 768 bytes).\n   - Output: Boolean (1 byte).\n\nVerification Logic (logic that will be supported directly by L1)\n\n1. Verify Function:\n   - Inputs:\n     - vk (Verification Key, 640 bytes).\n     - input (Array of public inputs, each 256 bits, 32 bytes; total size = number of inputs × 32 bytes).\n     - proof (SNARK proof, 192 bytes).\n   - Steps:\n     1. Input Validation:\n        - Ensure input.length + 1 == vk.ic.length.\n        - Ensure all input[i] values are less than FIELD_SIZE (256 bits, 32 bytes).\n     2. Compute vk_x:\n        - Start with vk_x as G1Point (0, 0) (64 bytes).\n        - For each public input:\n          - Scale vk.ic[i + 1] (64 bytes) by input[i] (32 bytes) using scalar_mul.\n          - Add the result to vk_x (64 bytes) using addition.\n        - Add vk.ic[0] (64 bytes) to finalize vk_x.\n     3. Pairing Check:\n        - Use pairingProd4 (extended pairing function for 4 arguments) to verify the pairing:\n          - Inputs: Negated proof a (64 bytes), proof b (128 bytes), vk.alfa1 (64 bytes), vk.beta2 (128 bytes), vk_x        (64 bytes), vk.gamma2 (128 bytes), proof c (64 bytes), and vk.delta2 (128 bytes).\n          - Output: Boolean (1 byte).\n   - Output:\n     - Returns true (1 byte) if the proof is valid, false (1 byte) otherwise.\n\n\nSummary of Sizes\n\n1. Key Sizes:\n   - Verification Key: 640 bytes. (example for 2 public inputs)\n   - Public Inputs: Each 256 bits, 32 bytes.\n   - SNARK Proof: 192 bytes.\n2. Operations:\n   - Addition (G1Point): Input = 64 bytes × 2, Output = 64 bytes.\n   - Scalar Multiplication (G1Point): Input = 64 bytes + 32 bytes, Output = 64 bytes.\n   - Pairing Check: Input = 768 bytes, Output = 1 byte.\n</code></pre>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2024-12-12T14:44:57.329Z",
        "updated_at": "2024-12-12T20:46:09.126Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/1",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 390,
        "post_number": 3,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p><strong>Re sufficiently safe:</strong></p>\n<p>Each of zk-friendly hashes should be addressed separately. For example Poseidons family functions, are widely used by zk community in production projects and by zk-community it considered sufficiently safe, but by wider cryptographers / research / cryptanalysts  community, the question reminds open. I think for proper answer to this question we need help of <a class=\"mention\" href=\"/u/proof\">@proof</a> &amp; Elihai. And if my understanding is correct, re Blake family hash functions are considered to be safe !</p>\n<hr>\n<p><strong>Re zkp system variant would require choosing a different hash function:</strong></p>\n<p>The short answer, most probably yes. Slightly more unfolded answer: for example for hash function like Poseidon, which is finite field dependent, it is clear that each zk system will prefer that chosen Poseidon hash to be based on its (zk-system) native finite field. This way the performance will be better since it will be highly aligned with specific proving system. This is one of the reasons we need to choose it carefully. I will try to address these things in separate post.<br>\nBlake family on the other hand can be presumably “good” for all / most of existing zkp systems, but not the best. Other perspective is the CPU friendliness, which is good for Kaspa-L1, that should be considered as well.</p>\n<hr>\n<p><strong>To summarize:</strong> the final choice is multi dimensional and part of the requirements contradict with each other.</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2024-12-16T11:10:30.362Z",
        "updated_at": "2024-12-16T11:12:27.002Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/3",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 405,
        "post_number": 8,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Here are benchmarks (CPU not ZK) for Blake3 &amp; Poseidon2. Apple MacBook Pro M4, single threaded.</p>\n<ul>\n<li>chatgpt only helped to organise the results in table <img src=\"https://research.kas.pa/images/emoji/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://research.kas.pa/uploads/default/original/1X/edcd51bc3ffc1bb0ef0f4e44401d6009ea65b3ce.jpeg\" data-download-href=\"https://research.kas.pa/uploads/default/edcd51bc3ffc1bb0ef0f4e44401d6009ea65b3ce\" title=\"telegram-cloud-photo-size-4-5999079653719000034-y\"><img src=\"https://research.kas.pa/uploads/default/optimized/1X/edcd51bc3ffc1bb0ef0f4e44401d6009ea65b3ce_2_642x499.jpeg\" alt=\"telegram-cloud-photo-size-4-5999079653719000034-y\" data-base62-sha1=\"xVH3pvm2kvGdF83xCNmztpXQK1E\" width=\"642\" height=\"499\" srcset=\"https://research.kas.pa/uploads/default/optimized/1X/edcd51bc3ffc1bb0ef0f4e44401d6009ea65b3ce_2_642x499.jpeg, https://research.kas.pa/uploads/default/optimized/1X/edcd51bc3ffc1bb0ef0f4e44401d6009ea65b3ce_2_963x748.jpeg 1.5x, https://research.kas.pa/uploads/default/original/1X/edcd51bc3ffc1bb0ef0f4e44401d6009ea65b3ce.jpeg 2x\" data-dominant-color=\"2F2F2F\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">telegram-cloud-photo-size-4-5999079653719000034-y</span><span class=\"informations\">1071×833 93.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<hr>\n<p>Few comments:</p>\n<ol>\n<li>Blake3 perf stays the same for 32 bytes and 64 bytes input since pre-image length of Blake3 is 64 bytes</li>\n<li>Small input as 64bytes is still important for Kaspa L1 since during Merkle Tree construction ~1/2 hashes will be with 64bytes input</li>\n<li>For Poseidon2 bench I used Plonky3 lib.</li>\n<li>Blake3 is blake3 v1.5.3 (official rust implementation)</li>\n</ol>\n<p>These are preliminary results, that maybe good enough for discussion but still far from being extremely accurate.</p>\n<p>Question:<br>\n<a class=\"mention\" href=\"/u/proof\">@proof</a> - do I understand it correctly that number of AIR constraints for Blake3 for n-rounds = 7, is 2976 + 900 ~= 4k ?</p>\n<p>*taken from the link provided by you.</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2024-12-23T20:47:25.975Z",
        "updated_at": "2024-12-24T08:27:34.606Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/8",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 406,
        "post_number": 9,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Am I correct in my understanding that groth and plonk will both require a trusted set-up phase for the L2s?</p>\n<p>I consider it desirable to allow L2 to avoid trusted set ups if they so choose, even if it comes at the price of something else.  Do the suggested op codes allow for Starks and (best) transparent snargs to be implemented?</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2024-12-24T13:43:39.350Z",
        "updated_at": "2024-12-24T13:43:39.350Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/9",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 413,
        "post_number": 10,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p><a class=\"mention\" href=\"/u/proof\">@proof</a></p>\n<p>In continuation to our conversation, could you describe here the different methods you see in which Starks can be integrated?</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2025-01-04T21:46:52.348Z",
        "updated_at": "2025-01-04T21:46:52.348Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/10",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 465,
        "post_number": 12,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Indeed both Plonk &amp; Groth16 require trusted-setup ceremony, but not for L2s. For each version of <code>zkVMs</code> in case of chain of 2 proof systems. If L2 decides to implement its logic directly in terms of Plonk/Groth16 then yes.</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-20T12:09:43.679Z",
        "updated_at": "2025-01-20T20:35:27.112Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/12",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 466,
        "post_number": 13,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>We’re implementing the EC curve ops which will allow sufficient flexibility without direct need for hard-forks.</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-20T12:12:37.520Z",
        "updated_at": "2025-01-20T12:12:37.520Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/13",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 482,
        "post_number": 16,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Not in current construction. Any <code>zk-verify function</code> (hash-bases, EC) can be implemented in terms of EC-based Math-Opcodes ( Groth16 / Plonk).</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-24T11:57:11.071Z",
        "updated_at": "2025-01-24T11:57:11.071Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/16",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 487,
        "post_number": 18,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Dear <a class=\"mention\" href=\"/u/supermainnet\">@superMainnet</a> - can I kindly ask you to re-read the whole blogpost ?</p>\n<p><strong>Key Points to Note</strong></p>\n<p>A <strong>chain of zk proofs</strong> (<strong>STARK/any other new proof system</strong> → <strong>SNARK</strong>) leverages the best of both worlds:</p>\n<p>• <strong>STARK</strong> for efficient, transparent proof generation of large-scale computations.</p>\n<p>• <strong>SNARK</strong> for cost-effective on-chain verification (small proof size, constant-time checks).</p>\n<p>This architecture is increasingly discussed in modern zero-knowledge ecosystems, as it alleviates the on-chain verification bottleneck of large STARK proofs while still enjoying the transparency and scalability that STARKs offer off-chain.</p>\n<ol>\n<li><strong>Proof Size</strong></li>\n</ol>\n<ul>\n<li><strong>STARK</strong> proofs grow (roughly) <strong>polylogarithmically/log</strong> with circuit size, which is still quite efficient for large n (especially with continual optimizations).</li>\n<li><strong>Groth16</strong> has some of the <em>smallest</em> proof sizes (~128–192 bytes) and <em>constant</em> verification time, but each circuit needs its own trusted setup.</li>\n<li><strong>PLONK</strong> typically produces a <em>constant-size proof</em> — often a few hundred bytes. This is larger than Groth16’s but still considered “small” in absolute terms.</li>\n</ul>\n<ol start=\"2\">\n<li><strong>Verification Complexity</strong></li>\n</ol>\n<ul>\n<li><strong>STARK</strong> verification is generally O(log n) or a small polynomial in log n.</li>\n<li><strong>Groth16</strong> verification is <em>constant</em> (independent of n)—only a handful of pairings (e.g., 3 pairings total).</li>\n<li><strong>PLONK</strong> also has a <em>constant</em> number of pairings/polynomial checks and is therefore <em>constant time</em> in circuit size.</li>\n</ul>\n<ol start=\"3\">\n<li><strong>Trusted Setup</strong></li>\n</ol>\n<ul>\n<li><strong>STARK</strong> systems avoid a trusted setup entirely (fully transparent).</li>\n<li><strong>Groth16</strong> requires a <em>circuit-specific</em> setup (each new circuit needs a new setup).</li>\n<li><strong>PLONK</strong> uses a universal (or updatable) setup that can be reused across many different circuits.</li>\n</ul>\n<ol start=\"4\">\n<li><strong>Security Assumptions</strong></li>\n</ol>\n<ul>\n<li>\n<p><strong>STARK</strong>: Relies on collision-resistant hash functions and no heavy number-theoretic assumptions—often touted as plausibly or partially <em>post-quantum</em> secure, though current standardization is still evolving.</p>\n</li>\n<li>\n<p><strong>Groth16</strong> and <strong>PLONK</strong>: Rely on elliptic-curve pairings and discrete-log assumptions, which are <em>not</em> believed to be secure against large-scale quantum computers.</p>\n</li>\n</ul>\n<p>In summary, <strong>STARKs</strong> offer transparency and good asymptotic scalability but come with larger proof sizes. <strong>Groth16</strong> remains popular for minimal proof sizes and fastest verification, while <strong>PLONK</strong> offers a middle ground of small (but slightly larger) proofs, constant verification, and a single universal trusted setup that simplifies deployment across many circuits.</p>\n<p><strong>Observations</strong></p>\n<ol>\n<li><strong>Tens to Hundreds of Kilobytes</strong></li>\n</ol>\n<p>• Real-world STARK proofs often land in the <strong>tens/handreds of kilobytes</strong> range for moderate-scale applications (batching thousands of trades, or verifying mid-sized programs).</p>\n<p>• Even not extremely large circuits (millions of constraints) can push proofs into the **hundreds of kilobytes / megabytes **.</p>\n<ol start=\"2\">\n<li><strong>Ongoing Optimizations</strong></li>\n</ol>\n<p>• Projects like StarkWare continue to optimize the FRI protocol, polynomial commitment schemes, and Cairo’s AIR (algebraic intermediate representation). These improvements may reduce proof sizes further or keep them manageable as circuit complexity grows.</p>\n<ol start=\"3\">\n<li><strong>Comparison to Groth16 / PLONK</strong></li>\n</ol>\n<p>• STARK proofs are significantly larger than the <em>constant-sized</em> proofs typical of Groth16 or PLONK (a few hundred bytes to sub-kilobyte).</p>\n<p>• In exchange, STARKs offer transparency (no trusted setup) and have good scaling properties for very large circuits.</p>\n<p><a href=\"https://vac.dev/rlog/zkVM-testing/?utm_source=substack&amp;utm_medium=email\">Here</a> is a recent comparison for zkVMs ( SP1, risc0 are stark-based ).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://research.kas.pa/uploads/default/original/1X/0f7ab920377ee95ba9b38bc763ca2215e951d5d3.png\" data-download-href=\"https://research.kas.pa/uploads/default/0f7ab920377ee95ba9b38bc763ca2215e951d5d3\" title=\"image\"><img src=\"https://research.kas.pa/uploads/default/optimized/1X/0f7ab920377ee95ba9b38bc763ca2215e951d5d3_2_443x500.png\" alt=\"image\" data-base62-sha1=\"2cW5TU2Zl69LgwoVCMbbYDtVekr\" width=\"443\" height=\"500\" srcset=\"https://research.kas.pa/uploads/default/optimized/1X/0f7ab920377ee95ba9b38bc763ca2215e951d5d3_2_443x500.png, https://research.kas.pa/uploads/default/optimized/1X/0f7ab920377ee95ba9b38bc763ca2215e951d5d3_2_664x750.png 1.5x, https://research.kas.pa/uploads/default/optimized/1X/0f7ab920377ee95ba9b38bc763ca2215e951d5d3_2_886x1000.png 2x\" data-dominant-color=\"090909\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">887×1001 56.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>In addition, stark-based verifiers are not easily implementable with BTC-like kaspa scripting language, hence:</p>\n<ol>\n<li>Each version of such verifier per specific version of zk-VM requires a hard-fork</li>\n<li>Kaspa L1 should commit upfront to specific zkVMs which is not the case with basic EC math op-codes. Math op-codes are not in any sense are equal to this.</li>\n<li>zkVM codebases are extremely complex and new, with ton of bugs inside and <strong>hundreds of thousands</strong> lines of code. They are <strong>safe</strong> in a way that people put real money in these L2s / zk-rollups but solid chains like Ethereum still are far from accepting it inside ( requirement is at least formal verification of code vs math )</li>\n<li>And many more other … “buts” … <img src=\"https://research.kas.pa/images/emoji/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></li>\n</ol>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-26T09:45:38.766Z",
        "updated_at": "2025-01-26T09:49:52.461Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/18",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 488,
        "post_number": 19,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Just to put here for the reference EIPs related to these math-ops:</p>\n<ol>\n<li><a href=\"https://eips.ethereum.org/EIPS/eip-196\" class=\"inline-onebox\">EIP-196: Precompiled contracts for addition and scalar multiplication on the elliptic curve alt_bn128</a></li>\n<li><a href=\"https://eips.ethereum.org/EIPS/eip-197\" class=\"inline-onebox\">EIP-197: Precompiled contracts for optimal ate pairing check on the elliptic curve alt_bn128</a></li>\n<li><a href=\"https://eips.ethereum.org/EIPS/eip-1108\" class=\"inline-onebox\">EIP-1108: Reduce alt_bn128 precompile gas costs</a></li>\n</ol>\n<p>Here is <code>in review</code> status of EC ops related to BLS12-381: <a href=\"https://eip.directory/eips/eip-2537\" class=\"inline-onebox\">EIP-2537: Precompile for BLS12-381 curve operations</a></p>\n<p>Here is a ton of information about ECs accepted in Ethereum, why and how and why others were rejected …</p>\n<ol>\n<li><a href=\"https://ethresear.ch\">https://ethresear.ch</a></li>\n<li><a href=\"https://ethereum-magicians.org\">https://ethereum-magicians.org</a></li>\n</ol>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-26T13:21:57.998Z",
        "updated_at": "2025-01-26T13:34:47.364Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/19",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 505,
        "post_number": 1,
        "topic_id": 294,
        "topic_title": "Sync Monitor / Mining Rule Engine",
        "topic_slug": "sync-monitor-mining-rule-engine",
        "content": "<h1><a name=\"p-505-introduction-1\" class=\"anchor\" href=\"#p-505-introduction-1\"></a>Introduction</h1>\n<p>This forum post is a summary of some of the discussions with <a class=\"mention\" href=\"/u/michaelsutton\">@michaelsutton</a> <a class=\"mention\" href=\"/u/hashdag\">@hashdag</a> and <a class=\"mention\" href=\"/u/someone235\">@someone235</a> which can be found at <a href=\"https://docs.google.com/document/d/1lpZrQWfecKuaVcpLOLXH2R12Uzq3Sjhtyi4IhrIcQmg/edit\" class=\"inline-onebox\">Sync Monitor / Mining Rule Engine - Google Docs</a> and <a href=\"https://t.me/kasparnd/1892\" class=\"inline-onebox\">Telegram: Contact @kasparnd</a></p>\n<p>Here we describe two pieces of related work:</p>\n<ul>\n<li>Creating a sync monitor/manager such that nodes can continue mining during a network halt under certain good conditions. This monitor answers the question “Should the node mine?”</li>\n<li>Creating a mining rule engine that allows mining behavior to adjust to conditions of the node or the network. This rule engine answers the question “How should this node mine?”</li>\n</ul>\n<h1><a name=\"p-505-sync-monitor-2\" class=\"anchor\" href=\"#p-505-sync-monitor-2\"></a>Sync Monitor</h1>\n<p>This monitor determines whether a node should mine given its observable current state. The rusty kaspa node allows for mining if (1) the node has at least one peer and if (2) the sink is recent. These conditions are sufficient for when the network is operating normally - that is, the network is producing blocks.</p>\n<p>What happens if network mining halts for a significant period of time to the point all nodes fail the “is sink recent?” condition? To recover from this halted state, there will have to be some manual intervention for mining to continue.</p>\n<p>We know the network is operating normally if it is producing blocks. We also know what the expected number of blocks there are per second (1 block per second in mainnet, but soon to become 10 BPS). Using these two bits of information, we can determine just how fast a node’s sync rate is. Specifically:</p>\n<p>sync_rate = (observed_blocks_per_second) / (expected_blocks_per_second)</p>\n<p>We can formally define that the “network is operating normally” if the sync_rate is above some threshold. If it falls below this threshold, then regardless of the recency of the sink, the node should mine bring the sync_rate back up.</p>\n<p>So, the proposed updated rule a sync monitor is as follows:</p>\n<ul>\n<li>The node is connected to at least one peer AND</li>\n<li>(The sink is recent OR the sync rate is below threshold)</li>\n</ul>\n<p>With this rule, we cover the scenario where the network halts. If you can think of more scenarios that should be covered, please feel free to suggest it below.</p>\n<h1><a name=\"p-505-mining-rule-engine-3\" class=\"anchor\" href=\"#p-505-mining-rule-engine-3\"></a>Mining Rule Engine</h1>\n<p>This rule engine determines how a given node should mine. It will be a new addition to the node so that it can attempt to recover from some scenarios, such as performance degradation, that are encountered through the course of a node’s operation. As this is new functionality, I will only describe some of the ideas for scenarios we may care about and some possible recovery methods.</p>\n<h2><a name=\"p-505-recovery-methods-4\" class=\"anchor\" href=\"#p-505-recovery-methods-4\"></a>Recovery Methods</h2>\n<ul>\n<li>Consider pointing only to blue blocks\n<ul>\n<li>Useful when header processing takes longer than some threshold (like 0.5sec) and there are constantly red blocks in the mergeset which is a hint that these red blocks might be the cause of the perf issue.</li>\n</ul>\n</li>\n<li>Consider mining empty blocks (no transactions)\n<ul>\n<li>Useful for mining blocks if something about the transactions is what is causing the error such as with the BadMerkleRoot errors during this freeze: <a href=\"https://someone235.medium.com/kaspa-20-minute-blockdag-freeze-post-mortem-7f58bc309bd6/\" class=\"inline-onebox\">Kaspa 20 minute BlockDAG freeze post-mortem | by Ori Newman | Medium</a>. This post-mortem also mentions this recovery method towards the end.</li>\n</ul>\n</li>\n</ul>\n<h2><a name=\"p-505-mining-scenarios-5\" class=\"anchor\" href=\"#p-505-mining-scenarios-5\"></a>Mining Scenarios</h2>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Mining Behavior</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Node is functioning normally</td>\n<td>Node will mine with the standard behavior (as it works today in mainnet)</td>\n</tr>\n<tr>\n<td>Node is in IBD and the sink timestamp is recent enough</td>\n<td>Node will mine with the standard behavior (as it works today in mainnet)</td>\n</tr>\n<tr>\n<td>Node is in IBD and the sink timestamp is not recent enough</td>\n<td>No mining is allowed</td>\n</tr>\n<tr>\n<td>Node is receiving multiple blocks that fail validation with BadMerkleRoot</td>\n<td>If the node continuously receives ONLY blocks that have BadMerkleRoot error, switch to mining only empty blocks (no transactions).</td>\n</tr>\n<tr>\n<td>Network mining has halted</td>\n<td>Considering the updated sync monitor logic above to track sync rate, this will be a completely unexpected scenario and will have to have manual intervention.</td>\n</tr>\n<tr>\n<td>Too many red blocks being merged causing performance degradation</td>\n<td>Allow a maximum of only 1 red block to be merged (or do not merge any red blocks) until node performance settles down.</td>\n</tr>\n</tbody>\n</table>\n</div><p>I encourage the reader to propose more scenarios that may need to be handled or recovery methods to handle such scenarios.</p>",
        "raw_content": "",
        "author": "coderofstuff",
        "created_at": "2025-02-01T06:12:36.334Z",
        "updated_at": "2025-02-01T07:26:15.024Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/sync-monitor-mining-rule-engine/294/1",
        "category_id": 7,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 463,
        "post_number": 1,
        "topic_id": 274,
        "topic_title": "Subnets sequencing commitments",
        "topic_slug": "subnets-sequencing-commitments",
        "content": "<p>Context/prerequisite:<a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/8\"> Based rollup design</a></p>\n<p>Notation glossary:</p>\n<ul>\n<li><span class=\"math\">H</span> - a hash function, e.g. SHA256, or blake, (see discussion <a href=\"https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219\">here</a> re zk-friendly hashes)</li>\n<li><span class=\"math\">MT(x_1,x_2...,x_k)</span> - the <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">Merkle tree</a> derived from <span class=\"math\">x_1,...,x_k</span>,  with some implicit hash function <span class=\"math\">H</span></li>\n<li><span class=\"math\">MR(x_1,x_2...,x_k)</span> - the root of the above Merkle Tree</li>\n<li><span class=\"math\">SQC(B)</span> - the “sequencing commitment” field of block <span class=\"math\">B</span></li>\n<li><span class=\"math\">B.sp</span> - the selected parent of the block <span class=\"math\">B</span></li>\n</ul>\n<p><strong>Introduction</strong></p>\n<p>In Kaspa’s based rollup design we suggested introducing a new field, which serves as a sequencing commitment (previously referred to as the history root).<br>\nThe new field is recursively defined as follows: let <span class=\"math\">B</span> be a block, and let <span class=\"math\">t_1...t_n</span> be all of <span class=\"math\">B</span>'s accepted transactions (more accurately, the tx-hash or tx-id of those, to be determined at a later stage), in the order they were accepted in.<br>\nIgnoring initialization of the first block in the smart contracts hardfork, the new field <span class=\"math\">SQC(B)</span> is:</p>\n<p><span class=\"math\">SQC(B)=H(SQC(B.sp),t_1, t_2...t_n),^1</span></p>\n<p>as illustrated below:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://research.kas.pa/uploads/default/original/1X/5ecd993d9e3414eab10e9a40d97ac4f6f06fa8f4.png\" data-download-href=\"https://research.kas.pa/uploads/default/5ecd993d9e3414eab10e9a40d97ac4f6f06fa8f4\" title=\"image\"><img src=\"https://research.kas.pa/uploads/default/optimized/1X/5ecd993d9e3414eab10e9a40d97ac4f6f06fa8f4_2_516x290.png\" alt=\"image\" data-base62-sha1=\"dwFngcL2zSDwiNbJbMSMuTNSz5O\" width=\"516\" height=\"290\" srcset=\"https://research.kas.pa/uploads/default/optimized/1X/5ecd993d9e3414eab10e9a40d97ac4f6f06fa8f4_2_516x290.png, https://research.kas.pa/uploads/default/optimized/1X/5ecd993d9e3414eab10e9a40d97ac4f6f06fa8f4_2_774x435.png 1.5x, https://research.kas.pa/uploads/default/optimized/1X/5ecd993d9e3414eab10e9a40d97ac4f6f06fa8f4_2_1032x580.png 2x\" data-dominant-color=\"F5F7F6\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1209×679 149 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>To prove the progression of this field between a selected chain block <span class=\"math\">Src</span> and a selected chain block <span class=\"math\">Dst</span> matches the progression of the rollup, the prover requires all txs that were accepted between both of them as <em>private inputs</em>  - inputs that the prover of a statement requires to produce a proof, but the verifier of that proof does not.<br>\nMore formally, these are all txs that were accepted on a selected chain block in the future of <span class=\"math\">Src</span> and in the past of <span class=\"math\">Dst</span>, including <span class=\"math\">Dst</span> itself.</p>\n<p>This scheme is perfectly functional, but it has the following inefficiency: a prover of a certain rollup, is really only interested  in those txs that are directed to their rollup. All other txs, and any block that accepted no tx associated with this rollup, are “noise” from their point of view.<br>\nYet in order to prove they are not associated with the rollup, provers are forced to take those as private inputs, increasing their runtime complexity.</p>\n<p>In particular, if a rollup is only active once every 5 minutes, it would still have to go through the entire selected chain and all transactions accepted on it.<br>\nI explore below a method to represent a particular rollup history in a more concise manner.</p>\n<p><strong>Separate Subnet Histories</strong></p>\n<p>A rollup is represented by a field in the transaction called subnetId, and we refer to each rollup as a “subnet”. Our suggestion (courtesy of <a class=\"mention\" href=\"/u/michaelsutton\">@michaelsutton</a>) is that <span class=\"math\">SQC</span> have an inner structure representing the different subnets’ histories.</p>\n<p>For a block <span class=\"math\">B</span>, and for each subnet <span class=\"math\">i</span>, let <span class=\"math\">t_{i_1}...t_{i_k}</span> be those transactions within <span class=\"math\">B</span>'s accepted txs <span class=\"math\">t_1, t_2...t_n</span> that belong to subnet <span class=\"math\">i</span>.</p>\n<p>We say a subnet <span class=\"math\">i</span> is active at block <span class=\"math\">B</span>, if there ever in history was an accepted transaction belonging to subnet <span class=\"math\">i</span> prior or at block <span class=\"math\">B</span> (this permanent definition of activeness will be relaxed below).</p>\n<p>We can recursively define an abstract field: if block <span class=\"math\">B</span> accepted no txs of subnet <span class=\"math\">i</span> this field takes the same value as its selected parent, i.e. <span class=\"math\">SQC_i(B)=SQC_i(B.sp)</span>.<br>\nOtherwise, <span class=\"math\">SQC_i(B)=H(SQC_i(B.sp),t_{i_1},...t_{i_k})</span>, or simply  <span class=\"math\">SQC_i(B)=H(t_{i_1},...t_{i_k})</span>, if <span class=\"math\">SQC_i(B.sp)</span> was previously undefined. Lastly,</p>\n<p><span class=\"math\">SQC(B)=MR(SQC_i(B)|\\textit{i is active as of block } B)</span>,</p>\n<p>meaning the Merkle root of the sequencing commitments of all active subnets.</p>\n<p>The prover of subnet <span class=\"math\">i</span> now only requires the accepted txs between <span class=\"math\">Src</span> and <span class=\"math\">Dst</span> that belong to subnet <span class=\"math\">i</span>, as well as  witnesses to prove the inclusion of <span class=\"math\">SQC_i(Src)</span>, <span class=\"math\">SQC_i(Dst)</span> in <span class=\"math\">SQC(Src)</span> and <span class=\"math\">SQC(Dst)</span>. These two witnesses are of logarithmic size in the number of subnets active in the network.</p>\n<p>This prover now  has <span class=\"math\">O(A_i+\\log R)</span> private inputs, where  <span class=\"math\">A_i</span> is the number of txs in subnet <span class=\"math\">i</span> at the span in between <span class=\"math\">Src</span> and <span class=\"math\">Dst</span>, and <span class=\"math\">R</span> is the number of active subnets. For some concreteness, with current parameters, <span class=\"math\">\\log R</span> is at most 160.</p>\n<p>The solution as is requires each L1 node to keep track of and compute <span class=\"math\">SQC_i</span> for all <span class=\"math\">R</span> subnets that are active, and subnets never become deactivated.  Effectively this creates a “registry” of subnets. Thought must be devoted to prevent unlimited growth of this registry or spamming attacks.</p>\n<p>I propose implementing pruning of the subnets:  nodes will also  store the last (selected chain) DAA score at which subnet <span class=\"math\">i</span> was updated, and subnets will only be considered active if the current DAA score is less than finality-depth greater than the last update on the subnet.</p>\n<p>If subnets become indisputably inactive, their stored data can be thrown away completely during pruning. This ensures a bound on the number of active subnets at any given time.</p>\n<p>Naive use of Merkle trees may result in recalculation of the entire tree at every block. To prevent this constant recalculation, I advocate using <a href=\"https://medium.com/@kelvinfichter/whats-a-sparse-merkle-tree-acda70aeb837\">sparse Merkle trees</a>.</p>\n<p>For ease of syncing and general data tracking, <span class=\"math\">SQC_i(B)</span> should be updated to explicitly contain the last DAA score subnet <span class=\"math\">i</span> was updated at, e.g.,<br>\n<span class=\"math\">SQC_i(B)=H(DAA(B),H(SQC_i(B.sp),t_{i_1},...t_{i_k}))</span> whenever an update occurs.</p>\n<p>A sketch of the structure of subnets history is provided below:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://research.kas.pa/uploads/default/original/1X/cc1489fbe5e4dba78f6d05e9b4c6835554a84656.jpeg\" data-download-href=\"https://research.kas.pa/uploads/default/cc1489fbe5e4dba78f6d05e9b4c6835554a84656\" title=\"image\"><img src=\"https://research.kas.pa/uploads/default/optimized/1X/cc1489fbe5e4dba78f6d05e9b4c6835554a84656_2_516x301.jpeg\" alt=\"image\" data-base62-sha1=\"t7npkz5JikXNMZVuYHYopTvCnR4\" width=\"516\" height=\"301\" srcset=\"https://research.kas.pa/uploads/default/optimized/1X/cc1489fbe5e4dba78f6d05e9b4c6835554a84656_2_516x301.jpeg, https://research.kas.pa/uploads/default/optimized/1X/cc1489fbe5e4dba78f6d05e9b4c6835554a84656_2_774x451.jpeg 1.5x, https://research.kas.pa/uploads/default/optimized/1X/cc1489fbe5e4dba78f6d05e9b4c6835554a84656_2_1032x602.jpeg 2x\" data-dominant-color=\"F1F3F3\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1196×698 172 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>The pruning means that a subnet should make at least a single tx every finality-depth blocks if it wishes not to be pruned. Finality depth is planned to be about half a day after the 10bps hard fork. It appears to me like a reasonable requirement of a subnet to demonstrate its activity by issuing a tx every 12 hours.</p>\n<p>Nevertheless, there is value in allowing a fallback mechanism in case that for whatever reason a subnet has been pruned prematurely. Hence I suggest we add another branch to  <span class=\"math\">SQC(B)</span>'s sparse Merkle tree, called <span class=\"math\">SQC_{TOT} (B)=H(SQC(B.sp),t_1,...,t_n)</span> which stores the total history as originally proposed, without division to subnets and without pruning. That way pruned subnets could still supply proofs - at the cost of waiving all optimizations.<span class=\"math\">^2</span> Another advantage of maintaining this divisionless branch, is that it maintains the original ordering of the accepted txs in their entirety, which may be required for some applications that choose to track all txs.</p>\n<p>[1] Previously <a class=\"mention\" href=\"/u/michaelsutton\">@michaelsutton</a> suggested  <span class=\"math\">H(MR(SQC(B.sp),t_1, t_2...t_n))</span>. The current reasoning to use a Merkle tree instead of a cumulative hash mostly reduces to overloading the new field with the responsibility previously kept by the <em>Accepted Transactions Merkle Tree</em> field, or possibly, the responsibilities of the Pchmr field proposed in <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0006.md\">KIP6</a> - if we later choose to implement a “logarithmic jumps” lookout into history instead of the linear one described here.<br>\nThis kind of decisions are to be made at a later stage and I see fit to decouple them from the subject at hand.</p>\n<p>[2] It is worth emphasizing that while a proof end point must refer to a relatively recent block for it to be valid, its starting point can be arbitrarily old.</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2025-01-20T11:22:14.519Z",
        "updated_at": "2025-01-20T11:32:28.541Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/1",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 464,
        "post_number": 2,
        "topic_id": 274,
        "topic_title": "Subnets sequencing commitments",
        "topic_slug": "subnets-sequencing-commitments",
        "content": "<p>I think that some L2s will be interested to know the order between 2 transactions from 2 different subnets. SQC-TOT will allow to extract this information as well, but maybe it worth considering to extend definition of the leaves to be subnet transaction and global sequence number ( sequence number of the transaction in the global acceptance order in this block ).</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-20T12:04:33.393Z",
        "updated_at": "2025-01-20T12:04:33.393Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/2",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 470,
        "post_number": 3,
        "topic_id": 274,
        "topic_title": "Subnets sequencing commitments",
        "topic_slug": "subnets-sequencing-commitments",
        "content": "<p>You’re right, that’s probably the better way to go. For the fallback mechanism we only really need <span class=\"math\">SQC(B.sp)</span> rather than anything else, and hashing the global index alongside the tx will prevent the need to hash all txs twice.</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2025-01-20T15:08:10.221Z",
        "updated_at": "2025-01-20T15:08:10.221Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/3",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 471,
        "post_number": 4,
        "topic_id": 274,
        "topic_title": "Subnets sequencing commitments",
        "topic_slug": "subnets-sequencing-commitments",
        "content": "<aside class=\"quote no-group\" data-username=\"FreshAir08\" data-post=\"1\" data-topic=\"274\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/freshair08/48/147_2.png\" class=\"avatar\"> FreshAir08:</div>\n<blockquote>\n<p>Naive use of Merkle trees may result in recalculation of the entire tree at every block. To prevent this constant recalculation, I advocate using <a href=\"https://medium.com/@kelvinfichter/whats-a-sparse-merkle-tree-acda70aeb837\">sparse Merkle trees</a>.</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/freshair08\">@FreshAir08</a> - can you please unfold this a bit … I can see how SMT helps for proof of non-inclusion ( very useful in some zk cases …), but where / how SMT will decrease CPU side computation done by Kaspa L1 ?</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-20T21:09:24.676Z",
        "updated_at": "2025-01-20T21:09:24.676Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/4",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 422,
        "post_number": 1,
        "topic_id": 251,
        "topic_title": "Conflicting Proofs Policy",
        "topic_slug": "conflicting-proofs-policy",
        "content": "<p>In Kaspa based rollups design, entities called provers will regularly submit proofs of an ordered aggregation of txs that occurred since the last proof was submitted –  a point in time which I will refer to as the latest settlement point. More precisely, txs are proven in batches according to the chain blocks that accepted them. These proofs are treated as regular txs from the point of view of L1. Currently there is no set bound planned on how late or how early can these proofs arrive. Provers may cooperate completely on the proving effort – there is no clear danger in them cooperating as the results of their computation are deterministically decided by the contents of the L1.</p>\n<p>Nevertheless for reasons of lack of coordination, or for plain avarice, two provers may end up submitting two distinct proofs, starting from the same L2 latest settlement point. The two proofs will naturally agree on a prefix of the proven txs, but one will usually contain a suffix missing from the second. I refer to such events as conflicting proofs. If the longer of the two is the first included in the L1 ledger, then it is clear that the second proof is to be ignored. But if the shorter is included first, it is less obvious what to do. Naively, the longer proof is treated as a double spend attempt/ invalid tx and ignored. However there are several issues with it, first, in terms of quality of service, the longer proof contains txs that the L1 has yet to see the proof of. Ideally, these proofs can be included immediately, but the naive design foregoes that. Second, the prover has worked tirelessly to create their longer proof, but now will have to start from anew, leading to losses, potentially making proving financially unsustainable if this occurs routinely. Third, this paves the way for a liveness attack on the L2: a malicious party can regularly submit proofs proving 1 block of txs to scoop all other provers (as supplying long proofs takes more time for the benevolent provers), resulting in the settlement to L1 progressing very slowly.</p>\n<p>Two possible solutions come to mind: the first, perform major logical changes to allow miners to include a more inclusive proof even if its settlement point has already been deprecated. I believe this is possible to engineer in some manner, but will likely demand a further level of utxos abstraction, more opcodes, and might end up introducing other complications.</p>\n<p>The second solution is the empty solution where we will accept this phenomenon as a fact of life, yet explain away why the problems are not as fatal as they superficially seem: a property of Snark/Stark proofs as I understand them is that they are componentized. i.e., proofs are not necessarily created as one massive indivisible black box block, rather the different stages of the computation could be broken apart and proven separately, then combined together. If this supposition is true, then even if scooped, a prover only loses the work put to prove the subset of txs proven by its competitor, and some constant metadata. Thus they can relatively quickly rebuild a new proof from the surplus txs they previously proved but their competitor did not. It is self apparent that if this supposition is true it will mitigate the effects of all 3 of the issues discussed above, without the need to take drastic measure. However the “if” here is crucial: I address the zk audience (<a class=\"mention\" href=\"/u/reshmem\">@reshmem</a> and <a class=\"mention\" href=\"/u/proof\">@proof</a> are explicitly called, but please others join in) with a question: is this supposition about the way snark/stark proofs behave true?</p>\n<p>As a sidenote, it is worth mentioning that “scooping” is not all bad: if the fear of being scooped rules the ring, one might expect provers to compete within themselves on proving as fast as possible, vastly improving quality of service. However the exact mechanics required to ensure such competition without collapsing to cannibalism and monopoly require more thought, and seem out of scope for this post.</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2025-01-08T18:34:30.099Z",
        "updated_at": "2025-01-09T10:07:02.213Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/conflicting-proofs-policy/251/1",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 483,
        "post_number": 2,
        "topic_id": 251,
        "topic_title": "Conflicting Proofs Policy",
        "topic_slug": "conflicting-proofs-policy",
        "content": "<p>Please don’t treat my answer as a complete one because it will be too long to answer and in any case I think that even with a very long detailed answer there is not possible to cover everything since every particular choice of ZK system should be considered separately.</p>\n<ol>\n<li>ZK schemes / systems in general support technics as recursion / folding and other smart technics on parallel proof generation but, this topic is very depends on particular system, privacy requirements and many other factors and choices made for this particular zk-system. It looks like it is up to the L2s to choose what is good for them. For example STARK based proving systems with recursion and / or folding are able to support it. Packing stage ( G16 / Plonk ) does not support recursion / folding natively, but MSMs can be paralleled, for example via Pippenger’s algo and FFTs parallel nature is obvious. Especially given the fact that witness in this case has no visible private requirements. Privacy protocols that have this type of requirements will use known technics how to accomplish what the need.</li>\n<li>Even without such support ( no deep dive to the details here ), it looks possible to control desired behaviour via L2s logic. For example L2s can choose the provers’ queue and anchor it inside spending script via ZK itself ( or via BTC-like kaspa script + Merkle-Root )…</li>\n</ol>\n<p>Bottom line, I think L1 should not care about it at all ( at least for the next 1-2 years . I highly support your second solution ) and let L2s to decide what is good for them and how to do it in a best possible / available way ( hence every week we see new <code>game-changing</code> zk scheme comes out.</p>\n<p>But, I still think that this problematic should be reflected in our overall design suggestions in order to provide (at least) some guidelines to these L2s that are new to ZK concepts. Auditors (ZK) on the other hand will catch these points very easily and it is a responsibility of each L2 to conduct such audit.</p>",
        "raw_content": "",
        "author": "reshmem",
        "created_at": "2025-01-24T12:30:59.310Z",
        "updated_at": "2025-01-24T12:37:19.472Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/conflicting-proofs-policy/251/2",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 438,
        "post_number": 1,
        "topic_id": 267,
        "topic_title": "Optimal Transaction Selection - A Multidimensional Knapsack Problem",
        "topic_slug": "optimal-transaction-selection-a-multidimensional-knapsack-problem",
        "content": "<p>KIP9 (Persistent Storage Mass) and KIP13 (Transient Storage Mass) introduced new types of mass which represent some resource consumption within a node. Originally there was only compute mass (usage of compute resource) but now there is also persistent storage mass (usage of storage space that persist between pruning periods) and transient storage mass (usage of storage space only within a pruning period).</p>\n<p>The KIPs above also propose to independently track each mass when checking for block mass limit on the consensus level. This allows fo optimal block space allocation for transactions that consume different resources. The block mass limit is <code>500,000</code> grams. So such independent tracking allows for including two transactions that have masses (compute:490,000g; persistent_storage_mass: 10,000g) and (compute:10,000g; persistent_storage_mass: 490,000g) in the same block.</p>\n<p>Now, an interesting question arises: On the mempool level, when constructing the block template, how can the transaction selection logic be updated to optimize for block space consumption?</p>",
        "raw_content": "",
        "author": "coderofstuff",
        "created_at": "2025-01-18T21:42:56.325Z",
        "updated_at": "2025-01-18T21:42:56.325Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/optimal-transaction-selection-a-multidimensional-knapsack-problem/267/1",
        "category_id": 7,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 420,
        "post_number": 2,
        "topic_id": 247,
        "topic_title": "Fees and throughput regulation dynamics",
        "topic_slug": "fees-and-throughput-regulation-dynamics",
        "content": "<p>A couple of notes and thoughts. Many of these might protrude a bit into the realm of the L2 design itself which may be out of scope but I want to mention them for the reader.</p>\n<p>First, there is no real problem in the L2 requiring another inner L2 fee in proportion to the L1 miner fee (if the inner fee is not sufficient, txs are included but considered invalid from the L2 perspective) is there? I think of the inner fee moving to a SC with its locked money only released by a proof tx. That’s a cleaner way to reward L2 provers for high activity in their subnet imo than forcing L1 to deal with the distribution. Upon reread that might be what was suggested in the post but I wouldn’t use the term “L1 fees” for that.</p>\n<p>Second, without stepping too much into the L2 shoes, I believe L2 rewards should encourage fast proving in some manner, i.e. rewarding higher fees for proofs that come “fast enough”. Could be handled in a similar manner to what I suggested above, with the excess money going back to the original payer if proofs didn’t arrive fast enough. Of course this does not apply to all L2: some may just be service providers and will provide fast proofs to their clients as part of their service.</p>\n<p>Lastly one might ponder what happens if the monetary policy of L2 ends up being unsustainable due to price drop, or unjust due to price increase. L2s can and arguably should have some update mechanism for the gas scale (and potentially other things). Obviously this update mechanism and the extents of what is allowed to be updated, presents many questions of L2 governance, which L2 creators will need to address according to their specific needs and ethea.</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2025-01-08T18:29:57.808Z",
        "updated_at": "2025-01-08T20:20:21.364Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/2",
        "category_id": 11,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 91,
        "post_number": 2,
        "topic_id": 62,
        "topic_title": "Including transactions of red blocks",
        "topic_slug": "including-transactions-of-red-blocks",
        "content": "<p>I really like the idea of including red blocks, but it seems that we’ll have some difficulties with mixing it with the idea of not validating double spends of red blocks. If we don’t keep the past UTXO of red blocks it means it won’t be trivial to count the number of sigops in a transaction, so it means we won’t know if a red block has more mass than allowed (and by proxy also the blue block that points it).</p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2020-04-16T13:28:35.261Z",
        "updated_at": "2020-04-16T13:28:35.261Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/2",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 344,
        "post_number": 3,
        "topic_id": 62,
        "topic_title": "Including transactions of red blocks",
        "topic_slug": "including-transactions-of-red-blocks",
        "content": "<p>Sorry for resurrecting this thread. I have a few questions along these lines and didn’t want to create a new thread when there was already one with minimal discussion.</p>\n<p>I just want to say first that argument about having protection against temporary 51% attacks is a compelling one for including txns within red blocks. It’s not mentioned above, but to include txns within red blocks necessarily means including red blocks in a block’s mergeset (mentioning it here in case it’s unclear to anyone reading).</p>\n<p>I’m wondering about a few things:</p>\n<ol>\n<li>Are red blocks ordered the same way as blue blocks would be (except they go after all the blue blocks in the same mergeset)? My intuition tells me it must be since they’d still have blue work info even if they’re red so they can be ordered that way. And since ordering is consensus sensitive then so ordering them must also be done in a consistent, stable manner.</li>\n<li>How deep of a red block (and txs within) are we allowing for inclusion here? I’m thinking about a few boundaries like the DAA window, the merge depth bound under which we (maybe?) no longer accept red blocks.</li>\n</ol>",
        "raw_content": "",
        "author": "coderofstuff",
        "created_at": "2024-12-10T16:49:11.887Z",
        "updated_at": "2024-12-10T16:49:11.887Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/3",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 263,
        "post_number": 2,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>Technical detail that missed: should both ways(P2SH and new script class) be applied to both schemes: threshold, semi-signature?</p>\n<p>I think <strong>P2SH works for both</strong>.<br>\nHowever Leaking of semi signature can lead to the same attack.<br>\nP2SH can add additional protection mechanisms like secret+min_threshold+lock_time or whatever logic allowed by engine/opcodes</p>\n<p><strong>P2pk is only okay for threshold</strong>.</p>",
        "raw_content": "",
        "author": "biryukovmaxim",
        "created_at": "2024-04-03T12:53:51.779Z",
        "updated_at": "2024-04-03T12:53:51.779Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/2",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 269,
        "post_number": 5,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p><a class=\"mention\" href=\"/u/biryukovmaxim\">@biryukovmaxim</a> has mentioned this elsewhere, but I think it’s worth noting here that we’d need to consider the experience for recovering P2SH addresses created for this purpose when recovering wallets. Since normally, we only recover P2PK addresses in those.</p>\n<hr>\n<p>The opt-out mechanism isn’t discussed above, so I’ll mention it: The owner can always use their regular private key, sign the UTXO and spend it like any other UTXO. When they do, they effectively opt-out of allowing to use that UTXO for further micropayments.</p>\n<hr>\n<p>I want to explore further the P2PK route. <code>spk</code> + <code>threshold</code> is mentioned above with the intent that <code>threshold</code> is a deterrent for spamming.</p>\n<p>Consider this scenario where a wallet has a single 200M <code>Kaspa</code> UTXO and the owner uses this KIP10 mechanism to convert (“KIP10’ize”) that into a 200M UTXO that can be used for micropayments by others, with threshold set to 0.00000001 KAS. A dedicated “hater” with a budget of 10,001 KAS can deny the use of that 200M UTXO by creating a chain of 100,000,000 transactions each reusing the 200M UTXO one after another after adding a small amount to it (2 utxos, 2 outputs; assume fee = 0.0001, increment = 0.00000001) for a duration of time. At 1 BPS, that should be ~1157 days since chained transactions can’t go in the same block so they’d have to go in (at best) consecutive blocks. The “hater” may have paid a hefty amount, but they have denied the use of the UTXO for a significant period of time enough to adversely affect another.</p>\n<p>My main questions with the above absurd scenario are:</p>\n<ol>\n<li>Should there be a maximum amount for the UTXO that can be KIP10’ized? Likely the answer should be no, since arbitrary thresholds are bad. But this brings up a UX concern: wallets should warn users about using this mechanism when they try to convert large utxos to the KIP10’ized ones. Maybe ask the owner to create a smaller UTXO then KIP10’ize that.</li>\n<li>Should there be a limit (set by the user) to the number of times a KIP10’ized UTXO can keep getting reused? Possibly by adding a <code>counter</code> to the SPK. Maybe <code>spk</code> + <code>threshold</code> + <code>limit</code>. Upon use of this UTXO, the <code>limit</code> must also be decreased. When <code>limit</code> reaches 0, it can no longer be reused. The owner then can opt-in again and re-KIP10’ize this UTXO, setting a new limit (or even a new threshold)</li>\n<li>If the owner attempts to spend a KIP10’ized UTXO (so they’ll have their signature from their private key), should their transaction be prioritized over another that uses the UTXO but doesn’t have a signature?</li>\n</ol>",
        "raw_content": "",
        "author": "coderofstuff",
        "created_at": "2024-04-04T05:22:49.514Z",
        "updated_at": "2024-04-04T05:22:49.514Z",
        "reply_count": 3,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/5",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 270,
        "post_number": 6,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<blockquote>\n<p>If the owner attempts to spend a KIP10’ized UTXO (so they’ll have their signature from their private key), should their transaction be prioritized over another that uses the UTXO but doesn’t have a signature?</p>\n</blockquote>\n<p>It would be nice to have the priority. Not sure how easy is to implement it on probably mempool level.</p>\n<blockquote>\n<p>Should there be a limit (set by the user) to the number of times a KIP10’ized UTXO can keep getting reused? Possibly by adding a counter to the SPK. Maybe spk + threshold + limit. Upon use of this UTXO, the limit must also be decreased. When limit reaches 0, it can no longer be reused. The owner then can opt-in again and re-KIP10’ize this UTXO, setting a new limit (or even a new threshold)</p>\n</blockquote>\n<p>Another ideas: frequency ratio, or time_lock/sequence_lock.<br>\nFrequency in meaning that utxos can be collected once per x blocks from not a signer.<br>\nTime_lock/sequence_lock - can be collected up to the time/sequence number, then work as regular p2pk</p>",
        "raw_content": "",
        "author": "biryukovmaxim",
        "created_at": "2024-04-04T05:48:16.593Z",
        "updated_at": "2024-04-04T05:48:16.593Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/6",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 276,
        "post_number": 9,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>It sounds to me like both options are are too prone to human error/selfish behavior.<br>\n<strong>Minimum increment threshold</strong>: the problem is a merchant wouldn’t care about the security of the network as a whole. She would care about her own security, followed by ease of usage, and the easiest usage, the one that will require the least action for both for her and her customers, would be having as little a threshold as possible. It might be bad for the network as a whole, but it’s good for her as an individual user, she has no incentive to set a high threshold as far as I can tell.<br>\n<strong>Semi-signatures</strong>: Similarly, I think leakage of the semi-sig will happen by the merchant herself. After a while of dealing with the back and forth of sending every customer the semi-key, merchants will lose patience and just advertise this on their web page. Why shouldn’t they? they don’t risk anything.</p>\n<p>Sure, the above will not occur for all the additive addresses, but it can happen with enough addresses, and a spammer could then use those to spam. The users of these addresses wouldn’t be individually effected from the attack, you can only get them to move to a safer address by asking for their good will.</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2024-04-08T01:32:11.949Z",
        "updated_at": "2024-04-08T01:32:11.949Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/9",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 278,
        "post_number": 11,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>Ok. I completely misunderstood the suggested problem then. I thought the worry was that state bloaters could piggyback on these inputs to artificially bloat the number/budget of input.  How exactly are these addresses to be dealt with by the storage mass formula?  are they completely ignored?  if they are just added as both inputs and outputs, doesn’t that allow a way to artificially increase the budget? taking a naive look at the bound provided in kip9 (\\sum_tx(storage_mass(tx)&gt;=C*growth^2(G)/budget), by substantially increasing the budget you would need less storage mass. Would a more tight analysis apply only to the attacker’s own budget and disregard the budget coming from the additive addresses?</p>",
        "raw_content": "",
        "author": "FreshAir08",
        "created_at": "2024-04-08T09:59:09.573Z",
        "updated_at": "2024-04-08T11:10:10.010Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/11",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 313,
        "post_number": 17,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<aside class=\"quote no-group\" data-username=\"coderofstuff\" data-post=\"5\" data-topic=\"168\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/coderofstuff/48/105_2.png\" class=\"avatar\"> coderofstuff:</div>\n<blockquote>\n<p>f the owner attempts to spend a KIP10’ized UTXO (so they’ll have their signature from their private key), should their transaction be prioritized over another that uses the UTXO but doesn’t have a signature?</p>\n</blockquote>\n</aside>\n<p>Currently we have rbf, owner can icrease fee to be prioritized over borrowers or vice versa</p>",
        "raw_content": "",
        "author": "biryukovmaxim",
        "created_at": "2024-10-15T07:18:06.703Z",
        "updated_at": "2024-10-15T07:18:06.703Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/17",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 314,
        "post_number": 18,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<aside class=\"quote no-group\" data-username=\"coderofstuff\" data-post=\"5\" data-topic=\"168\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/coderofstuff/48/105_2.png\" class=\"avatar\"> coderofstuff:</div>\n<blockquote>\n<p>Should there be a limit (set by the user) to the number of times a KIP10’ized UTXO can keep getting reused? Possibly by adding a <code>counter</code> to the SPK. Maybe <code>spk</code> + <code>threshold</code> + <code>limit</code>. Upon use of this UTXO, the <code>limit</code> must also be decreased. When <code>limit</code> reaches 0, it can no longer be reused. The owner then can opt-in again and re-KIP10’ize this UTXO, setting a new limit (or even a new threshold)</p>\n</blockquote>\n</aside>\n<p>such script/scenario is implemented here:</p><aside class=\"onebox githubpullrequest\" data-onebox-src=\"https://github.com/kaspanet/rusty-kaspa/pull/487/commits/e5e8b64ef4d0fa31ad1f080ef6e5f7e6e775bf93\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/kaspanet/rusty-kaspa/pull/487/commits/e5e8b64ef4d0fa31ad1f080ef6e5f7e6e775bf93\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/kaspanet/rusty-kaspa</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n    <div class=\"github-icon-container\" title=\"Commit\">\n      <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 16 16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M10.5 7.75a2.5 2.5 0 11-5 0 2.5 2.5 0 015 0zm1.43.75a4.002 4.002 0 01-7.86 0H.75a.75.75 0 110-1.5h3.32a4.001 4.001 0 017.86 0h3.32a.75.75 0 110 1.5h-3.32z\"></path></svg>\n    </div>\n\n\n\n\n  <div class=\"github-info-container\">\n      <h4>\n        <a href=\"https://github.com/kaspanet/rusty-kaspa/pull/487/commits/e5e8b64ef4d0fa31ad1f080ef6e5f7e6e775bf93\" target=\"_blank\" rel=\"noopener nofollow ugc\">Implement one-time and two-times threshold borrowing scenarios\n</a>\n      </h4>\n\n      <div class=\"github-info\">\n        <span>\n          Commit by\n          <a href=\"https://github.com/biryukovmaxim\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n            <img alt=\"biryukovmaxim\" src=\"https://research.kas.pa/uploads/default/original/1X/bc4807c670b41ce4380627f5d6cb8f2bc28583c5.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\" data-dominant-color=\"928F92\">\n            biryukovmaxim\n          </a> - <a href=\"https://github.com/kaspanet/rusty-kaspa/pull/487/commits/e5e8b64ef4d0fa31ad1f080ef6e5f7e6e775bf93\" target=\"_blank\" rel=\"noopener nofollow ugc\">Add KIP-10 Mutual Transaction Opcodes and Hard Fork Support</a>\n        </span>\n      </div>\n\n\n\n\n    <div class=\"branches\">\n      <code>kaspanet:master</code> ← <code>biryukovmaxim:kip-10-mutual-tx</code>\n    </div>\n\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">- Add threshold_scenario_limited_one_time function\n- Add threshold_scenario_limi<span class=\"show-more-container\"><a href=\"https://github.com/kaspanet/rusty-kaspa/pull/487\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"show-more\">…</a></span><span class=\"excerpt hidden\">ted_2_times function\n- Create generate_limited_time_script for reusable script generation\n- Implement nested script structure for two-times borrowing\n- Update documentation for both scenarios\n- Add tests for owner spending, borrowing, and invalid attempts in both cases\n- Ensure consistent error handling and logging across scenarios\n- Refactor to use more generic script generation approach</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n",
        "raw_content": "",
        "author": "biryukovmaxim",
        "created_at": "2024-10-19T10:13:44.689Z",
        "updated_at": "2024-10-19T10:13:44.689Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/18",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 316,
        "post_number": 19,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>Btw, I suggest changing the OpOutputSpk, OpOutputAmount behavior to have an index argument, where -1 will refer to the index of the current input (or add OpCurrInputIdx). This will allow more complex applications (e.g. checking that a payment is being divided to two addresses in some ratio).</p>\n<p>I think that we shouldn’t do the same for the input opcodes, because it’ll break the assumption that you only need the current input utxo to validate a certain input</p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2024-10-29T06:15:12.969Z",
        "updated_at": "2024-10-29T06:15:12.969Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/19",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 317,
        "post_number": 20,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>I like the idea in general. I believe that having relative indexes is more flexible and useful.</p>\n<p>To cover both case Im going to introduce a flag, depending on that the engine will apply absolute/relative logic</p>",
        "raw_content": "",
        "author": "biryukovmaxim",
        "created_at": "2024-10-29T09:56:58.827Z",
        "updated_at": "2024-10-29T09:56:58.827Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/20",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 318,
        "post_number": 21,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>I think it’s best to use <code>OpCurrInputIdx</code> to support both relative and absolute in a more elegant and composable way</p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2024-11-04T17:58:13.146Z",
        "updated_at": "2024-11-04T17:58:13.146Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/21",
        "category_id": 9,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 309,
        "post_number": 1,
        "topic_id": 189,
        "topic_title": "KIP 6 discussion thread",
        "topic_slug": "kip-6-discussion-thread",
        "content": "<p>This is the discussion thread for <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0006.md\">KIP 6</a></p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2024-10-07T08:53:02.978Z",
        "updated_at": "2024-10-07T09:19:41.300Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/1",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 310,
        "post_number": 2,
        "topic_id": 189,
        "topic_title": "KIP 6 discussion thread",
        "topic_slug": "kip-6-discussion-thread",
        "content": "<p>Right now calculating the pruning point from block b POV is quite annoying: P is</p>\n<pre><code>    Self::finality_depth()\n        + Self::merge_depth_bound() * 2\n        + 4 * Self::prev_mergeset_size_limit() * Self::ghostdag_k() as u64\n        + 2 * Self::ghostdag_k() as u64\n        + 2\n</code></pre>\n<p>and the difference between two pruning points is at least F, which makes calculations quite annoying.<br>\nAlso, for KIP6 related calculations, we can’t easily calc <code>prev_posterity(b)</code> because each block points to its pruning point, and <code>prev_posterity(b)</code> is somewhere between <code>b</code> and <code>b.pruning_point</code>.</p>\n<p>So I suggest:</p>\n<ol>\n<li>Change P to 3F (or the equivalent of 72 hrs)</li>\n<li>replacing the field <code>b.pruning_point</code> with <code>b.prev_posterity</code>.</li>\n<li>Optional: set the posterity period to 1 hour</li>\n</ol>\n<p>This will also simplify the functions <code>are_pruning_points_in_valid_chain</code>, <code>next_pruning_points_and_candidate_by_ghostdag_data</code> and <code>are_pruning_points_violating_finality</code></p>\n<p>Another point is that because of the current structure we can’t validate finality violations of less than 2F when replacing a pruning point proof. This will fix it.</p>\n<p><a class=\"mention\" href=\"/u/michaelsutton\">@michaelsutton</a> <a class=\"mention\" href=\"/u/deshe2\">@Deshe2</a></p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2024-10-07T09:02:16.728Z",
        "updated_at": "2024-10-07T09:04:27.430Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/2",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 311,
        "post_number": 3,
        "topic_id": 189,
        "topic_title": "KIP 6 discussion thread",
        "topic_slug": "kip-6-discussion-thread",
        "content": "<p>And to avoid the ambiguity of the field <code>b.pruning_point/prev_posterity</code>, 72 hours after the HF we’ll hardcode the new pruning point and tell <code>are_pruning_points_in_valid_chain</code> and <code>are_pruning_points_violating_finality</code> to stop the check there</p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2024-10-07T09:05:02.805Z",
        "updated_at": "2024-10-07T09:05:02.805Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/3",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 312,
        "post_number": 4,
        "topic_id": 189,
        "topic_title": "KIP 6 discussion thread",
        "topic_slug": "kip-6-discussion-thread",
        "content": "<p>And unrelated suggestion: I suggest to replace the notion of <code>PoChM Merkle root</code> with an <a href=\"https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md\">MMR</a> (of all blocks until <code>prev_posterity</code> or even of the whole history). This should simplify the design</p>",
        "raw_content": "",
        "author": "someone235",
        "created_at": "2024-10-07T09:08:08.034Z",
        "updated_at": "2024-10-07T09:08:08.034Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/4",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 151,
        "post_number": 6,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<p>AFAIR in finality/pruning blueScore is used for time approximation as a side effect, the main reason is for the pruning proof which heavily leans on <code>k</code> which is only true under blueScore.</p>\n<p>IMHO as long as blueScore gives us a lower bound on time then it’s good enough for finality/pruning, but that’s not true for the monetary policy and lock times</p>",
        "raw_content": "",
        "author": "elichai2",
        "created_at": "2020-12-10T11:35:49.954Z",
        "updated_at": "2020-12-10T11:35:49.954Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/6",
        "category_id": 1,
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "post_id": 568,
        "post_number": 1,
        "topic_id": 347,
        "topic_title": "On the inherent tension between multileader consensus and inclusion-time proving",
        "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
        "content": "<p>Parallelized L1s (e.g., Kaspa’s 10bps block-DAG, advanced multileader designs) inherently offer sub-RTT block time and strong intra-round censorship resistance. A key byproduct of their parallelism is execution uncertainty at inclusion time: transactions are included prior to final global ordering and execution. This uncertainty is not a flaw but an enabler for features such as MEV-resistance strategies which operate by obscuring sequence predictability from block composers. At the same time, for universal synchronous composability across multiple based ZK rollups (often conceived as distinct logic zones, each managing independent state), inclusion-time proving represents a near-ideal: Achieving atomic cross-zone operations for composable txns necessitates complex off-chain coordination—related to our <a href=\"https://research.kas.pa/t/a-basic-framework-for-proofs-stitching/323\">framework for proof stitching</a>—before these operations culminate in L1 settlement. Inclusion-time proving would offer immediate, verifiable L1 commitment for the state transitions resulting from these coordinated efforts.</p>\n<h4><a name=\"p-568-the-inherent-counter-duality-1\" class=\"anchor\" href=\"#p-568-the-inherent-counter-duality-1\"></a>The inherent counter-duality</h4>\n<p>The conflict between inclusion-time proving and execution uncertainty is direct:</p>\n<ul>\n<li>Inclusion-time proving mandates a known, unambiguous pre-state for proof generation at the moment of L1 inclusion.</li>\n<li>Execution uncertainty (resulting from multileader, and conducive to MEV-resistance) implies an indefinite pre-state at L1 inclusion, dependent on eventual sequencing of concurrently processed, potentially contending transactions.</li>\n</ul>\n<p>This basic conflict presents a choice. We opt for multileader consensus, embracing its natural execution uncertainty. Consequently, true inclusion-time proving for L1-visible L2 effects (like state commitments) cannot be achieved. Proofs for such effects must therefore be deferred, appearing on L1 only after parallel processing converges and transaction order is sufficiently established to define a clear state.</p>\n<h4><a name=\"p-568-proof-availability-requirement-2\" class=\"anchor\" href=\"#p-568-proof-availability-requirement-2\"></a>Proof availability requirement</h4>\n<p>This inherent gap introduces a critical challenge: L1 has already accepted the transaction data (achieved DA for it) and the system is, in a sense, committed to its potential effects. What if the required proof never materializes? This immediately necessitates a robust proof DA mechanism—the eventual availability of the proof itself must be guaranteed or its absence handled gracefully.</p>\n<p>Furthermore, in an ecosystem of autonomous Logic Zones (LGs)—where each LG is responsible for generating proofs for its own operational segments but cannot compel others—atomic cross-LG operations create interdependencies. The successful L1 settlement of such a composite transaction thus becomes reliant on the timely L1 submission and verification of valid proofs from all participating LGs. This critical interdependency, particularly given the autonomy of each LG, naturally leads to an operational model we term “Timebound proof settlement”.</p>\n<h4><a name=\"p-568-timebound-proof-settlement-3\" class=\"anchor\" href=\"#p-568-timebound-proof-settlement-3\"></a>Timebound proof settlement</h4>\n<p>Under this model, transaction data first achieves L1 DA. Ultimate L1 settlement of its cross-domain effects, however, is explicitly dependent on subsequent L1 verification of ZK proofs, which must be submitted within a defined time window, <span class=\"math\">T</span>, post-L1 sequencing. Confirmation of an L2 operation’s L1 impact is thus its proof-verified settlement within <span class=\"math\">T</span>; failure by any party in a multi-segment operation to provide its proof within this bound means that segment (and potentially the entire atomic operation) fails to settle, with penalties ensuring accountability.</p>\n<p>A key implication of timebound proof settlement is the viability of fast, user-side <em>optimistic confirmation</em> well before L1 proof settlement. Unlike inclusion-time proving where prover censorship directly blocks L1 inclusion, here L1 DA of a transaction already binds it to a based rollup’s L1-registered program. Any designated prover failing to subsequently prove such an L1-committed transaction compromises their rollup’s liveness. This incentive structure extends to multi-rollup atomic operations: users running composite execution nodes can optimistically confirm transactions, relying on each participating rollup’s self-interest in maintaining its own liveness by submitting its proof segment. While such “fat node” optimistic confirmation offers immediate feedback, the underlying L1 settlement latency itself—determined by L1 sequencing plus the cumulative L2 proving times—remains crucial. Importantly, as ZK proving technology continues its rapid advance towards near real-time performance (where real-time &lt;&lt; 12 seconds…), this L1 settlement latency under timebound proof settlement is poised to significantly decrease, enhancing the model’s practicality.</p>\n<p>This timebound proof settlement approach contrasts with embedding full witness DA within L1 transaction payloads, which, while ensuring eventual provability, imposes substantial and constant DA overhead.</p>\n<hr>\n<p>The architectural path an L1 takes will profoundly shape its multileader/MEV characteristics and the efficiency of its rollup ecosystem’s composability. Future L1 designs might explore tiered DA/execution models, offering distinct contexts for “uncertain inclusion” and “certain inclusion” (perhaps with different fee structures or trust assumptions). Ultimately, while timebound proof settlement offers a pragmatic path, novel cryptographic approaches (e.g., proofs over partially indeterminate states) could eventually reshape these trade-offs.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-05-22T19:49:02.766Z",
        "updated_at": "2025-05-22T21:13:01.069Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/1",
        "category_id": 11
      },
      {
        "post_id": 569,
        "post_number": 2,
        "topic_id": 347,
        "topic_title": "On the inherent tension between multileader consensus and inclusion-time proving",
        "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
        "content": "<p>Would it be possible to have a common default deadline (e.g. 5 min), but allow rollups to explicitly set a longer one when needed – for async interop or complex composition cases?<br>\nIn the interop layer, we’d isolate state impact, so if a cross-rollup proof fails, only the affected zone is rejected, and the rest of the rollup proceeds unaffected.</p>",
        "raw_content": "",
        "author": "Pavel_Emdin",
        "created_at": "2025-05-23T08:55:05.346Z",
        "updated_at": "2025-05-23T08:55:05.346Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/2",
        "category_id": 11
      },
      {
        "post_id": 473,
        "post_number": 1,
        "topic_id": 279,
        "topic_title": "Crescendo Hardfork discussion thread",
        "topic_slug": "crescendo-hardfork-discussion-thread",
        "content": "<p><em>This is a discussion thread for KIP-14 — The 10-BPS Crescendo Hardfork</em>: <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0014.md\" class=\"inline-onebox\">kips/kip-0014.md at master · kaspanet/kips · GitHub</a></p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-01-21T21:11:53.611Z",
        "updated_at": "2025-01-22T19:04:34.518Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/1",
        "category_id": 9
      },
      {
        "post_id": 507,
        "post_number": 3,
        "topic_id": 279,
        "topic_title": "Crescendo Hardfork discussion thread",
        "topic_slug": "crescendo-hardfork-discussion-thread",
        "content": "<p>36 hours will slightly increase the storage upper bound (from 190GB for transient storage to ~230GB), but it does seem reasonable.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-02-03T17:31:29.374Z",
        "updated_at": "2025-02-03T17:31:29.374Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/3",
        "category_id": 9
      },
      {
        "post_id": 525,
        "post_number": 5,
        "topic_id": 279,
        "topic_title": "Crescendo Hardfork discussion thread",
        "topic_slug": "crescendo-hardfork-discussion-thread",
        "content": "<p>Any decision about KIP-15?</p>",
        "raw_content": "",
        "author": "Jacek_Kozicki",
        "created_at": "2025-02-27T22:12:19.277Z",
        "updated_at": "2025-02-27T22:12:19.277Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/5",
        "category_id": 9
      },
      {
        "post_id": 339,
        "post_number": 1,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p>This post aims to provide a possible picture of how based zk (zero-knowledge) rollups can be designed to operate over Kaspa’s UTXO-based L1 (see <a class=\"mention\" href=\"/u/hashdag\">@hashdag</a>’s <a href=\"https://research.kas.pa/t/atomic-composability-and-other-considerations-for-l1-l2-support/193\">post</a> for broader context). This is by no means a final design, but rather the accumulation of several months of discussions with <a class=\"mention\" href=\"/u/hashdag\">@hashdag</a>, <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a> and Ilia@starkware, which is presented here for aligning the core R&amp;D discussion in the Kaspa community around common ground before finalizing (the L1 part of) the design.</p>\n<p>I’m making a deliberate effort in this post to use the already established jargon and language of the broader SC community. To this end I suggest that unfamiliar readers review documents such as <a href=\"https://ethresear.ch/t/based-rollups-superpowers-from-l1-sequencing/15016\">eth-research-based</a>, <a href=\"https://ethereum.org/en/developers/docs/scaling/zk-rollups/\">eth-docs-zk</a> and nested links therein before proceeding with this post.</p>\n<blockquote>\n<h5>Brief description of based zk rollups</h5>\n<p>A zk rollup is an L2 scaling solution where L2 operators use succinct zk validity proofs to prove correct smart contract execution without requiring L1 validators to run the full computations themselves. A based zk rollup is a type of rollup design where the L2 is committed to operations submitted to L1 as data blobs (payloads in Kaspa’s jargon), and cannot censor or manipulate the order dictated by L1.</p>\n</blockquote>\n<h3><a name=\"p-339-based-zk-rollups-as-an-l1-l2-protocol-1\" class=\"anchor\" href=\"#p-339-based-zk-rollups-as-an-l1-l2-protocol-1\"></a>Based zk rollups as an L1 ↔ L2 protocol</h3>\n<p>Since based zk rollups represent an interaction between the base layer (L1) and the rollup layer (L2), the L1 must expose certain functionalities to support rollup operations. Conceptually, this defines a protocol between L1 validators and L2 provers.</p>\n<p>Logical functionalities expected from L1:</p>\n<ol>\n<li>\n<p><strong>Aggregate rollup transactions</strong>: L1 aggregates user-submitted data blobs in the order received, providing a reliable anchoring for L2 validity proofs.</p>\n</li>\n<li>\n<p><strong>Verify proof submissions</strong>: L2 operators submit zk proofs that confirm correct processing of these transactions, ensuring L2 execution follows the agreed protocol and updating the L2 state commitments stored on L1.</p>\n</li>\n<li>\n<p><strong>Entry/Exit of L1 funds</strong>: The protocol must enable deposits and withdrawals of native L1 funds to and from L2, ensuring consistent state and authorized spending.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Key point:</strong> L1 acting as the zk proof verifier (point 2) is crucial for enabling the native currency (KAS) to serve as collateral for L2 financial activity, underscoring the importance of point 3. Additional benefits of having L1 verify the proofs include establishing a clear, single point of verification (enforced by L1 consensus rather than requiring each interested party to perform it individually) and providing proof of state commitment for new L2 validators.</p>\n</blockquote>\n<h4><a name=\"p-339-vm-based-vs-utxo-based-l1-2\" class=\"anchor\" href=\"#p-339-vm-based-vs-utxo-based-l1-2\"></a>VM-based vs. UTXO-based L1</h4>\n<p>When the L1 is a fully-fledged smart contract VM, the establishment of this protocol is straightforward: the rollup designer (i) publishes a core contract on L1 with a set of rules for L1 validators to follow when interacting with the rollup; and (ii) specifies a program hash (<code>PROG</code>) that the L2 provers are required to prove the execution of (via a zk proof, <code>ZKP</code>). This two-sided interplay establishes a well-defined commitment of the rollup to its users.</p>\n<p>For UTXO/scripting-based L1s like Kaspa, a more embedded approach is required in order to support various rollup designs in the most generic and flexible way.</p>\n<h3><a name=\"p-339-technical-design-of-the-l1-side-of-the-protocol-in-utxo-systems-3\" class=\"anchor\" href=\"#p-339-technical-design-of-the-l1-side-of-the-protocol-in-utxo-systems-3\"></a>Technical design of the L1 side of the protocol in UTXO systems</h3>\n<p>To begin with, I present a simplified design which assumes a single rollup managed by a single state commitment on L1. Following this minimum-viable design I discuss the implications of alleviating these assumptions.</p>\n<h4><a name=\"p-339-kaspa-dag-preliminaries-4\" class=\"anchor\" href=\"#p-339-kaspa-dag-preliminaries-4\"></a>Kaspa DAG preliminaries</h4>\n<p>A block <span class=\"math\">B \\in G</span> has a selected parent  <span class=\"math\">\\in parents(B)</span> as chosen by GHOSTDAG. The mergeset of <span class=\"math\">B</span> is defined as <span class=\"math\">past(B) \\setminus past(\\text{selected parent of } B)</span>. The block inherits the ordering from its selected parent and appends its mergeset in some consensus-agreed topological order. Block <span class=\"math\">C</span> is considered a chain block from <span class=\"math\">B</span> 's pov if there’s a path of selected parent links from <span class=\"math\">B</span> to <span class=\"math\">C</span> (which means the DAG ordering of <span class=\"math\">B</span> is an extension of <span class=\"math\">C</span> ’s DAG ordering).</p>\n<h4><a name=\"p-339-detailed-protocol-design-5\" class=\"anchor\" href=\"#p-339-detailed-protocol-design-5\"></a>Detailed protocol design</h4>\n<h5><a name=\"p-339-recursive-dag-ordering-commitments-6\" class=\"anchor\" href=\"#p-339-recursive-dag-ordering-commitments-6\"></a>Recursive DAG ordering commitments:</h5>\n<p>A new header field, called <code>ordered_history_merkle_root</code>, will be introduced to commit to the full, ordered transaction history. The purpose of this field is to maintain a recursive commitment to the complete sequence of (rollup) transactions. The leftmost leaf of the underlying Merkle tree will contain the selected parent’s <code>ordered_history_merkle_root</code>, thereby recursively referencing the entire ordered history. The remaining tree leaves correspond to the ordered sequence of mergeset transactions, as induced by the mergeset block order.</p>\n<h5><a name=\"p-339-zk-related-opcodes-7\" class=\"anchor\" href=\"#p-339-zk-related-opcodes-7\"></a>ZK-related opcodes:</h5>\n<ul>\n<li><code>OpZkVerify</code>: An opcode that accepts public proof arguments and verifies the correctness of the zk proof (our final design might decompose this opcode into more basic cryptographic primitives; however, this is out of scope for this post and will be continued by <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a>).</li>\n<li><code>OpChainBlockHistoryRoot</code>: An opcode that provides access to the <code>ordered_history_merkle_root</code> field of a previous chain block. This opcode expects a block hash as an argument and fails if the block has been pruned (i.e., its depth has passed a threshold) or is not a chain-block from the perspective of the merging block executing the script. It will be used to supply valid anchor points to which the ZKP can prove execution.</li>\n</ul>\n<h5><a name=\"p-339-state-commitment-utxo-8\" class=\"anchor\" href=\"#p-339-state-commitment-utxo-8\"></a>State-commitment UTXO:</h5>\n<p>The UTXO responsible for managing the rollup state will appear as an ordinary UTXO from L1’s perspective, with no special handling or differentiation at the base layer. The UTXO spk (<code>script_public_key</code>) will be of type <code>p2sh</code> (pay to script hash), which will represent the hash of a more complex structure. Specifically, it will be the hash of the following pre-image:</p>\n<ol>\n<li><code>PROG</code> (the hash of the permanent program L2 is obligated to execute)</li>\n<li><code>state_commitment</code> (the L2 state commitment)</li>\n<li><code>history_merkle_root</code> (the <code>ordered_history_merkle_root</code> from L1’s header, representing the point in the DAG ordering up to which L1 transactions have been processed to produce the corresponding L2 state commitment)</li>\n<li>Additional auxiliary data required to verify a ZKP (e.g., a well-known verification key)</li>\n<li>The remaining execution script (will be specified below)</li>\n</ol>\n<h5><a name=\"p-339-proof-transaction-9\" class=\"anchor\" href=\"#p-339-proof-transaction-9\"></a>Proof transaction:</h5>\n<p>In its minimal form, a proof transaction consists of an incoming state-commitment UTXO, an outgoing <strong>updated</strong> state-commitment UTXO and a signature revealing the pre-images and the ZKP.</p>\n<p>Assuming such <strong><code>in</code></strong>, <strong><code>out</code></strong> UTXOs and a signature script <strong><code>sig</code></strong>, the following pseudo code outlines the script execution required to verify the validity of this signature (and logically verify the L2 state transition):</p>\n<pre><code class=\"lang-auto\"> //\n // All data is extracted from sig\n // Some operations below might use new Kip10 introspection opcodes\n //\n // Skipping the part where the script itself is proven to be in the preimage \n // (which is standard p2sh processing) \n //\n\n // Prove preimages\n show (prog_in , commit_in , hr_in , ...) is the preimage of in.spk\n show (prog_out, commit_out, hr_out, ...) is the preimage of out.spk\n verify prog_in == prog_out // verify prog is preserved\n \n // Verify L1 history-root anchoring\n extract block_hash from sig // the chain block we are claiming execution to\n hr_ref &lt;- OpChainBlockHistoryRoot( block_hash ) // fails if anchoring is invalid\n verify hr_ref == hr_out\n\n // Verify the proof\n extract zkp from sig\n OpZkVerify( proof: zkp, proof_pub_inputs: [commit_in, commit_out, hr_in, hr_out]) \n // ^ omitting prog and other auxiliary verification data\n</code></pre>\n<h5><a name=\"p-339-l2-program-semantics-10\" class=\"anchor\" href=\"#p-339-l2-program-semantics-10\"></a>L2 program semantics:</h5>\n<p>In order to provide the desired <em>based</em> rollup guarantees, the execution specified by the (publicly known) L2 <code>PROG</code> must strictly adhere to the following rules:</p>\n<ul>\n<li>Reveal (through private program inputs) the full tree diff claimed to be processed: <strong><code>T(hr_out)</code></strong> <span class=\"math\">\\setminus</span> <strong><code>T(hr_in)</code></strong></li>\n<li>Execute the identified transactions in order, without any additions or removals</li>\n</ul>\n<p>Observe that the first rule, combined with the <code>OpChainBlockHistoryRoot</code> call within the script, ensures that the state commitment always advances to a valid state:</p>\n<ul>\n<li>The script verifies that <strong><code>hr_out</code></strong> is a valid chain block history commitment.</li>\n<li>The <code>PROG</code> verifies that <strong><code>hr_out</code></strong> recursively references <strong><code>hr_in</code></strong> and, as a result, must be an extension of it.\n<ul>\n<li>This verification by <code>PROG</code> is enforced by L1 through <code>OpZkVerify</code>.</li>\n</ul>\n</li>\n</ul>\n<h5><a name=\"p-339-operational-flow-11\" class=\"anchor\" href=\"#p-339-operational-flow-11\"></a>Operational flow:</h5>\n<ol>\n<li><span class=\"math\">Tx_1, Tx_2, ..., Tx_n</span> with data payloads are submitted to the DAG, included in blocks, and accepted by chain blocks <span class=\"math\">C_1, C_2, ..., C_n</span>, respectively.</li>\n<li>The transaction hashes are embedded into the <code>ordered_history_merkle_root</code> fields of the corresponding headers, enforced as part of L1 consensus validation.</li>\n<li>An L2 prover chooses to prove execution up to block <span class=\"math\">C_i</span></li>\n<li>A proof transaction referencing the initial state-commitment UTXO and producing a new state-commitment UTXO (encoding the new history root <code>ordered_history_merkle_root</code><span class=\"math\">(C_i)</span>) is created.</li>\n<li>The proof is validated by L1, and the new state-commitment UTXO replaces the previous one in the UTXO set, recording the L2 state transition on L1.</li>\n</ol>\n<h4><a name=\"p-339-soundness-and-flexibility-of-the-proposed-design-12\" class=\"anchor\" href=\"#p-339-soundness-and-flexibility-of-the-proposed-design-12\"></a>Soundness and flexibility of the proposed design</h4>\n<p>The design presented supports fully based zk rollups by embedding ordered history Merkle roots into block headers and introducing new zk-related opcodes into Kaspa’s script engine. The history roots provide modular and complete evidence of DAG ordering, enabling provers to select granularity over any period of consecutive chain blocks (while still requiring processing in mergeset bulks). The combination of L1 script opcodes and L2 <code>PROG</code> programmability provides substantial flexibility for rollup design.</p>\n<h3><a name=\"p-339-points-for-subsequent-discussion-13\" class=\"anchor\" href=\"#p-339-points-for-subsequent-discussion-13\"></a>Points for subsequent discussion</h3>\n<p>Despite its focus on the most baseline design, this post is already becoming rather long, so I’ll wrap up by briefly outlining some key “zoom-in” points for further discussion and refinement.</p>\n<ol>\n<li>\n<p><strong>Uniqueness of the state-commitment UTXO</strong><br>\n1.1 <em>Challenge:</em> Proving that a specific UTXO represents “the” well-known authorized state commitment for a given rollup. Although state transitions do not require such proof, it is important, for instance, for proving L2 state to a newly syncing L2 node.<br>\n1.2 <em>Solution Direction:</em> L2 source code can define a “genesis” state (encoded in the initial UTXO), and zk validity proofs can recursively attest that the current state originated from that genesis.</p>\n</li>\n<li>\n<p><strong>Entry/Exit of L1 funds</strong><br>\n2.1 Allow deposits to static addresses representing the L2.<br>\n2.2 Allow withdrawals back to L1 (as additional proof transaction outcomes).<br>\n2.3 The <span class=\"math\">N</span>-to-<span class=\"math\">const</span> problem: Address the challenges arising from local limits on transaction size and the potentially many outcomes resulting from a batched proof operation.</p>\n</li>\n<li>\n<p><strong>Extension to many rollups</strong> ¹<br>\n3.1 <em>Requirement/Desire:</em> Each L2 rollup prover should only need to execute <span class=\"math\">O(\\text{rollup activity})</span> within their <code>PROG</code> proof execution.<br>\n3.2 <em>Solution Direction:</em> Manage the L1 history Merkle tree by grouping by rollup and further dividing into activity/inactivity branches. Note: Applying the grouping recursively might result in long-term storage requirements per rollup, which has broader implications.</p>\n</li>\n<li>\n<p><strong>Multiple state commitments per rollup</strong><br>\n4.1 <em>Challenge:</em> Allowing L1 to manage multiple state commitments for a single rollup in order to balance scalability and validity (allowing different provers to partially advance independent segments/logic zones of L2 state).<br>\n4.2 <em>Solution Direction:</em> Implement partitioned state commitments on L1, representing dynamic <em>cuts</em> of the L2 state tree. If taken to the extreme, this solution could allow a user to solely control their own L2 account via a dedicated state commitment on L1.</p>\n</li>\n</ol>\n<p>Another major aspect not discussed in this post is the zero-knowledge technology stack to be supported and its implications for L1 components (e.g., the hash function used to construct the Merkle history trees). Laving this part to <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a>, <a class=\"mention\" href=\"/u/aspect\">@aspect</a> and others for full follow-up dives.</p>\n<p>[¹] The introduction of many rollups (or subnets in Kaspa’s jargon) touches on conceptual topics such as L2 state fragmentation and atomic composability, which are beyond the scope of this post and were preliminarily discussed in <a class=\"mention\" href=\"/u/hashdag\">@hashdag</a>’s post. Here, I’m referring merely to the technical definitions and consequences.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-12-09T13:09:26.728Z",
        "updated_at": "2025-01-09T12:48:11.413Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/1",
        "category_id": 11
      },
      {
        "post_id": 347,
        "post_number": 2,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p>I know that for zkEVM, a single account holds the balances of all users in the rollup. It looks like this design makes that account a UTXO which contains a commitment in the form of a merkle root of a merkle tree that commits to all the current balances of existing accounts in the rollup.</p>\n<p>Re 1: Highly cogent and reasonbale.</p>\n<p>Re 2: Will the entire tree be posted on-chain or just the root of the tree? If it is just the root, how do users get their branch in the tree in order to be capable of exiting without permission when they want to?</p>\n<p>Re 3: Not sure how you gonna do that based on your description. It sounds like some clustering. Personal thought is not sure if this is a high priority issue (do you need that many rollups on Kaspa or having one that works is more important?)</p>\n<p>Re 4: Not sure if understood correctly, but if the concern is about scalability and validity, I think fully distribututed ZKPs may be something interesting to think of. The scheme distributes proof generation across multiple machines and require minimal communication among them; it allows us to distribute ZKP generation in zkRollups and zkEVM among multiple participants as mining pools. Participants may share the reward for proof generation, akin to miners in PoW chains like Kaspa. This also is related to 3 since then groupin may be unecessary.</p>\n<p>One more thing: do we have a role like an aggregator (such as one in CKB)?</p>",
        "raw_content": "",
        "author": "YesComrade",
        "created_at": "2024-12-10T21:26:43.666Z",
        "updated_at": "2024-12-10T21:26:43.666Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/2",
        "category_id": 11
      },
      {
        "post_id": 351,
        "post_number": 3,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p>In the basic design - yes, the UTXO will contain a single state root representing all existing rollup accounts.</p>\n<p>Re 2. Only the root. Any user operation on his L2 account will mutate the root, and the prover reporting this root mutation (via a proof tx) will be obligated to apply the outcome in L1 in the form of an additional tx output going to the user’s L1 address. This will be enforced as part of the PROG.</p>\n<p>Re 3. As mentioned, the above description applies to the basic design. In a more advanced/dynamic design, we can have the L2 state committed to L1 in multiple, fragmented state commitments—where at the extreme, a single account can have its own L1 state-commitment UTXO.<br>\nMy mental picture of this is keeping the image of L2 state as a single tree, but drawing a tree cut representing the subtree roots which are reported to L1 (see image)</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://research.kas.pa/uploads/default/original/1X/3481b217c383510bd3c65aff9e8ee5ab923ef747.png\" data-download-href=\"https://research.kas.pa/uploads/default/3481b217c383510bd3c65aff9e8ee5ab923ef747\" title=\"image\"><img src=\"https://research.kas.pa/uploads/default/optimized/1X/3481b217c383510bd3c65aff9e8ee5ab923ef747_2_690x226.png\" alt=\"image\" data-base62-sha1=\"7uuHpr68MaQyl7bWYlyQY18C1p5\" width=\"690\" height=\"226\" srcset=\"https://research.kas.pa/uploads/default/optimized/1X/3481b217c383510bd3c65aff9e8ee5ab923ef747_2_690x226.png, https://research.kas.pa/uploads/default/optimized/1X/3481b217c383510bd3c65aff9e8ee5ab923ef747_2_1035x339.png 1.5x, https://research.kas.pa/uploads/default/original/1X/3481b217c383510bd3c65aff9e8ee5ab923ef747.png 2x\" data-dominant-color=\"F7F8F9\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1072×352 35.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Notes:</p>\n<ul>\n<li>Such L2 split strategies can be static (as in scenario 2), or dynamic (scenario 3) in which case they will be supported via an L2 “detach” command which locally breaks a commitment to its tree-child commitments (on L1 this will look like a proof tx with a single incoming UTXO representing the previous subtree root and multiple output UTXO entries representing its children)</li>\n<li>In scenario 3, accounts 13, 14 can be solely controlled by their sole owner, perhaps even requiring that each operation must be provided with an immediate inline ZKP. Think krc20 token holders performing a send—the transaction can consume the state-commitments UTXOs of both the sender and recipient and can output the updated UTXOs with the updated state commitments, where the signature is an inline ZKP (credit: <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a>, <span class=\"mention\">@aspect</span>).</li>\n<li>Of course there are numerous subtleties and complexities to this dynamic design which I’m neglecting here, one major one being “how to prove a subtree state commitment can be advanced w/o needing to execute unrelated rollup transactions (for showing non of them touched this part of the state)?”. This requires some form of “exclusion proof” or an explicit way to state read/write dependencies (cc: <span class=\"mention\">@aspect</span>).</li>\n</ul>\n<p>Re 4. I totally agree, I think proof generation should be a distributed effort. Imho it can be a coordinated collective effort, and even centralized to some degree, since decentralization and censorship-resistance are enforced by L1 due to the based design guarantees.</p>\n<p>Re the final remark. Afaiu the aggregator role in CKB is exactly the <em>non-based</em> part where an L2 operator is required to collect off-chain batches, hence it’s irrelevant to this based design. That being said, there’s no way to enforce such a thing from L1 (unless you define a single <code>METAPROG</code> which all rollups must extend)—the point is to allow based rollups and to make them the default way to go, not to forbid non-based approaches.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-12-11T11:01:55.226Z",
        "updated_at": "2024-12-11T11:01:55.226Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/3",
        "category_id": 11
      },
      {
        "post_id": 355,
        "post_number": 5,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p>I’ll try to address the questions to some degree, though I think some of the answers will be fully clarified only after in-depth follow-up posts.</p>\n<p>Note: for simplicity, my points below still assume the baseline design with a single state-commitment UTXO per rollup instance, but can easily be extended to the multiple-commitment scheme described in my previous comment.</p>\n<p>Re 1. First - this area is definitely still not finalized and you (or any other reader) should join the brainstorming. My current thinking is as follows: Think of L2 (a rollup instance) as a “virtual” wallet which might own many UTXOs on L1. All spks (addresses) for this wallet will be <code>p2sh</code>; however, some will be dynamic and some static. Like you mention in point 3, the state-commitment UTXO is an everchanging dynamic <code>p2sh</code> address. In addition to that there will be a set of static <code>p2sh</code> addresses which can be driven from <code>PROG</code> ¹. The static addresses will be used by users for deposit/entry. Spending these UTXOs must be done through a proof transaction and the spending sig must reveal the preimage script which will delegate verification to the primary input sig of the proof tx (the one containing the ZKP) ².</p>\n<p>Re 2. The L2 <code>state_commitment</code> would usually be a Merkle root of its state (e.g., the root of a Patricia tree). But our L1 design should not rely on specific assumptions about it.</p>\n<p>Re 3: As mentioned, yes, it means a different <code>p2sh</code> address each time. Explorers will learn how to track these addresses and treat them as a continuous entity by following the specific L2 encoding.</p>\n<p>Re 4. Great question. This is precisely why <code>OpChainBlockHistoryRoot</code> ³ is set to fail if the block hash isn’t a chain block from pov of the <em>executing merging block</em>. If a reorg invalidates a previously used anchoring chain block, then the proof tx using that anchor will be invalidated as well (when executed through the new chain), thus effectively “unspending” the spent state-commitment UTXO. This means that following a reorg, L2 provers will need to resubmit proofs proving execution according to the order dictated by the new chain segment. Your mental picture here should be a proof chain following the DAG selected chain. Note that if done correctly, L2 provers can reuse hierarchic zk proofs used to compose the previous proof-chain. I.e., a reorg does not necessarily mean full re-computation of all reversed proofs.</p>\n<p>Re 5. It can be the full amount deposited to L2 all concentrated in the “dynamic” state-commitment UTXO. I don’s see this as an issue.</p>\n<p>[¹] These addresses can incorporate KIP10-style additive schemes for improved management of the L2 UTXO subset and for compliance with KIP9<br>\n[²] The advanced reader might notice that this scheme requires the “Uniqueness of the state-commitment UTXO” property I mentioned at the end of the post.<br>\n[³] Unrelated note on <code>OpChainBlockHistoryRoot</code>. The chain depth we allow access to here will affect syncing of new L1 nodes. We will need to sync a chain segment of that length below the pruning point in order for the syncee to be able to process all transactions above the pruning point deterministically.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-12-12T11:10:23.989Z",
        "updated_at": "2024-12-22T15:17:24.401Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/5",
        "category_id": 11
      },
      {
        "post_id": 394,
        "post_number": 6,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p>It took me a while to find this ancient post in Bitcoin Forum: <a href=\"https://bitcointalk.org/index.php?topic=101734.0\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Storing UTXOs in a Balanced Merkle Tree (zero-trust nodes with O(1)-storage)</a>.</p>\n<p>I think it answers the basic questions of storing UTXOs plus that its <span class=\"hashtag-raw\">#7</span> reply (starting with “Node-deletion is not the inverse of node-insertion”) is related to your Re 1. (addressing the entry/exit question) above.</p>",
        "raw_content": "",
        "author": "LostandFound",
        "created_at": "2024-12-18T13:52:22.577Z",
        "updated_at": "2024-12-18T13:52:22.577Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/6",
        "category_id": 11
      },
      {
        "post_id": 398,
        "post_number": 7,
        "topic_id": 208,
        "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
        "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
        "content": "<p><a class=\"mention\" href=\"/u/michaelsutton\">@michaelsutton</a> then I think the biggest thing what this post wasn’t addressing is the “how to be based” question? I can think of execution of a zkSNARK verification program pre-configured with a verification key (like a SNARK VM) . It can be run and implemented in a really simple way with only two challenge scenarios and just 3 instructions ADD, SUB and MULKAS, running any zk-snark generator lib targeting groth16, for example.</p>\n<p>Certainly bridge that allows users to transfer assets between L1 and L2 would be needed. That should also easily solve the problem of deposit and withdrawal. There may be two types of proofs that will be needed: one for transaction inclusion and another one for state transition of bridge sc on the L2 side. For both a zkSNARK groth16 proof can be a good fit and proof recursion should be supported to allow Plony2 and STARKs proofs if needed.</p>\n<p>But if a role of operator is introduced then I guess this is agian <em>non-based</em>, right? As based-approach may be challenging and taking some time and saying “not to forbid non-based approaches”, will Kaspa welcome third party teams to make a non-based rollup plan on Kaspa? [Question 1]</p>\n<p>Also what happens if the target block with the target TX is invalidated by reorg? The <code>OpChainBlockHistoryRoot</code> looks like a revert. Practically, for an unconfirmed transaction does the transaction remain in the mempool and is reprocessed? Would this end up with a new TX hash? How might we be notified of this new TX? For a confirmed (multiple confirmations) transaction (though may not be likely) will it be marked as reverted? I guess the answer is Yes as you mentioned about to resubmit. Could this potentially lead to a situation in which a TX1 is confirmed on L1 while another TX2 is later confirmed on L1 on the canonical, which means TX2 is final but TX1 is reverted after TX2 is submitted? [Question 2]</p>\n<p>An additional thought on reorg: is it possible to figure out a way maybe just do experiements? I have seen people do it for Opstack by Setting up an L2 replica – Bridge OAS from L1 to L2 using three different accounts (A, B, C) – Fork L1, and then roll up L2 to the miner’s side – Transfer – Merge L1’s minor chain with the majority – Set up another replica. Then just see what happens to the accounts. Would it be possible to figure out ansuwers to such questions just empirically maybe? So different L2s will at least know what will happen at least. [Question 3]</p>",
        "raw_content": "",
        "author": "YesComrade",
        "created_at": "2024-12-21T20:50:12.322Z",
        "updated_at": "2024-12-21T20:50:53.006Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/7",
        "category_id": 11
      },
      {
        "post_id": 395,
        "post_number": 4,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Regarding hashes (after consulting with smart folks):</p>\n<p>Preliminary remark: “zk-friendliness” of a hash function is not a purely intrinsic property - it depends on design/efficiency of the underlying AIRs (at least in the context of STARKs). AIR efficiency can vary by orders of magnitude, and of course AIR design depends on the particular proving protocol <em>and implementation</em> (it’s a bit of an art).</p>\n<p>To start with some names, Blake, Pedersen, and Poseidon are all used in production rollups (e.g Starknet).</p>\n<ol>\n<li>\n<p>Blake (like keccak and sha256) consists of many bitwise operations so it’s very execution friendly. From the zk perspective, bitwise operations are only native to binary fields, which are not commonly used at the moment (neither for EC nor for STARKs). Hence Blake isn’t zk-native. As remarked above, zk-friendliness depends on the ambient proving protocol and implementation. There’s a <a href=\"https://hackmd.io/@starkware-hackmd/SJUbOQj9C\" rel=\"noopener nofollow ugc\">very efficient AIR</a> for the <a href=\"https://github.com/starkware-libs/stwo\" rel=\"noopener nofollow ugc\">Stwo prover</a> for STARKs over Mersenne 31.</p>\n</li>\n<li>\n<p>Pedersen (roughly) takes two felts, concatenates them, and returns the associated multiple of some EC generator. It’s relatively zk-friendly if your proving protocol works over the same field. Pedersen lacks entropy in small fields such as Mersenne 31 and consequently isn’t secure in the naive approach. On the other hand, proving is <em>much</em> more efficient over small fields. There’s a middle ground if you use ECs over an extension field, but then you’re losing proving efficiency due to the overhead of representing EC ops using base felts. Note EC ops are also relatively heavy to execute.</p>\n</li>\n<li>\n<p>Poseidon takes a vector of felts and returns another via sequences of matrix multiplications and pointwise powers and additions. Hence it’s very execution-friendly and also quite zk-friendly assuming efficient AIRs. It also has plenty of entropy so it can be used over smaller fields. If you want a secure zk-friendly hash compatible with small fields, I think this is the best option.</p>\n</li>\n</ol>\n<p>I’d choose between Blake and small-field Poseidon depending on whether you want to optimize for execution time or proving time.</p>",
        "raw_content": "",
        "author": "proof",
        "created_at": "2024-12-19T13:54:23.515Z",
        "updated_at": "2024-12-19T13:57:10.296Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/4",
        "category_id": 11
      },
      {
        "post_id": 401,
        "post_number": 6,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p><a class=\"mention\" href=\"/u/hashdag\">@hashdag</a>, <a class=\"mention\" href=\"/u/proof\">@proof</a> indeed suggested that</p>\n<blockquote>\n<p>Poseidon takes … hence it’s very <em>execution-friendly</em></p>\n</blockquote>\n<p>(where by “execution-friendly” I’m assuming he means L1 friendly), however afaik from <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a> it is still an ~order of magnitude more expensive to compute than Blake-class hash functions. So it seems like we need exact benchmark numbers before making any conclusion here?</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-12-22T13:44:02.976Z",
        "updated_at": "2024-12-22T13:44:02.976Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/6",
        "category_id": 11
      },
      {
        "post_id": 402,
        "post_number": 7,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Sorry, my post was unclear. Blake will probably be faster to run on x86 than Poseidon (even on a small field). Perhaps the execution time will be comparable if the field is sufficiently small, in which case Poseidon will be the best of both worlds (as it’s much more efficient to prove with Circle STARKs over a small field). In my opinion such a core level decision warrants benchmarks.</p>",
        "raw_content": "",
        "author": "proof",
        "created_at": "2024-12-22T15:52:19.405Z",
        "updated_at": "2024-12-22T15:52:19.405Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/7",
        "category_id": 11
      },
      {
        "post_id": 440,
        "post_number": 11,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>Zero-knowledge proof is a rapidly developing technology. Even for Plonky, there are multiple versions like Plonky2, Plonky3, and FFLONK. In Ethereum, this can be implemented through smart contract updates. How does Kaspa ensure that it can quickly support more advanced proof systems when they become available on the market? Or must it undergo another hardfork?</p>",
        "raw_content": "",
        "author": "Dash",
        "created_at": "2025-01-19T14:40:02.682Z",
        "updated_at": "2025-01-19T14:48:19.647Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/11",
        "category_id": 11
      },
      {
        "post_id": 481,
        "post_number": 15,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<aside class=\"quote no-group\" data-username=\"reshmem\" data-post=\"13\" data-topic=\"219\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/reshmem/48/111_2.png\" class=\"avatar\"> reshmem:</div>\n<blockquote>\n<p>We’re implementing the EC curve ops which will allow sufficient flexibility without direct need for hard-forks.</p>\n</blockquote>\n</aside>\n<p>this is an assumption of a EC-based zk system? hash-based zk system would still be a hard fork?</p>",
        "raw_content": "",
        "author": "superMainnet",
        "created_at": "2025-01-23T20:55:26.788Z",
        "updated_at": "2025-01-23T20:55:26.788Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/15",
        "category_id": 11
      },
      {
        "post_id": 484,
        "post_number": 17,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<aside class=\"quote no-group\" data-username=\"reshmem\" data-post=\"16\" data-topic=\"219\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/reshmem/48/111_2.png\" class=\"avatar\"> reshmem:</div>\n<blockquote>\n<p>can be implemented in terms of EC-based Math-Opcodes ( Groth16 / Plonk).</p>\n</blockquote>\n</aside>\n<p>is there secondary proving going on?</p>\n<p>“While traditional zk-SNARKs rely on cutting-edge cryptographic hard problems and assumptions, the only cryptographic ingredient in a STARK proof system is a collision-resistant hash function.”</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://aszepieniec.github.io/stark-anatomy/\">\n  <header class=\"source\">\n\n      <a href=\"https://aszepieniec.github.io/stark-anatomy/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Anatomy of a STARK</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://aszepieniec.github.io/stark-anatomy/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Anatomy of a STARK, Part 0: Introduction</a></h3>\n\n  <p>STARK tutorial with supporting source code in python.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n",
        "raw_content": "",
        "author": "superMainnet",
        "created_at": "2025-01-24T14:42:11.717Z",
        "updated_at": "2025-01-24T14:42:11.717Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/17",
        "category_id": 11
      },
      {
        "post_id": 492,
        "post_number": 21,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<aside class=\"quote no-group\" data-username=\"reshmem\" data-post=\"18\" data-topic=\"219\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/reshmem/48/111_2.png\" class=\"avatar\"> reshmem:</div>\n<blockquote>\n<p>A <strong>chain of zk proofs</strong> (<strong>STARK/any other new proof system</strong> → <strong>SNARK</strong>) leverages the best of both worlds:</p>\n</blockquote>\n</aside>\n<p>the wall of ai generated background info is appreciated, but in the interest of time a simple “yes” would have saved me 20 minutes</p>\n<aside class=\"quote no-group\" data-username=\"superMainnet\" data-post=\"17\" data-topic=\"219\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/letter_avatar_proxy/v4/letter/s/d78d45/48.png\" class=\"avatar\"> superMainnet:</div>\n<blockquote>\n<p>is there secondary proving going on?</p>\n</blockquote>\n</aside>",
        "raw_content": "",
        "author": "superMainnet",
        "created_at": "2025-01-28T16:22:54.145Z",
        "updated_at": "2025-01-28T16:23:14.616Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/21",
        "category_id": 11
      },
      {
        "post_id": 493,
        "post_number": 22,
        "topic_id": 219,
        "topic_title": "Additional practical considerations re hash function and zk opcodes",
        "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
        "content": "<p>fyi groth16 seems to be falling out of favor in the Ethereum community</p>\n<aside class=\"onebox twitterstatus\" data-onebox-src=\"https://x.com/VitalikButerin/status/1883629223002394953\">\n  <header class=\"source\">\n\n      <a href=\"https://x.com/VitalikButerin/status/1883629223002394953\" target=\"_blank\" rel=\"noopener nofollow ugc\">x.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://research.kas.pa/uploads/default/original/1X/e2d30883d50b390f817d22407a4364952ea4d383.jpeg\" class=\"thumbnail onebox-avatar\" data-dominant-color=\"A39B9B\" width=\"200\" height=\"200\">\n<h4><a href=\"https://x.com/VitalikButerin/status/1883629223002394953\" target=\"_blank\" rel=\"noopener nofollow ugc\">vitalik.eth (@VitalikButerin) on X</a></h4>\n<div class=\"twitter-screen-name\"><a href=\"https://x.com/VitalikButerin/status/1883629223002394953\" target=\"_blank\" rel=\"noopener nofollow ugc\">@VitalikButerin</a></div>\n\n<div class=\"tweet\">\n  <span class=\"tweet-description\">One of the many things that we need to acceeeeeeelerate is abolishing groth16\n\nPer-application trusted setups are just not ok in the 2020s. Universal setup at the minimum, ideally no setup at all\n\nThis requires big improvements on infra and standardization for newer SNARK algos</span>\n</div>\n\n<div class=\"date\">\n  <a href=\"https://x.com/VitalikButerin/status/1883629223002394953\" class=\"timestamp\" target=\"_blank\" rel=\"noopener nofollow ugc\"></a>\n\n\n</div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<aside class=\"onebox twitterstatus\" data-onebox-src=\"https://x.com/MauroAligned/status/1884226672100053284\">\n  <header class=\"source\">\n\n      <a href=\"https://x.com/MauroAligned/status/1884226672100053284\" target=\"_blank\" rel=\"noopener nofollow ugc\">x.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://research.kas.pa/uploads/default/original/1X/ff47c4385d4610e28ec613ee13de44c12ab73481.jpeg\" class=\"thumbnail onebox-avatar\" data-dominant-color=\"7E7571\" width=\"200\" height=\"200\">\n<h4><a href=\"https://x.com/MauroAligned/status/1884226672100053284\" target=\"_blank\" rel=\"noopener nofollow ugc\">Mauro Toscano 🟩 (@MauroAligned) on X</a></h4>\n<div class=\"twitter-screen-name\"><a href=\"https://x.com/MauroAligned/status/1884226672100053284\" target=\"_blank\" rel=\"noopener nofollow ugc\">@MauroAligned</a></div>\n\n<div class=\"tweet\">\n  <span class=\"tweet-description\">It's incredible that Groth16 is still widely used today. It's not the fastest, requires a trusted setup, it's not even universal, and most of the new tech have to do a lot of work to convert from their proving system to it.\n\nBut not a surprise, since it's the cheapest proving</span>\n</div>\n\n<div class=\"date\">\n  <a href=\"https://x.com/MauroAligned/status/1884226672100053284\" class=\"timestamp\" target=\"_blank\" rel=\"noopener nofollow ugc\"></a>\n\n\n</div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n",
        "raw_content": "",
        "author": "superMainnet",
        "created_at": "2025-01-28T16:26:15.555Z",
        "updated_at": "2025-01-28T16:26:15.555Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/22",
        "category_id": 11
      },
      {
        "post_id": 504,
        "post_number": 1,
        "topic_id": 293,
        "topic_title": "Thoughts about covenant and async message standardization",
        "topic_slug": "thoughts-about-covenant-and-async-message-standardization",
        "content": "<p>The <a href=\"https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258\">canonical bridge</a> post essentially suggested a way to authenticate that a UTXO belongs to some “covenant chain”. A natural continuation of this would be to standardize the idea of a covenant and to allow authenticating it with some identity, in a way which is authorized by L1. One use-case for such standardization would be async message passing between covenants/rollups where L1 will certify the sender identity. It could also simplify the implementation of a delegation script such as the one described in the post.</p>\n<p>A reoccurring theme of covenant scripts is that they maintain some static part of the script (e.g., <code>CANON</code>, <code>PROG</code>) throughout time, while other parts mutate and represent the covenant dynamic state.<br>\nI propose that a covenant script will contain a script-header specifying a mask which will allow to extract the static part of the script. This can be done by specifying masking ranges (e.g., ignore bytes <code>22-45</code>) or by inserting special markers within the script itself (the latter is useful if the dynamic parts are also dynamic in size).<br>\nAnother complementing operation would be hashing the header/markers along with the static parts they signify, such that any script following the covenant will hash to the same “covenant identifier”.</p>\n<p>I can see two immediate use-cases for such a mechanism.</p>\n<ol>\n<li>Async message passing: a covenant transaction (proof transaction in the rollup context), can contain a payload marked with a special <code>SYSTEM</code> prefix indicating that L1 authorized this payload. The payload will include a sender field with the sender id, and the message itself. L1 will verify (as a transaction acceptance rule), that the sender id corresponds to the standard covenant identifier of one of the spent input scripts. This means that other rollups interested in messages from this sender will only need to follow such messages (as long as they performed a one-time check/audit verifying that the underlying covenant is well-formed and cannot be forged)</li>\n<li>Delegation scripts: the standardization essentially captures the notion of “well-formed” defined in the linked post. The <code>DELEGATE</code> script can thus be simplified to verify that the primary input has the desired covenant identifier.</li>\n</ol>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-01-31T12:53:42.823Z",
        "updated_at": "2025-01-31T13:52:33.664Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/thoughts-about-covenant-and-async-message-standardization/293/1",
        "category_id": 11
      },
      {
        "post_id": 500,
        "post_number": 5,
        "topic_id": 274,
        "topic_title": "Subnets sequencing commitments",
        "topic_slug": "subnets-sequencing-commitments",
        "content": "<p>Following the conversation, I’d like to make a final proposal/recap combining all: hierarchic subnets; subnet expiry; global mergeset sequence number and elements of KIP-6.</p>\n<p>I’ll use <span class=\"math\">\\psi, \\psi_i</span> as short for <span class=\"math\">SQC, SQC_i</span>.</p>\n<hr>\n<p>Notation:</p>\n<ul>\n<li><span class=\"math\">mergeset(B) := [t_1, ..., t_n]</span> denotes the sequence of <span class=\"math\">n</span> txs accepted by block <span class=\"math\">B</span> in order</li>\n<li>for each subnet <span class=\"math\">i</span>, let <span class=\"math\">t_{i_1}...t_{i_k}</span> be those transactions within <span class=\"math\">mergeset(B)</span> that belong to subnet <span class=\"math\">i</span>.<br>\nNote that <span class=\"math\">i_j</span> is the original index within <span class=\"math\">mergeset(B)</span> of the <span class=\"math\">j</span>'th subnet tx.</li>\n<li>we (abuse notation and) say <span class=\"math\">i \\in mergeset(B)</span> iff there exists a tx in <span class=\"math\">mergeset(B)</span> which belongs to subnet <span class=\"math\">i</span></li>\n<li><span class=\"math\">B.sp_i</span> is the <span class=\"math\">i</span>'th selected parent starting from <span class=\"math\">B</span>. I.e., <span class=\"math\">B.sp = B.sp_1, (B.sp).sp = B.sp_2</span> and so on.</li>\n<li>Denote <span class=\"math\">expiry(i, B) := bluescore(B) - bluescore(B.sp_m)</span>; where <span class=\"math\">B.sp_m</span> is the most recent chain ancestor of <span class=\"math\">B</span> such that <span class=\"math\">i \\in mergeset(B.sp_m)</span>; or <span class=\"math\">genesis</span> if no such block.</li>\n</ul>\n<hr>\n<p>Definition 1:</p>\n<p><span class=\"math\">\\psi_i(B) :=</span></p>\n<ul>\n<li><span class=\"math\">\\psi_i(B.sp)</span>; iff <span class=\"math\">i \\notin mergeset(B)</span></li>\n<li><span class=\"math\">H(\\psi_i(B.sp), (t_{i_1}, i_1), ..., (t_{i_k}, i_k))</span>; otherwise</li>\n</ul>\n<hr>\n<p>Definition 2:</p>\n<p><span class=\"math\">\\psi(B) := MR(\\psi(B.sp_1), \\psi(B.sp_2), ..., \\psi(B.sp_{2^m}), </span> <span class=\"math\"> \\psi_i(B) \\text{ }|</span> <span class=\"math\">\\text{ for all  } i \\text{  s.t.  }  expiry(i, B) \\le F)</span> where <span class=\"math\">F</span> is some expiry constant, and <span class=\"math\">m</span> is set in a way detailed later.</p>\n<hr>\n<p>The above definition of <span class=\"math\">\\psi(B)</span> accomplishes the following:</p>\n<ul>\n<li>Subnets can prove their execution in <span class=\"math\">O(\\text{subnet activity})</span> time as long as there’s at least a single subnet tx every <span class=\"math\">F</span> epoch (nit: each proof requires additional <span class=\"math\">O(log(\\text{#non-expired-subnets}) + log(\\text{mergeset-tx-limit}))</span>)</li>\n<li>The size of the maintained subnet tree (supporting the construction of <span class=\"math\">\\psi(B)</span>) is bounded by the number of non-expired subnets which in turn is bounded by <span class=\"math\">F\\cdot \\text{mergeset-tx-limit}</span> (assuming a worst-case scenario where each tx in the epoch belongs to a unique subnet).</li>\n<li>By setting <span class=\"math\">F\\approx \\text{pruning-period-len}</span> we get a balance where tree storage requirements are only a fraction of the storage required for keeping header and transaction data in the pruning period on the one hand, while maintaining efficient subnet proving requires only a single tx per <span class=\"math\">F</span> epoch on the other hand (which can be thought of as minimal tax paid by any reasonable non-spam subnet).</li>\n<li>The addition of the global indices <span class=\"math\">i_1, ..., i_k</span> hashed within def. 1 allows proving cross-subnet DAG order relations.</li>\n<li>The series <span class=\"math\">\\psi(B.sp_1), \\psi(B.sp_2), ..., \\psi(B.sp_{2^m})</span> embedded within <span class=\"math\">\\psi(B)</span> is set such that <span class=\"math\">B.sp_{2^{m+1}}</span> would be below the previous pruning/posterity point (see <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0006.md\">KIP-6</a> by <a class=\"mention\" href=\"/u/deshe2\">@Deshe2</a>). This construction can be used to prove arbitrary transaction acceptance in the L1 DAG throughout history with the same <span class=\"math\">θ(log(N)loglog(N))</span> complexity suggested in the KIP (technically this might require switching <span class=\"math\">H</span> to <span class=\"math\">MR</span> in def. 1).<br>\nNote that we do this without going through the header-chain, thus not providing <em>PoChM</em>  explicitly, nor the possibility to prove transaction <em>inclusion</em>.</li>\n</ul>\n<hr>\n<p>The mention of using SMR’s <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a> is not in relation to non-inclusion proofs but rather for efficiently managing a mutating key-value tree with log depth and with in-consensus semantics. The key here is the subnet id and <span class=\"math\">\\psi_i(B)</span> is the mutating value. Keeping a dense tree for all active (non-expired) subnets, would require mutations and tree rebalancing strategies which are hard to form in-consensus. Reinvestigating this, I guess we can also use a Patricia Tree (with another hashing layer over keys) which might provide better density tradeoffs.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-01-30T15:18:12.735Z",
        "updated_at": "2025-01-31T11:46:30.681Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/5",
        "category_id": 11
      },
      {
        "post_id": 503,
        "post_number": 6,
        "topic_id": 274,
        "topic_title": "Subnets sequencing commitments",
        "topic_slug": "subnets-sequencing-commitments",
        "content": "<aside class=\"quote no-group\" data-username=\"michaelsutton\" data-post=\"5\" data-topic=\"274\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/michaelsutton/48/130_2.png\" class=\"avatar\"> michaelsutton:</div>\n<blockquote>\n<p>Subnets can prove their execution in <span class=\"math\">O(\\text{subnet activity})</span> time as long as there’s at least a single subnet tx every <span class=\"math\">F</span> epoch</p>\n</blockquote>\n</aside>\n<p>I noticed that expired subnets can prove inactivity by providing a non-inclusion proof every <span class=\"math\">F</span> epoch. I.e., by showing subnet <span class=\"math\">i</span> is not included in <span class=\"math\">\\psi(B)</span>, you essentially prove there was no activity for the <span class=\"math\">F</span> epoch prior to <span class=\"math\">B</span>. So a subnet inactive for <span class=\"math\">T</span> time will only require <span class=\"math\">O(T/F)</span> proof steps.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-01-31T11:58:41.571Z",
        "updated_at": "2025-01-31T11:58:41.571Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/6",
        "category_id": 11
      },
      {
        "post_id": 429,
        "post_number": 1,
        "topic_id": 258,
        "topic_title": "L1<>L2 canonical bridge (entry/exit mechanism)",
        "topic_slug": "l1-l2-canonical-bridge-entry-exit-mechanism",
        "content": "<p>Context/prerequisite: <a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208\">part 1 – based rollups design</a>.</p>\n<p>A primary objective of the based rollup design is to support the entry and exit of L1 KAS funds to and from the L2—a mechanism often referred to as a canonical bridge.</p>\n<p>The end result should be a bridged KAS token (henceforth referred to as <code>BKAS</code>), which can be used on L2 and has a 1:1 relation with native L1 KAS. From the perspective of L1, this means there will be a “pool” of native KAS allocated across several UTXOs, which is locked and owned by the L2. The internal distribution of this pool is managed by the L2 state. A user should be able to initiate an “entry” by sending KAS to their L2 account via an L1 operation—sending KAS to a designated, well-known L2 address while specifying (via the payload) their destination address within L2. Complementarily, they should be able to initiate an “exit” request, demanding that the L2 send their <code>BKAS</code> back from L2 to an L1 address.</p>\n<h3><a name=\"p-429-distinguishing-semantics-of-entry-and-exit-operations-1\" class=\"anchor\" href=\"#p-429-distinguishing-semantics-of-entry-and-exit-operations-1\"></a>Distinguishing semantics of Entry and Exit operations</h3>\n<p>From the perspective of L1, entry and exit operations serve fundamentally different purposes and feature distinct behaviors. An entry operation is an L1 transaction that is validated inline by L1 itself, making it effective immediately for both the transfer of funds on L1 and the corresponding state update on L2. In contrast, an exit operation requires internal L2 authorization, which cannot be directly validated by L1. Instead, it depends on the submission of a zero-knowledge proof (ZKP) to confirm its validity and take effect on L1.</p>\n<p>This semantic distinction highlights a key operational difference: entry funds can be used immediately on L2, even before the entry operation has been formally proven via ZKP. For example, an entry operation could provide the necessary collateral for a subsequent proof and exit request by another user, even if its own proof is still pending. This immediate usability of entry funds reflects the simpler integration of entry operations, contrasting with the more complex requirements of exit operations.</p>\n<p>Below, we dive into the technical details and complexities of designing a bridge to support entry and exit operations. Specifically, canonical bridges have been successfully designed in the industry and are relatively easy to come up with when the L1 is an SC layer as well. Our goal here is constructing such a bridge above the UTXO/scripting model. We focus on two key aspects:</p>\n<ul>\n<li>Using Kaspa’s scripting capabilities to form user-friendly entry addresses that can be easily used by users. This set of addresses and their corresponding UTXOs will effectively define an “L2 virtual wallet” which can be used by provers for managing L2-owned KAS funds on L1.</li>\n<li>Addressing the issue of many exit operations resulting from a single proof transaction, which might surpass local L1 transaction size limits.</li>\n</ul>\n<blockquote>\n<p><strong>Note:</strong> Exit operations are one type of outcome that must be transmitted from L2 to L1. More generally, this primitive is also required for asynchronous messages sent over L1 between rollup instances. These outcomes, verified by L1 as part of the ZKP check, attest to state reads within L2 and can be shared in plain text with other L2s. This process can be viewed as part of an abstract “outbox” that the prover is obligated to deliver to L1.</p>\n</blockquote>\n<hr>\n<h3><a name=\"p-429-static-entry-addresses-and-the-primary-state-commitment-utxo-2\" class=\"anchor\" href=\"#p-429-static-entry-addresses-and-the-primary-state-commitment-utxo-2\"></a>Static entry addresses and the primary state commitment UTXO</h3>\n<p>As detailed in <a href=\"https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208\">part 1</a>, the state commitment UTXO of a rollup instance contains an ever-changing dynamic <code>p2sh</code> address. Conceptually, this address cannot be used as the receiving address for entry operations, both because it is short-lived (expiring with the next proof) and because spending it inherently requires a full <code>ZKP</code>. Instead, we define static addresses that delegate their spending authorization to an existing <code>ZKP</code> provided by primary proof scripts. This motivates the following introduction of static “delegation” <code>p2sh</code> addresses, which directly resolve both issues: the address is no longer dynamic, and its spending signature no longer requires full <code>ZKP</code> verification.</p>\n<h4><a name=\"p-429-specification-of-static-delegation-scripts-3\" class=\"anchor\" href=\"#p-429-specification-of-static-delegation-scripts-3\"></a>Specification of static delegation scripts</h4>\n<p>To focus on solving the foundational problem, I assume a single state commitment UTXO representing the full rollup state. Extending the discussion below to multiple state commitment UTXOs per rollup introduces additional complexities that require careful consideration and is therefore out of scope for this post.</p>\n<h5><a name=\"p-429-preliminaries-4\" class=\"anchor\" href=\"#p-429-preliminaries-4\"></a>Preliminaries</h5>\n<p>To establish a foundation for the discussion, let us revisit the definition of key components related to the state commitment process:</p>\n<ul>\n<li>Denote <span class=\"math\">SCU_i</span> as the <span class=\"math\">i</span>'th State Commitment UTXO of a given rollup instance since inception. In other words, <span class=\"math\">[SCU_0, \\dots, SCU_i, \\dots, SCU_n]</span> forms a chain of UTXOs where the <span class=\"math\">i</span>'th proof transaction spends <span class=\"math\">SCU_{i-1}</span> and creates <span class=\"math\">SCU_i</span>.</li>\n<li>Denote <span class=\"math\">key(SCU_i)</span> as the key identifier of this UTXO on L1. As per standard UTXO management, this key is composed of the following tuple (often referred to as the outpoint): <span class=\"math\">(\\text{prev tx id}, \\text{output index})</span>. In other words, the UTXO entry is uniquely identified by the transaction that outputs it and the index of that specific output within the transaction.</li>\n<li>Similarly, denote <span class=\"math\">script(SCU_i)</span> as the <code>p2sh</code> script specified within <span class=\"math\">SCU_i</span>.</li>\n<li>Define the rollup’s canonical L1 script as <code>CANON</code> (see pseudo code below).</li>\n<li>Recall that <code>PROG</code> is the hash of the permanent program L2 is obligated to execute.</li>\n</ul>\n<h5><a name=\"p-429-canonical-state-commitment-script-5\" class=\"anchor\" href=\"#p-429-canonical-state-commitment-script-5\"></a>Canonical state commitment script</h5>\n<p>To provide a more rigorous explanation of the canonical script’s role in verifying state commitments, its process is detailed below in pseudo-code (reiterating** parts of the code described in part 1 under ‘Proof transaction’). This expanded explanation also integrates the p2sh preimage verification step, ensuring a complete picture:</p>\n<p>** Note that I’m adopting <a class=\"mention\" href=\"/u/hashdag\">@hashdag</a>’s terminology (<a href=\"https://research.kas.pa/t/updateable-list-of-l1-l2-topics-to-flesh-out-before-finalizing-design/237\">here</a>) and referring to the previously known <code>history_merkle_root</code> as the <code>sequencing_commitment</code> (<code>seq_in</code> in the code below).</p>\n<pre data-code-wrap=\"javascript\"><code class=\"lang-javascript\">Inputs: script_hash, sig_script\n-------------------------------\n\n[&lt;sig_data&gt;, &lt;redeem_script&gt;] = sig_script\n// Standard p2sh preimage verify\nverify OpBlake2b(input: redeem_script) == script_hash\n\n// Decompose redeem_script into its canonical script (only opcodes) and\n// its data arguments. \n// The L2 PROG will appear as a push data opcode in the canonical\n// script, creating a strong binding between them. Additionally, the\n// script encodes the incoming L2 state commitment (state_in), and\n// the incoming L1 sequencing commitment (seq_in)\n[&lt;CANON&gt;, [&lt;PROG&gt;, &lt;state_in&gt;, &lt;seq_in&gt;]] = redeem_script\n\n// Execute the verified preimage script (converted into pseudo code for \n// brevity).\n// I.e., CANON is the actual pseudo code listed below in its stack-based\n// script language format (this transfer to execution of the inner script \n// is part of the standard p2sh processing)\nExecute CANON(PROG, state_in, seq_in, sig_data):\n  // Extract arguments from signature data\n  [&lt;out_script&gt;, &lt;chainblock_hash&gt;, &lt;zkp&gt;] = sig_data\n \n  // Verify the output script preimage. OpTxOutputSpk is a new Kip10 \n  // introspection opcode and is a crucial component in defining \n  // this \"covenant\" between the spending and the output scripts\n  verify OpBlake2b(input: out_script) == OpTxOutputSpk(index: 0)\n  \n  // Decompose the out script, which is expected to be in the same format\n  // as the input script and deviate only in the dynamic elements (state\n  // and seq). Note that this decomposition might require new data-masking\n  // opcodes.\n  [&lt;out_canon&gt;, [&lt;out_prog&gt;, &lt;state_out&gt;, &lt;seq_out&gt;]] = out_script\n\n  // Verify PROG is preserved\n  verify out_prog == PROG\n\n  // Verify the canonical script itself is preserved (excluding\n  // the extracted variables state_out and seq_out). This can be\n  // thought of as the \"covenant\" by which the output script must\n  // follow the input script   \n  verify out_canon == CANON  \n \n  // Verify L1 sequencing root anchoring\n  verify OpChainBlockHistoryRoot(hash: chainblock_hash) == seq_out\n\n  // Verify the state transition ZK proof\n  OpZkVerify(prog: PROG, proof: zkp, \n             proof_pub_inputs: [state_in, state_out, seq_in, seq_out]) \n  // ^ omitting other auxiliary verification data\n</code></pre>\n<p>The canonical script enforces the correctness of state transitions, ensuring both the input and output states, as well as sequencing commitments, are consistent with the L2 program (<code>PROG</code> ) and the <code>ZKP</code>.</p>\n<h5><a name=\"p-429-static-delegation-script-6\" class=\"anchor\" href=\"#p-429-static-delegation-script-6\"></a>Static delegation script</h5>\n<p>To support entry operations without directly relying on dynamic state commitments, the following delegation script is proposed. This static script delegates certain responsibilities to the canonical script while remaining independent of the current state or history.</p>\n<pre data-code-wrap=\"javascript\"><code class=\"lang-javascript\">Inputs: script_hash, sig_script\n-------------------------------\n\n[&lt;sig_data&gt;, &lt;redeem_script&gt;] = sig_script\n// Standard p2sh preimage verify\nverify OpBlake2b(input: redeem_script) == script_hash\n\n// Decompose redeem_script into its script part and data arguments\n[&lt;DELEGATE&gt;, [&lt;PROG&gt;, &lt;CANON&gt;]] = redeem_script\n\n// Execute the delegation script\nExecute DELEGATE(PROG, CANON, sig_data):\n  [&lt;primary_script&gt;] = sig_data\n\n  // Verify primary script preimage\n  verify OpBlake2b(input: primary_script) == OpTxInputSpk(index: 0)\n\n  // Decompose primary script\n  [&lt;canon_primary&gt;, [&lt;prog_primary&gt;, ...]] = primary_script\n\n  // Verify PROG is preserved\n  verify prog_primary == PROG\n\n  // Verify the delegation targets a correct canonical script\n  verify canon_primary == CANON\n</code></pre>\n<p>The delegation scheme builds on the inherent property that a transaction is only accepted when all its inputs are validated as correctly spent. Through this mechanism, the delegator ensures that the primary input script meets specific requirements, authorizing its own spending if those conditions are satisfied.</p>\n<p>The delegation script described above achieves a static structure by avoiding reliance on specific sequencing or dynamic state commitments, as shown by the <code>redeem_script</code> preimage in the code snippet. However, without showcasing further properties, the scheme remains vulnerable to the following attack vector.</p>\n<p><strong>Attack vector</strong>. Alice sends funds from a standard L1 Schnorr address to a script structured correctly with <code>CANON</code> and <code>PROG</code> but using fabricated <code>state</code> and <code>seq</code> commitments. She then constructs a proof transaction, using the resulting UTXO as the primary input and adding additional inputs from static delegation addresses, redirecting the KAS funds to the primary state-commitment output. The result is that the funds are locked in a malformed state-commitment UTXO distinct from the authentic proof chain. Even if she uses a valid <code>(state, seq)</code> pair, the problem persists as she has effectively created a separate proof chain.</p>\n<p><strong>The challenge</strong>. Addressing this attack requires incorporating information beyond the script itself. This is because L1 cannot enforce restrictions on the “entrance” to a script covenant—for instance, any user can send funds to an opaque <code>p2sh</code> address. Consequently, authentic and forged state-commitment UTXOs are indistinguishable at the script level. Solutions based on static registration of rollup-associated information outside the UTXO set are currently ruled out, as they would introduce significant complexity and compromise the cleanliness and soundness of L1 state management.</p>\n<p><strong>Key-based authentication</strong>. An alternative solution involves leveraging UTXO keys to differentiate between authentic and non-authentic state-commitment UTXOs. A UTXO key is uniquely derived from the transaction that creates it, allowing the authenticity of a UTXO to be tied directly to its creating transaction. One possible approach relies on defining a genesis UTXO in the L2 source code and proving that the current UTXO input belongs to a transaction chain originating from this genesis. While requiring the entire proof chain as a witness in each new proof transition is impractical, a feasible alternative involves using a constant-length suffix of the proof chain, as outlined in the following section (in collaboration with <a class=\"mention\" href=\"/u/freshair08\">@FreshAir08</a>; based on preliminary discussions with <a class=\"mention\" href=\"/u/reshmem\">@reshmem</a> &amp; <a class=\"mention\" href=\"/u/hashdag\">@hashdag</a> as well).</p>\n<h4><a name=\"p-429-key-based-state-commitment-utxo-authentication-7\" class=\"anchor\" href=\"#p-429-key-based-state-commitment-utxo-authentication-7\"></a>Key-based state-commitment UTXO authentication</h4>\n<p>We propose the following scheme as in illustrative example of a construction providing state-commitment authenticity.</p>\n<ul>\n<li>A <code>GENESIS</code> UTXO is hardcoded in L2 PROG via its source code</li>\n<li>The <code>CANON</code> part of the canonical script is hardcoded in L2 PROG as well. Note that this is a pure script without any dependency on PROG (thus no hash cycles)</li>\n<li>The <code>ZKP</code> receives as public input the key of the currently spent state-commitment UTXO, i.e., <span class=\"math\">key(SCU_{i-1})</span></li>\n<li>The corresponding L2 prover program (producing this <code>ZKP</code>) receives as private witness the full <span class=\"math\">i-2, i-1</span> proof transactions and the preimage of <span class=\"math\">script(SCU_{i-2})</span></li>\n<li>The following logic is executed as part of the program:</li>\n</ul>\n<pre data-code-wrap=\"javascript\"><code class=\"lang-javascript\">// L2 program running for each proof transition\n//\n// Arguments: input_key - the UTXO key of the currently\n//                        spent state-commitment UTXO\n//            source_tx - the source transaction of input_key \n//                        (i.e., proof transaction i-1)\n//            source_script - the preimage of the script hash used as\n//                            input for source_tx \n//            script_affirming_tx - proof transaction i-2; provided for\n//                                  affirming source_script (since the\n//                                  input of i-1 does not specify it)\nPROG(...,pub_inputs:     [..., input_key], \n         private_inputs: [..., source_tx, source_script, script_affirming_tx]):\n  ...\n  // First, verify that input_key is indeed an output of source_tx\n  verify (blake2b(source_tx), 0) == input_key\n\n  if source_tx.inputs[0].outpoint == GENESIS:\n    // Allow entering the covenant only via the hardcoded genesis UTXO\n    pass\n  else:\n    // Otherwise, we must verify that input_key was produced as part of the\n    // covenant. To do that we acquire the incoming script through the i-2\n    // transaction, decompose it, and verify that it follows the expected \n    // canonical script  \n \n    // Verify the linkage between the two transactions (i-2 -&gt; i-1)\n    verify (blake2b(script_affirming_tx), 0) == source_tx.inputs[0].outpoint\n    // Verify that source_script is the preimage \n    verify blake2b(source_script) == script_affirming_tx.outputs[0].script\n    // Decompose the script \n    [&lt;canon&gt;, [&lt;_prog&gt;, &lt;_state&gt;, &lt;_seq&gt;]] = source_script \n    // Verify it follows the expected canonical script\n    verify canon == CANON\n  ...\n</code></pre>\n<p><strong>Definition 1</strong>: A state commitment UTXO is <em>well-formed</em> if its script preimage can be decomposed into <code>[&lt;CANON&gt;, [&lt;PROG&gt;, &lt;*&gt;, &lt;*&gt;]]</code>.</p>\n<p><strong>Definition 2</strong>: A state commitment UTXO is <em>forged</em> if it hasn’t originated from the <code>GENESIS</code> UTXO, i.e., <code>GENESIS</code> was never part of a transaction chain leading to it.</p>\n<p><strong>Claim 1</strong>: A well-formed forged state commitment UTXO is unspendable.</p>\n<p><strong>Proof</strong>: Assume for contradiction that such a UTXO exists. Then there must be a maximal transaction <span class=\"math\">X</span> on the chain creating it, where the first input script <span class=\"math\">S_1</span> is not well-formed (or at the very least missing altogether, since root transactions are always coinbase). By the maximality of <span class=\"math\">X</span>, it follows that the output of <span class=\"math\">X</span>, used as the first input for the following transaction, is well-formed. Denote this output as <span class=\"math\">S_2</span>.</p>\n<p>There are two cases to consider:</p>\n<ol>\n<li><strong>Case 1: <span class=\"math\">S_1</span> is partially malformed</strong><br>\nHere, <span class=\"math\">S_1</span> can be decomposed into <code>[&lt;CANON&gt;, [&lt;PROG'&gt;, &lt;*&gt;, &lt;*&gt;]]</code>, but it uses an incorrect <code>PROG'</code>. In this case, <span class=\"math\">X</span> would fail L1 verification because <span class=\"math\">S_2</span> will decompose into <code>[&lt;CANON&gt;, [&lt;PROG&gt;, &lt;*&gt;, &lt;*&gt;]]</code>, and the L1 <code>CANON</code> execution would attempt to verify that <code>PROG'</code> is preserved, resulting in failure.</li>\n<li><strong>Case 2: <span class=\"math\">S_1</span> is completely malformed</strong><br>\nHere, <span class=\"math\">S_1</span> cannot even be decomposed into <code>[&lt;CANON&gt;, [&lt;*&gt;, &lt;*&gt;, &lt;*&gt;]]</code>. However, <span class=\"math\">S_2</span> <em>is</em> well-formed. From our contradiction assumption, <span class=\"math\">S_2</span> must be spendable. The spender must provide a valid <code>ZKP</code> based on the real <code>PROG</code>. However, the prover must include <span class=\"math\">X</span> as a witness, and during <code>PROG</code> execution, both branches will fail:<br>\n(i) <span class=\"math\">X</span>'s first input key cannot be <code>GENESIS</code>, as it is forged; and<br>\n(ii) <span class=\"math\">S_1</span> cannot be decomposed into <code>CANON</code>, so it will fail the final line of execution.</li>\n</ol>\n<p><strong>Corollary</strong>: Delegation scripts cannot be redirected to a forged proof chain.</p>\n<p><strong>Proof</strong>: The last line of <code>DELEGATE</code> verifies that the primary input script is well-formed, thus by Claim 1 it can only be spent if it is not forged.</p>\n<h5><a name=\"p-429-l2-initialization-procedure-8\" class=\"anchor\" href=\"#p-429-l2-initialization-procedure-8\"></a>L2 initialization procedure</h5>\n<p>To initialize an L2 system with this scheme, the process begins by sending KAS on L1 to an ordinary address (e.g., Schnorr) controlled by L2 initiators. The resulting UTXO key from this transaction is then added to the L2 source code and designated as <code>GENESIS</code>. This key serves as the starting point for the rollup’s transaction chain.</p>\n<p>The next step is to create the canonical script <code>CANON</code> and add it as a constant to the L2 source code. While <code>CANON</code> is not directly part of the L2 system, it is essential for enabling chain link verification between transactions. Once <code>CANON</code> is defined, the L2 program is compiled to produce <code>PROG</code>, which serves as the main logic for generating proofs.</p>\n<p>Using <code>CANON</code>, <code>PROG</code>, and the initial <code>state</code> and <code>seq</code>, the initial state-commitment script is composed. This script represents the starting point for the rollup’s state and sequencing commitments. An L1 transaction is then performed to transfer funds from the <code>GENESIS</code> UTXO to the newly created state-commitment script, marking the rollup’s formal initiation.</p>\n<p>After the transaction is confirmed, a proof of its acceptance on L1 should be saved as part of the L2’s integrity data. This proof can also be shared with new L2 nodes to establish trust in the L2 initialization process. The combination of L1-approved proof transactions and L2-verified chain links ensures that only authenticated state-commitment UTXOs pass <code>ZKP</code> verification on L1, effectively mitigating the attack vector of redirecting delegated address funds.</p>\n<h5><a name=\"p-429-syncing-new-l2-nodes-9\" class=\"anchor\" href=\"#p-429-syncing-new-l2-nodes-9\"></a>Syncing new L2 nodes</h5>\n<p>This scheme provides a trustless mechanism for fully syncing new L2 nodes from the recent state. By verifying the correct state-commitment UTXO and the suffix of the transaction chain leading to it (e.g., the last two transactions), new nodes can use the authenticated L2 state commitment on L1 to confirm the newly synced L2 state is consistent with this commitment.</p>\n<hr>\n<h3><a name=\"p-429-exit-operations-as-proof-outputs-10\" class=\"anchor\" href=\"#p-429-exit-operations-as-proof-outputs-10\"></a>Exit operations as proof outputs</h3>\n<p>(Thx <a class=\"mention\" href=\"/u/freshair08\">@FreshAir08</a> for writing the majority of this section.)</p>\n<p>After a user issues a “withdrawal” transaction to L2, the associated funds are no longer available in L2 but remain in the L2-owned addresses. When a proof is submitted, it must ensure and enforce the transfer of these funds from the addresses to the requested L1 address.</p>\n<p>Conceptually, these pending withdrawal transactions form an <em>outbox</em> of exit operations, which can be inferred from the executed L1 transactions within the proved period. To support this mechanism, we propose the following additions:</p>\n<ul>\n<li>L1 <code>CANON</code> script change: modify <code>CANON</code> to compute the cumulative hash of all outputs in the current transaction except the primary state commitment output (similar to the Schnorr sighash process). This hash will be passed to <code>OpZkVerify</code> as an additional public proof input.</li>\n<li>L2 <code>PROG</code> change: update <code>PROG</code> to compile a list of expected proof outputs from the outbox and compute their cumulative hash using the same method employed by <code>CANON</code>. This hash is then verified against the public input provided in the proof.</li>\n</ul>\n<hr>\n<h4><a name=\"p-429-n-to-const-problem-11\" class=\"anchor\" href=\"#p-429-n-to-const-problem-11\"></a><span class=\"math\">N</span> to Const problem</h4>\n<p>So far, we have assumed that the prover can include all pending exit outputs in a single proof transaction. However, due to mass limitations (e.g., KIP9), these added outputs could potentially take up a significant amount of block space. While higher fees for the prover or slower L2 progress are concerns, the main issue is that the transaction might exceed the block size limit, causing a deadlock in the L2. In extreme cases, even a single mergeset could congest an entire block.</p>\n<p>We explore two possible solutions to address this issue:</p>\n<ol>\n<li>\n<p>Instead of directly including all L1 addresses in outputs, public keys (or more general scripts) can be used. Provers would transfer funds to special-purpose <code>p2sh</code> addresses that represent all public keys associated with the withdrawals. These addresses would allow each public key to spend only its share of the funds, similar to a KIP10-like mechanism. The public keys could be stored in a Merkle tree within the redeem script, and an additional opcode might be added to verify Merkle witnesses efficiently.</p>\n</li>\n<li>\n<p>If funds are not immediately repatriated to L1, they can remain in a designated L2 outbox, which acts as an extension to the L2 state. The program would enforce that the active L2 state cannot advance until the outbox has been cleared. If the outbox becomes too congested, the L1 sequencing commitment would remain unchanged, and only the outbox would be updated. Adapting the hash commitments discussed earlier to this structure would be straightforward.<br>\nA more lenient variation of this idea could allow state advancement with partial outbox clearing. For example, provers might be required to clear the outbox at twice the rate they add to it. This restriction could activate only once the outbox exceeds a predefined congestion limit.</p>\n</li>\n</ol>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-01-13T12:49:19.647Z",
        "updated_at": "2025-01-13T20:28:30.653Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258/1",
        "category_id": 11
      },
      {
        "post_id": 490,
        "post_number": 2,
        "topic_id": 258,
        "topic_title": "L1<>L2 canonical bridge (entry/exit mechanism)",
        "topic_slug": "l1-l2-canonical-bridge-entry-exit-mechanism",
        "content": "<aside class=\"quote no-group\" data-username=\"michaelsutton\" data-post=\"1\" data-topic=\"258\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/michaelsutton/48/130_2.png\" class=\"avatar\"> michaelsutton:</div>\n<blockquote>\n<p>Key-based state-commitment UTXO authentication</p>\n</blockquote>\n</aside>\n<p>Worth mentioning that after writing this post I bumped into a similar authenticity <a href=\"https://starkware.co/blog/implementing-a-bridge-covenant-on-op-cat-bitcoin/\">proposal</a> by starkware:</p>\n<blockquote>\n<p>To address this, we perform what is called a “genesis check.” Essentially, we make the aggregation covenant check its previous transaction and the transaction preceding that one — that is, its ancestor transactions. The covenant verifies that these transactions contain the same covenant script and perform the same checks. In this way, we achieve an inductive transaction history check. Because both of the previous transactions performed the same checks as this covenant does, we know that the ancestors of that transaction did the same, all the way back to the leaf (i.e., the genesis transaction).</p>\n</blockquote>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2025-01-28T10:40:09.751Z",
        "updated_at": "2025-01-28T10:40:09.751Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258/2",
        "category_id": 11
      },
      {
        "post_id": 444,
        "post_number": 3,
        "topic_id": 247,
        "topic_title": "Fees and throughput regulation dynamics",
        "topic_slug": "fees-and-throughput-regulation-dynamics",
        "content": "<p>Do I understand correctly why “revenue and fees go to L1 sequencers, not to L2 provers” – is it precisely because it “adds complexity for sequencers (miners) optimizing their blocks”? Before, miners were incentivized to perform only the optimization to select transactions based on storage-mass, now, in addition to that, they will also perform transaction selection based on gas amount. While economics and profitability of miner business very much depends on the demand and the competition, we can think of ZK provers as almost having constant costs to operate - therefore, they are not in danger if they are paid just enough?</p>\n<p>Or maybe you say a more general thing, since you seem to downplay the actual amount of additional work they will do in practice, that it is only miners that are incentivised in the system for increased demand for block space, no matter where the demand is coming from?</p>",
        "raw_content": "",
        "author": "oudeis",
        "created_at": "2025-01-19T22:00:02.770Z",
        "updated_at": "2025-01-19T22:00:02.770Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/3",
        "category_id": 11
      },
      {
        "post_id": 239,
        "post_number": 1,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>This thread is here to house a discussion about KIP9 and our approach to mitigating UTXO bloat using quadratic costs via local constraints. This is to house discussions both about <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0009.md\">KIP9</a> and the research paper (which would be linked here as soon as the preprint is published).</p>",
        "raw_content": "",
        "author": "Deshe2",
        "created_at": "2024-03-29T08:46:56.387Z",
        "updated_at": "2024-03-29T09:10:04.936Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/1",
        "category_id": 1
      },
      {
        "post_id": 247,
        "post_number": 2,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>Hello,</p>\n<p>Thank you for the great work in (and writeup of) KIP9. The formulas look so “simple” but I am sure a whole lot of thinking and considerations went into this work.</p>\n<p>However my question is more legalese/practical. It concerns this section of KIP9: \" <em>The downside to this solution is that the merchant must constantly have a hot wallet available and cooperate with the customer to create a mutually signed transaction. In a following KIP, we will specify <em>auto-compounding</em> wallet addresses, where UTXOs owned by such addresses will allow anyone to add to their balance without the owner of the UTXO having to sign it. Among other applications, this mechanism will allow the millionaire to purchase ice cream as described above, using her wallet alone.</em>\"</p>\n<p>Would the merchant be able to set limits on people (addresses) that can append to their hot wallet? For example, let us say there is an address widely known to be associated with an hacker or some rogue organization. I would imagine that it needs to be possible for a merchant to prevent that hacker/organization from spending their ill-gotten gains at my crypto-based business. Or another case, someone steals 1 million tokens/coins and in a fit of sudden generosity, she/he distributes the tainted cash to as many hot wallets (owned by innocent merchants) as possible. Now those merchants are roped into potential AML, criminal …investigations.</p>\n<p>I have lost my train of thought on this. I suppose this line of thinking is asking crypto to do more (enforce) than is possible with cash. If I have a tomato stand and a customer comes in, it is not always possible for me to know that they got their cash from an armed robbery, embezzlement or whatever. There are also anti-discrimination laws that could prevent me from denying them service. However,</p>",
        "raw_content": "",
        "author": "dr_g",
        "created_at": "2024-03-30T17:18:18.892Z",
        "updated_at": "2024-03-30T17:18:18.892Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/2",
        "category_id": 1
      },
      {
        "post_id": 248,
        "post_number": 3,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>Like any other cryptocurrency, also in Kaspa, one can send funds to any address without getting any kind of “permission” from the receiver. This has nothing to do with this suggestion.</p>\n<p>The new thing here is that the <em>payer</em> can spend a UTXO owned by the payee, as long as the transaction creates a corresponding output UTXO with a larger amount going to the same payee address. The concern with such an approach would be the ability of spammers to interfere with ongoing payments by spending the same UTXOs. The KIP will circumvent this by allowing the address owner to specify a minimum increment value.</p>\n<p>Other intermediate designs are possible: for instance such addresses can require a semi-sig which requires knowing some secret (which is not the full private key only known to the owner). This way the seller can only share the secret with trusted customers.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-03-30T18:01:46.258Z",
        "updated_at": "2024-03-30T21:26:36.116Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/3",
        "category_id": 1
      },
      {
        "post_id": 249,
        "post_number": 4,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>Thank you, Michael, for this clarification. Really appreciate it.</p>",
        "raw_content": "",
        "author": "dr_g",
        "created_at": "2024-03-30T21:41:28.291Z",
        "updated_at": "2024-03-30T21:41:28.291Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/4",
        "category_id": 1
      },
      {
        "post_id": 251,
        "post_number": 5,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>Hi,<br>\nSeems like the main point is to reject transactions with a very small but multiple/a lot of outputs<br>\nAnd the KIP proposes a formula to figure out what tx are of this type</p>\n<p>Now the question,<br>\nThe proposed formula naturally rejects tx with very small outputs<br>\nBut since those tx <strong>might</strong> be legal it also proposes a way to send them anyway, by letting a wallet to find a right path for this<br>\nDoesn’t the fact that this path <strong>exists</strong> contradicts the very purpose of blocking those transactions for the first place?<br>\ni.e. the attacker can prepare the needed inputs using his own wallet and then sends the dust</p>",
        "raw_content": "",
        "author": "Tsirkin_Evgeny",
        "created_at": "2024-03-31T09:37:16.775Z",
        "updated_at": "2024-03-31T09:37:16.775Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/5",
        "category_id": 1
      },
      {
        "post_id": 252,
        "post_number": 6,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>That’s exactly the point, the path exists, but it’s costly. While the cost is tolerable for a single point to point payment, it will become intolerable for an attacker wishing to create thousands or millions of such entries. Additionally, for a real genuine user wishing to make a micropayment we also analyze and suggest the costless mutual transaction <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0009.md#micropayments\" rel=\"noopener nofollow ugc\">method</a></p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-03-31T13:17:11.168Z",
        "updated_at": "2024-03-31T13:17:59.718Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/6",
        "category_id": 1
      },
      {
        "post_id": 253,
        "post_number": 7,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>I would say that the entire principle behind the solution is that transactions are <strong>not</strong> rejected (In practice they are, for several reasons, e.g. P2P policies or block mass limits, but the validity of the solution does not rely on this limit). Rejecting transactions means setting thresholds, and thresholds are generally bad, because they are arbitrary and usually based on ephemeral metrics such as the <em>current</em> Kaspa price, the <em>current</em> average fee, etc…<br>\nThe idea here is to allow any transaction, but choosing a smart policy for pricing these transactions. The path exists, and if you want to use it even though it is less than optimal, that’s your prerogative. The pricing is set in such a way that you couldn’t do meaningful damage without spending huge resources. So it isn’t about legal/illegal, it is about having to pay for your decisions.</p>",
        "raw_content": "",
        "author": "Deshe2",
        "created_at": "2024-03-31T18:04:28.614Z",
        "updated_at": "2024-03-31T18:04:28.614Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/7",
        "category_id": 1
      },
      {
        "post_id": 260,
        "post_number": 8,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>Thank you guys for the response</p>\n<p>I was thinking - the KIP is trying to adjust the price of the tx <strong>alone</strong><br>\nThis make sense since it makes the calculation stateless and thus cheaper.</p>\n<p>However,<br>\nWhy not make it stateful by i.e. counting the number of total “mass” in last period of time and adjust the price/rules based on that.</p>\n<p>This imitates the real world more precise - if there is a congestion in the network you get higher fees.<br>\nIt is also free from the downside of complicating micropayments in “normal” circumstances.</p>",
        "raw_content": "",
        "author": "Tsirkin_Evgeny",
        "created_at": "2024-04-03T12:32:43.007Z",
        "updated_at": "2024-04-03T12:32:43.007Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/8",
        "category_id": 1
      },
      {
        "post_id": 274,
        "post_number": 9,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>Using dependence has extremely high costs in terms of the ability of the network to scale, and I am not sure I see any benefit. Obtaining such strong guarantees using only local calculation is a magical property one should have an exceptionally good reason to concede.</p>",
        "raw_content": "",
        "author": "Deshe2",
        "created_at": "2024-04-05T15:29:51.413Z",
        "updated_at": "2024-04-05T15:29:51.413Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/9",
        "category_id": 1
      },
      {
        "post_id": 326,
        "post_number": 10,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<p>Hi. Maybe i am a little late on this topic, but let me ask something: If I am a very famous merchant of automated microservices that accepts kaspa. My address will have some UTXO with spend script that anyone of my millions clients to use to spend sending me the same ammount plus the service fee. What about the concurrency in this case? Won’t even I be blocked to spend this UTXO competing with my own clients to use it?</p>",
        "raw_content": "",
        "author": "patrickdalla",
        "created_at": "2024-11-13T00:57:05.451Z",
        "updated_at": "2024-11-13T00:57:05.451Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/10",
        "category_id": 1
      },
      {
        "post_id": 329,
        "post_number": 11,
        "topic_id": 159,
        "topic_title": "Quadratic storage mass and KIP9",
        "topic_slug": "quadratic-storage-mass-and-kip9",
        "content": "<ol>\n<li>The final design implemented <a href=\"https://github.com/kaspanet/rusty-kaspa/pull/487\">here</a> is actually more broad and allows designs such as using secondary secrets, limited one time borrows, and so on, see <a href=\"https://github.com/kaspanet/rusty-kaspa/blob/master/crypto/txscript/examples/kip-10.rs\">examples</a>.</li>\n<li>A merchant with such millions of clients still has a natural concurrency limit. For instance 1k clients online making concurrent payments within timeframes of 1-5 seconds. Such a merchant should manage ~1k additive addresses (each with 1 or more UTXOs) which are distributed and managed tightly between the current clients. If tight management is undesired, he can manage a pool of 100k (&gt;&gt; 1k) addresses and rely on randomness to reduce collisions to be negligible. My main point is – with threshold additive addresses we’re in the realm of engineering creativity.</li>\n</ol>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-11-13T09:56:47.888Z",
        "updated_at": "2024-11-13T09:56:47.888Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/11",
        "category_id": 1
      },
      {
        "post_id": 255,
        "post_number": 1,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>The following proposal draft is intended for fostering discussion and gathering various conceptual and technical feedback before turning into a well-formed KIP.</p>\n<p>This is a follow-up on the <a href=\"https://github.com/kaspanet/kips/blob/master/kip-0009.md#micropayments\">micropayments</a> discussion initiated by KIP9.</p>\n<p>The basic idea is to allow reception of coins in an additive way, where a UTXO owned by the recipient can be spent by the payer transaction (without a signature) as long as the same transaction creates a corresponding output UTXO with a <em>larger</em> amount going to the same recipient address. For an informal background see <a href=\"https://twitter.com/MichaelSuttonIL/status/1774521157745578152\">here</a>.</p>\n<h3><a name=\"opt-in-rather-than-opt-out-1\" class=\"anchor\" href=\"#opt-in-rather-than-opt-out-1\"></a>Opt-in rather than opt-out</h3>\n<p>Allowing every UTXO to be spent this way is an opening for spam attacks, where a user trying to spend his own UTXOs can be outraced by an attacker spending the same UTXO through the new mechanism (by adding negligible amounts to it). This would essentially remove the underlying assumption of almost any wallet which assumes exclusive access to spending his own UTXOs.</p>\n<p>Therefor our suggestion is to allow this only for special addresses (<em>opt-in</em>) where such an address can be configured in various ways to protect from concurrent-spending spam attacks. In the context of this document we’ll name such addresses <em><strong>additive addresses</strong></em>.</p>\n<p><strong>Minimum increment threshold</strong>  The idea is to have the address (or more concretely, the <code>script_pub_key</code>) define a minimum increment value. Any transaction spending the UTXO and adding to it less than the threshold will be invalid. By the owner specifying this threshold depending on his use case and market value conditions, such spam attacks will become costly and reduced to the point they only benefit the owner.</p>\n<p><strong>Semi-signatures</strong> Another possibility is to allow spending only with a semi-sig which requires knowing some secret (which is not the full private key only known to the owner). This way the payee can share the secret only with trusted senders.</p>\n<h3><a name=\"technical-aspects-2\" class=\"anchor\" href=\"#technical-aspects-2\"></a>Technical aspects</h3>\n<h4><a name=\"script-public-key-format-3\" class=\"anchor\" href=\"#script-public-key-format-3\"></a>Script public key format</h4>\n<p>At the consensus engine level, a Kaspa address is encoded into a <code>ScriptPublicKey</code>.</p>\n<p>An additive address with a configured increment threshold can be encoded as a <code>ScriptPublicKey</code> in two possible ways: (i) a special version of <code>p2pk</code> with the threshold appended at the end (<code>u64</code>); (ii) using <code>p2sh</code> and hashing the usual <code>spk</code> + <code>threshold</code> to a single hash.</p>\n<p>The first method has the following main advantage:</p>\n<ul>\n<li>The new <code>spk</code> will have a known prefix (the <code>version</code> + the usual <code>spk</code>), which will allow easy searching for such addresses by wallets (possibly recreated from a seed w/o additional info). It’s worth noting that Kaspa nodes use key-value based databases which support prefix-key searches, thus allowing for such prefix queries in an efficient way.</li>\n</ul>\n<p>The second method is more robust and general and fits into existing script engine schemes.  We can also support both options.</p>\n<h4><a name=\"new-script-engine-opcodes-4\" class=\"anchor\" href=\"#new-script-engine-opcodes-4\"></a>New script-engine opcodes</h4>\n<p>The main missing component in the script engine is the ability to read from the transaction <em>outputs</em>. It seems that the most straightforward way is to add a new opcode which allows reading an output at index (and accessing the <code>amount</code> and recipient <code>spk</code>). This way, the script engine can make the required verification by accessing the spent <code>spk</code>, the recipient <code>spk</code> and the respective amounts. The signature can be used for adding any additional info needed such as providing proof of a  semi-sig etc.</p>\n<h4><a name=\"additional-remarks-5\" class=\"anchor\" href=\"#additional-remarks-5\"></a>Additional remarks</h4>\n<ol>\n<li>The transaction input index should be mapped to a known output index to avoid multiple spends being mapped to the same output (perhaps simply <code>i</code> to <code>i</code>, forcing such inputs to be first)</li>\n<li>It’ll be the first time that validating a sig requires looking at tx outputs (so broader context). This can be crucial. For instance such “signature” validations cannot be cached as valid</li>\n<li>Besides the aforementioned schemes there can be a type of special address which allows additive mining. Meaning miners can use this UTXO as input to coinbase txs and output the reward + previous balance. This can be seen as a minimum increment threshold where the threshold is dynamic and is equal to the current mining reward. To avoid any kind of attack, the input UTXO will be used only if it’s available at the time the coinbase tx is built (by the merging block), otherwise we fallback to normal behavior. This is an important feature for 10BPS where nearly 1M rewards will be mined per day.</li>\n</ol>\n<p>This writeup is an intermediate summary of ongoing discussions with <a class=\"mention\" href=\"/u/ori\">@ori</a>, <a class=\"mention\" href=\"/u/deshe2\">@Deshe2</a>, <a class=\"mention\" href=\"/u/hashdag\">@hashdag</a>, <span class=\"mention\">@biryukovmaxim</span>, <span class=\"mention\">@coderofstuff</span>, <a class=\"mention\" href=\"/u/ey51\">@ey51</a></p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-04-03T10:34:02.737Z",
        "updated_at": "2024-04-03T10:47:23.023Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/1",
        "category_id": 9
      },
      {
        "post_id": 265,
        "post_number": 3,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>Why would the semi signature leak the secret? can’t we provide same guarantees as usual sigs? you mean it’s leaked by showing it to the payer? if so, I don’t think that’s an issue, it’s a type of trust model</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-04-03T14:05:52.030Z",
        "updated_at": "2024-04-03T14:08:06.264Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/3",
        "category_id": 9
      },
      {
        "post_id": 266,
        "post_number": 4,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>As mentioned, the minimum increment threshold [MIT, :)] will be readily adjustable. That is good. If i understand correctly, the MIT will be in units of Kaspa. Is this correct?</p>\n<p>And now a really uneducated question:</p>\n<ol>\n<li>Could the merchant provide the MIT in units of $ or €, £ and the system automatically converts to Kaspa. Cryptocurrencies fluctuate wildly (often within minutes) and widely (intraday moves of up to 100% or more), so it could make sense for some merchants to put a floor in some desired fiat currencies. Obviously the system would need instantaneous knowledge of the fiat value per Kaspa, as well as conversion ratios between variousnfiat currencies.</li>\n</ol>\n<p>Best.</p>",
        "raw_content": "",
        "author": "dr_g",
        "created_at": "2024-04-03T20:07:35.790Z",
        "updated_at": "2024-04-03T20:07:35.790Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/4",
        "category_id": 9
      },
      {
        "post_id": 272,
        "post_number": 7,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>Fyi I’m not a Blockchain expert, simply making observations from a general perspective. Feel free to ignore or criticise if there’s a technical detail invalidating me <img src=\"https://research.kas.pa/images/emoji/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>The problem with the threshold model is that it doesn’t solve this problem: For a high tx volume business, they could be outraced by their non-malicious clients for their additive UTXO too.</p>\n<p>The problem with Semi Sigs is that it does not prevent malicious actors.</p>\n<p>The problem here essentially stems from the inability to have a Critical Section in a decentralised multi-threaded world. In an ideal world, one would have a RW-Lock or something similar where the owner of the wallet would have their Mutex take precedence.</p>\n<p>I have some off-the-cuff ideas for exploring this problem:</p>\n<ol>\n<li>\n<p>Have a designated UTXO which can be the only source of any micro-payments. Once that UTXO is used in a transaction, the output UTXO becomes the next Exclusive Additive UTXO (EAU). If an account has no balance or no previous EAU, create one with a value of 0(?). When the user wants to have exclusive access to the EAU, they simply create a new one and wait for future additive transactions to point to that one.<br>\nThis also avoids the possibility of chaining attacks as described above, as it allows a user to gain exclusive access to the EAU by rerouting traffic away from it in advance.</p>\n</li>\n<li>\n<p>Any transaction by the user involving an EAU could be composed of 2 sub-transactions:</p>\n</li>\n</ol>\n<ul>\n<li>A signal to lock the EAU from any unsigned transactions until the next signed transaction</li>\n<li>The actual transaction signed by the wallet owner, which unlocks UTXO after</li>\n</ul>",
        "raw_content": "",
        "author": "GGabi",
        "created_at": "2024-04-04T14:13:28.544Z",
        "updated_at": "2024-04-04T15:09:22.923Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/7",
        "category_id": 9
      },
      {
        "post_id": 273,
        "post_number": 8,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<aside class=\"quote no-group\" data-username=\"GGabi\" data-post=\"7\" data-topic=\"168\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/letter_avatar_proxy/v4/letter/g/d07c76/48.png\" class=\"avatar\"> GGabi:</div>\n<blockquote>\n<p>a high tx volume business, they could be outraced by their non-malicious clients for their additive UTXO too</p>\n</blockquote>\n</aside>\n<p>A high tx volume business can manage a set of additive addresses per some local unit, like a checkout in the supermarket, or a server session, etc. Basically, If malicious users get penalized and all you need is management of resources, it can be managed by the wallet owner in many ways.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-04-04T16:40:39.855Z",
        "updated_at": "2024-04-04T16:40:39.855Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/8",
        "category_id": 9
      },
      {
        "post_id": 277,
        "post_number": 10,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<aside class=\"quote no-group\" data-username=\"FreshAir08\" data-post=\"9\" data-topic=\"168\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/letter_avatar_proxy/v4/letter/f/b19c9b/48.png\" class=\"avatar\"> FreshAir08:</div>\n<blockquote>\n<p>It might be bad for the network as a whole, but it’s good for her as an individual user, she has no incentive to set a high threshold as far as I can tell</p>\n</blockquote>\n</aside>\n<p>The network cares nothing about such spam. It simply means the UTXO is spent (and increased) by the spammer rather than by an actual customer. The only one who should care is the merchant which cannot carry on deals</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-04-08T07:38:30.109Z",
        "updated_at": "2024-04-08T07:38:30.109Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/10",
        "category_id": 9
      },
      {
        "post_id": 280,
        "post_number": 12,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<aside class=\"quote no-group quote-modified\" data-username=\"FreshAir08\" data-post=\"11\" data-topic=\"168\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/letter_avatar_proxy/v4/letter/f/b19c9b/48.png\" class=\"avatar\"> FreshAir08:</div>\n<blockquote>\n<p>Would a more tight analysis apply only to the attacker’s own budget and not the budget of the additive addresses?</p>\n</blockquote>\n</aside>\n<p>Nice observation. Yes, a tighter analysis would show exactly that. In fact, it’s part of a more general phenomena where the budget needs to be equally spread across the growth in order to reach the bound tightly (where in the case of additive addresses they only increase in budget while keeping the same number of entries)</p>\n<p>The tighter analysis is also required for showing that sufficient budget is locked through time (with correlation to the <code>growth</code> is consumes). The relevant bound is a more complicated expression which is still wip.</p>",
        "raw_content": "",
        "author": "michaelsutton",
        "created_at": "2024-04-08T11:43:34.514Z",
        "updated_at": "2024-04-08T11:43:34.514Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/12",
        "category_id": 9
      },
      {
        "post_id": 282,
        "post_number": 14,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>Not a full answer but here is some back-of-the-envelope math:<br>\nSay we have an attacker that wants to break one input with <span class=\"math\">b</span> KAS into <span class=\"math\">k</span> inputs with <span class=\"math\">b/k</span> KAS. Then (using <span class=\"math\">C=1</span>) this should cost <span class=\"math\">(k-1)^2/b</span> mass.</p>\n<p>Your concern is if they use extra budget (from an additive UTXO or otherwise) of <span class=\"math\">a</span> KAS then they break <span class=\"math\">(a+b)</span> kas into <span class=\"math\">k</span> UTXOs with <span class=\"math\">b/k</span> KAS and a single UTXO with <span class=\"math\">a</span> KAS, and the bound only implies that this would cost <span class=\"math\">k^2/(a+b)</span> mass. So if <span class=\"math\">a\\gg b</span> the cost greatly decreases (and additive UTXOs mean they wouldn’t even have to obtain the <span class=\"math\">a</span> KAS themselves).</p>\n<p>However, we can easily prove that any attack that breaks <span class=\"math\">a+b</span> KAS into <span class=\"math\">k</span> UTXOs of size <span class=\"math\">b/k</span> and a single UTXO of size <span class=\"math\">a</span> actually costs at least <span class=\"math\">k^2/b</span> mass, regardless of <span class=\"math\">a</span>: Say that the attack costs <span class=\"math\">v</span>, then if I have <span class=\"math\">a+nb</span> KAS I can repeat the attack <span class=\"math\">n</span> times to create <span class=\"math\">nk</span> inputs of size <span class=\"math\">b/k</span> and a single input of size <span class=\"math\">a</span> for a cost of <span class=\"math\">nv</span>. However, the original bound tells us that such an attack must cost at least <span class=\"math\">(nk)^2/(a+nb)</span> mass, so it holds that <span class=\"math\">nv\\ge(nk)^2/(a+nb)</span>. Rearranging we get <span class=\"math\">n\\left(k^{2}-vb\\right)\\le va</span> and this inequality is obviously violated for a sufficiently large <span class=\"math\">n</span> unless <span class=\"math\">k^2-vb\\le 0</span>.</p>",
        "raw_content": "",
        "author": "Deshe2",
        "created_at": "2024-04-11T15:18:07.798Z",
        "updated_at": "2024-04-15T10:54:00.424Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/14",
        "category_id": 9
      },
      {
        "post_id": 284,
        "post_number": 15,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>you can pick out users that opt into a better peel chain?</p>",
        "raw_content": "",
        "author": "Alcyone",
        "created_at": "2024-04-12T11:28:01.843Z",
        "updated_at": "2024-04-12T11:28:01.843Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/15",
        "category_id": 9
      },
      {
        "post_id": 298,
        "post_number": 16,
        "topic_id": 168,
        "topic_title": "Auto-compounding/Additive addresses — KIP10 draft",
        "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
        "content": "<p>I’m sorry, I didn’t understand the question. What do you mean by a “peel chain”?</p>",
        "raw_content": "",
        "author": "Deshe2",
        "created_at": "2024-04-15T10:43:44.592Z",
        "updated_at": "2024-04-15T10:43:44.592Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/16",
        "category_id": 9
      },
      {
        "post_id": 156,
        "post_number": 1,
        "topic_id": 95,
        "topic_title": "Some of the intuition behind the design of the invalidation rules for pruning",
        "topic_slug": "some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning",
        "content": "<p>(A formal proof of the security of our pruning protocol can be found <a href=\"https://github.com/kaspanet/docs/blob/main/Reference/prunality/Prunality.pdf\">here</a>, the purpose of the current post is to provide an intuitive exposition)</p>\n<p>Finality is the practice of not allowing reorgs below some fixed depth, pruning is the practice of removing unnecessary block data. The two are strongly coupled: blocks which weren’t finalized may not be pruned. On the other hand, it is not generally true that finalized blocks may be pruned, as the data they contain might be needed to compute the UTXO set of incoming blocks.</p>\n<p>More concretely, a pruning block <span class=\"math\">P</span> is a block with the property that any blocks which affect the UTXO set of the virtual block are in its future. Differently put, if when <span class=\"math\">B</span> was discovered it was not in <span class=\"math\">P.Future</span>, then at no point in time will <span class=\"math\">B</span> be in <span class=\"math\">Virtual.Past</span>.</p>\n<p>In order to make things meaningful in the context of consensus, we apply the usual trick: we define the pruning block <span class=\"math\">P</span> of a block <span class=\"math\">B</span>, <span class=\"math\">B.P</span>, and then define “the” pruning block to be <span class=\"math\">P=Virtual.P</span>. The way we do this is straightforward: we set a pruning parameter <span class=\"math\">\\pi</span>, and then define <span class=\"math\">B.P</span> to be the “block at depth <span class=\"math\">\\pi</span> from <span class=\"math\">B</span> ”, that is, <span class=\"math\">B.P</span> is the most recent block in the selected chain of <span class=\"math\">B</span> such that <span class=\"math\">B.BlueScore - B.P.BlueScore &gt; \\pi</span>.</p>\n<p>We similarly define a finality parameter <span class=\"math\">\\phi &lt; \\pi</span>, and define <span class=\"math\">B.F</span> to be the block at depth <span class=\"math\">\\phi</span>, and in particular “the” finality block is <span class=\"math\">F=Virtual.F</span>. The finality rule becomes very straightforward: any block whose inclusion would cause a reorg after which <span class=\"math\">F</span> is not in the selected chain is considered a finality breaking block. The handling of such a scenario is to alert the community while halting approval of any and all transactions until the community manually resolves the conflict. From the point of view of designing a pruning algorithm, we may assume that this never happens.</p>\n<p>Our goal is to design a pruning algorithm which is resistant to pruning attacks. A pruning attack is a scenario where an adversary manages to force the network to require the data of an already pruned block. In particular, of a block outside <span class=\"math\">P.Future</span>. Since <span class=\"math\">P</span> is guaranteed to be in the selected chain by our definitions, and since <span class=\"math\">\\phi&lt;\\pi</span>, the only way to achieve this is to force the network to require the data of a block in <span class=\"math\">P.Anticone</span>.</p>\n<p>Note that no approach can deterministically prevent this scenario. For any block <span class=\"math\">B</span>, a <span class=\"math\">51%</span> attack can easily make it so that <span class=\"math\">B</span> would be in <span class=\"math\">Virtual.Past</span> at some point, and it follows that any attacker could achieve this feat with positive probability. Our best possible approach is to design the mechanism such that the probability of success decays exponentially fast as a function of the pruning parameter <span class=\"math\">\\pi</span>. This kind of “compromise” is not unique to our protocol, and is ubiquitous to any kind of security which relies on PoW.</p>\n<p>Our design approach employs the apparatus of invalidation rules. We set some criteria for when a block is considered invalid, and require that the virtual block is always valid, with the hopes that the rules imply that a pruning attack is impossible for a 49% attacker (or more accurately, that the probability of ever carrying such an attack diminishes exponentially with <span class=\"math\">\\pi</span> for any <span class=\"math\">\\alpha</span> attacker where <span class=\"math\">\\alpha &lt; 1/2</span> is fixed). Such rules must be in consensus, in the sense that one could read from the structure of the graph (and no other external data such as clocks and time signatures) whether a block is valid. This requirement is utterly crucial, since invalidating blocks with respect to inconsistent data will almost certainly lead to some splitting attack in which some blocks are considered invalid by some honest nodes, but are required to calculate the UTXO sets of some other honest nodes.</p>\n<p>The first invalidation rule is self evident: if a block is invalid then so is any block pointing to it. Since we discard such blocks, we can’t trust blocks which point at them as the data is unavailable.</p>\n<p>Note a crucial subtlety: we do not require that <span class=\"math\">Virtual.Past</span> will not include any blocks in <span class=\"math\">P.Anticone</span>. Such a requirement is far too strong and there are many legitimate scenarios in which it is infringed, even when the network is completely honest (in particular, if <span class=\"math\">P'\\in P.Anticone</span> and <span class=\"math\">P,P'\\in Virtual.Past</span> then <span class=\"math\">P'</span> will be in the past of any block pointing having the same parents as <span class=\"math\">Virtual</span>). Our requirement has to do with the time the node learned of <span class=\"math\">B</span>. By appealing to time we are actually breaking the consensus: network delay and network topology makes it so different nodes learn of blocks in different orders, making it possible that <span class=\"math\">B</span> violates this condition for one node but not for the other. This makes it impossible to simply require that “blocks which were in <span class=\"math\">P.Anticone</span> when they arrived are invalid”, since this is not a consensus rule.</p>\n<p>The solution is to design the system in such a way that even if a block is only accepted by some of the nodes, its data will never actually be required. Intuitively, such blocks diverge from the selected chain in a very deep point: either <span class=\"math\">P</span> itself or a block very close to <span class=\"math\">P</span> (in particular, a block which was created at most twice the roundtrip time of a block since <span class=\"math\">P</span> was discovered by an honest node). This could be used to enforce the policy that such blocks are not needed, but designing a set of rules which achieves this is quite delicate.</p>\n<p>The high-level idea of the pruning mechanism is to force any attacker who wishes to perform such an attack, in which the node requires data from a block which was in <span class=\"math\">P.Anticone</span> at the time it was discovered, into a block race. That is, making such an attack possible only if the attacker is able to create blocks at a higher rate than the honest network. We introduce how this is achieved from the bottom up.</p>\n<p>The first thing one might try is to invalidate any block <span class=\"math\">B</span> which directly points at a block in <span class=\"math\">B.P.Anticone</span>. This is a good start, but also obviously not sufficient, as it could be easily sidestepped by using an “intermediate” block <span class=\"math\">C</span> which points to a block in <span class=\"math\">P.Anticone</span> and is also a parent of <span class=\"math\">B</span>.</p>\n<p>A next reasonable step is to say “OK, so we invalidate <span class=\"math\">B</span> if it has a block <span class=\"math\">P’</span> in <span class=\"math\">P.Anticone</span> in its past, and there is no block in <span class=\"math\">B.Blueset</span> with <span class=\"math\">P’</span> in its past. Still, no dice. An attacker can overcome this by having the intermediate block <span class=\"math\">C</span> also point to a block close enough to <span class=\"math\">B</span> so that <span class=\"math\">C</span> be in B.Blueset, but <span class=\"math\">P’</span> not in <span class=\"math\">C.P.Anticone</span>. It might not be possible to create such a <span class=\"math\">C</span>, but in that case we could just “add more steps”, that is, create <span class=\"math\">C</span> in <span class=\"math\">B.Blueset</span> and <span class=\"math\">C’</span> in <span class=\"math\">C.Blueset</span> such that <span class=\"math\">P’</span> is not in <span class=\"math\">C’.P.Anticone</span>. If this is also not possible, we can add yet another step. The overall number of steps we will require is bound by a low constant (up to negligible probability), making a pruning attack feasible.</p>\n<p>What we need to do is to somehow verify that the block <span class=\"math\">D</span> in <span class=\"math\">B.Past</span> which is also in <span class=\"math\">B.P.Anticone</span> is not recent, and that it was in the graph long before <span class=\"math\">B</span>. We can’t rely on timestamps to do so. What we can do, is to rely on the protocol. In particular, if <span class=\"math\">D</span> has been known for a while, then there should be honest blocks which know of it. In particular, there should be a block <span class=\"math\">C</span> in <span class=\"math\">B.Blueset</span> such that <span class=\"math\">B.F</span> is in <span class=\"math\">C.SelectedChain</span> (ensuring <span class=\"math\">C</span> is much newer than <span class=\"math\">B.P</span>) and <span class=\"math\">D</span> is in <span class=\"math\">C.Past</span>. We call such a block <span class=\"math\">C</span> a “kosherizing” block. It is a block which is on one hand reliable, and on the other hand familiar with <span class=\"math\">D</span>.</p>\n<p>Note that there are two possibilities for <span class=\"math\">C</span>: if <span class=\"math\">D</span> is not in <span class=\"math\">C.P.Anticone</span>, then (assuming <span class=\"math\">C</span> was discovered before <span class=\"math\">B</span>, an assumption we revisit later) <span class=\"math\">D</span> could not have been in <span class=\"math\">P.Anticone</span> when it was discovered. Otherwise, <span class=\"math\">C</span> must also admit a kosherizing block, or it is invalid, and the argument continues inductively.</p>\n<p>This leads us to our second invalidation rule: a block <span class=\"math\">B</span> is invalid if there is a block <span class=\"math\">D</span> in <span class=\"math\">B.Past</span> which is in <span class=\"math\">B.P.Anticone</span>, for which there is no kosherizing block.</p>\n<p>Are these two rules sufficient to prevent a pruning attack? Not quite, but we are getting there. The problem is that an adversary could still carry out what we call a “climbing attack”. In this attack the adversary first creates a block <span class=\"math\">K</span> which points both to <span class=\"math\">D</span> and to the finality block <span class=\"math\">F</span>. This block will act as a kosherizing block. It then creates a succession of blocks, the first one, <span class=\"math\">K_1</span>, pointing at <span class=\"math\">K</span>, and at the highest block it can on the selected chain such that <span class=\"math\">K</span> remains in its blue set. The next block, <span class=\"math\">K_2</span>, will point to <span class=\"math\">K_1</span> and the highest chain block it can point to while keeping <span class=\"math\">K_2</span> in its selected chain, etc., until creating a block high enough that it remains blue while pointing to it and to the current tip. The amount of blocks needed to create this attack is constant, about <span class=\"math\">f/k</span>.</p>\n<p>The solution to this problem is to impose a “bounded merge” rule. Recall that the merge set of a block <span class=\"math\">B</span> is defined to be the set of blocks in <span class=\"math\">B.Past</span> which are neither <span class=\"math\">B.SelectedParent</span> nor in <span class=\"math\">B.SelectedParent.Past</span>. The third consensus rule limits the size of this set. That is, we fix some parameter <span class=\"math\">L</span> and impose the rule that a block may not have a merge set of size more than <span class=\"math\">L</span>. We will say something about how <span class=\"math\">L</span> should be chosen in a moment, but for now let us consider the implication of this rule.</p>\n<p>Remember that the entire goal of the attack is to make it so that a block which was in <span class=\"math\">P.Anticone</span> while it was discovered will never be in <span class=\"math\">Virtual.past</span>. The attacker has the freedom to choose a chain block <span class=\"math\">P’</span>, create a block in <span class=\"math\">P’.Anticone</span>, and withhold it until <span class=\"math\">P’</span> is the pruning block. All blocks created up to that point must be withheld.</p>\n<p>Let us first consider an attacker which is restricted to a premining attack. That is, the attack mines some blocks on the side, and then releases them all at once and does not create any more blocks. If the attack was successful, this implies that all the blocks created by the attacker are in the merge set of the new virtual block, and in particular, there are at most <span class=\"math\">L</span> of them. This implies at once that choosing <span class=\"math\">L&lt;f/k</span> makes such attacks impossible.</p>\n<p>The attacker of course is not bound to this restriction, and may create blocks after the premined blocks were created, in the hope of kosherizing one of the bad blocks further down the line. Now here is the crux: after being published, the honest network will not point at any of the bad blocks, as it will consider all of them red and in particular pointing at them means including in their past a block in <span class=\"math\">P.Anticone</span> with no kosherizing block. The attacker, on the other hand, can not extend the attack to beyond while pointing at chain blocks, since the chain block would be their selected parent, which will make their merge set larger than <span class=\"math\">L</span>. The upshot of all of this is that after publishing the attacker can only make up to <span class=\"math\">L</span> blocks, each of which climbing at most <span class=\"math\">k</span> blocks up the selected chain (as it has to consider the top bad block blue), after which they can not have an honest block be their selected parent. In other words, they are in a block race against the network. At this point the security proof of PHANTOM shows that a &lt;50% attacker will almost certainly lose this race.</p>\n<p>Of course, this argument seems only relevant to a particular attack vector, but it is provable that any successful attack must create such a succession (so, in particular, this is the optimal attack).</p>\n<p>So, in summary, the three invalidation rules are that a block <span class=\"math\">B</span> is considered invalid if:</p>\n<ol>\n<li>\n<span class=\"math\">B</span> has an invalid parent,</li>\n<li>\n<span class=\"math\">B.Mergeset</span> is larger than <span class=\"math\">L</span>, or</li>\n<li>There is a block <span class=\"math\">C</span> which is both in <span class=\"math\">B.Past</span> and <span class=\"math\">B.P.Anticone</span> such that it has no kosherizing block, where a kosherizing block is a block <span class=\"math\">D</span> in <span class=\"math\">B.Blueset</span> such that <span class=\"math\">B.F</span> is in <span class=\"math\">D.SelectedChain</span>, and such that <span class=\"math\">C</span> is in <span class=\"math\">D.Past</span>\n</li>\n</ol>\n<p>The security follows by a theorem (proven by Sutton and myself): under these invalidation rules, and under the assumption that the virtual block must be valid, if the following hold:</p>\n<ol>\n<li>There is never a reorg of depth more than <span class=\"math\">\\phi</span>, and</li>\n<li>There is an honest majority,</li>\n</ol>\n<p>then if <span class=\"math\">C</span> was in <span class=\"math\">P.Anticone</span> when it was discovered, then the probability that at some point in the future <span class=\"math\">C</span> would be in <span class=\"math\">Virtual.Past</span> is exponentially small in <span class=\"math\">p</span>.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-17T10:11:27.452Z",
        "updated_at": "2021-12-01T10:00:40.141Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning/95/1",
        "category_id": 1
      },
      {
        "post_id": 234,
        "post_number": 2,
        "topic_id": 95,
        "topic_title": "Some of the intuition behind the design of the invalidation rules for pruning",
        "topic_slug": "some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning",
        "content": "<p>The great research and what shall I do for my account activate?</p>",
        "raw_content": "",
        "author": "Kalid_Sherefudin",
        "created_at": "2023-10-18T19:41:33.259Z",
        "updated_at": "2023-10-18T19:41:33.259Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning/95/2",
        "category_id": 1
      },
      {
        "post_id": 185,
        "post_number": 1,
        "topic_id": 122,
        "topic_title": "Miner funds consolidation",
        "topic_slug": "miner-funds-consolidation",
        "content": "<p>Currently the miners of Kaspa create a single UTXO every block they mine.</p>\n<p>This causes UTXO-set bloat, and creates difficulty for miners to spend their mined coins.</p>\n<p>This is also a blocker for an increased block rate, 10x block rate will create 10x UTXOs.</p>",
        "raw_content": "",
        "author": "mikez",
        "created_at": "2022-01-03T17:09:01.639Z",
        "updated_at": "2022-01-03T17:09:01.639Z",
        "reply_count": 2,
        "url": "https://research.kas.pa/t/miner-funds-consolidation/122/1",
        "category_id": 7
      },
      {
        "post_id": 187,
        "post_number": 2,
        "topic_id": 122,
        "topic_title": "Miner funds consolidation",
        "topic_slug": "miner-funds-consolidation",
        "content": "<p>I suggest implementing a special kind of transaction, a compounding one, where source and destination addresses are the same, input UTXOs are strictly coinbase ones, and the  output value is slightly greater than the sum of the inputs; this excess is calculated as a certain small coefficient multiplied by a polynomial or exponential function of the number of used coinbase UTXOs and their values in order to encourage the user to combine as many outputs as possible in one transaction, instead of making many small unions that do not give a significant gain to a system.</p>\n<p>Such transactions are to be launched only from the wallet manually, thus eliminating the need for the mining software to access the user’s password, but it will be rational for the user to automate this process by periodically starting the wallet with the appropriate command, especially for users who will mine for sale and who therefore need the maximum profit at any given time. The command to the wallet can be supplied with parameters, so that it either terminates without executing if the number of unspent outputs is not enough to get the maximum possible benefit (the wallet itself must determine what the limit is, based on the maximum allowed transaction mass), or so that it is executed unconditionally, with the key like ‘–force’.</p>",
        "raw_content": "",
        "author": "Affiele",
        "created_at": "2022-01-04T20:39:29.313Z",
        "updated_at": "2022-01-04T20:39:29.313Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/miner-funds-consolidation/122/2",
        "category_id": 7
      },
      {
        "post_id": 190,
        "post_number": 5,
        "topic_id": 122,
        "topic_title": "Miner funds consolidation",
        "topic_slug": "miner-funds-consolidation",
        "content": "<p>| But this effects the total emission of coins<br>\nYes it does but in a predictable way and with a calculable upper limit.</p>",
        "raw_content": "",
        "author": "Affiele",
        "created_at": "2022-01-05T16:03:16.486Z",
        "updated_at": "2022-01-05T16:03:31.501Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/miner-funds-consolidation/122/5",
        "category_id": 7
      },
      {
        "post_id": 204,
        "post_number": 6,
        "topic_id": 122,
        "topic_title": "Miner funds consolidation",
        "topic_slug": "miner-funds-consolidation",
        "content": "<aside class=\"quote no-group\" data-username=\"mikez\" data-post=\"1\" data-topic=\"122\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/mikez/48/37_2.png\" class=\"avatar\"> mikez:</div>\n<blockquote>\n<p>and creates difficulty for miners to spend their mined coins.</p>\n</blockquote>\n</aside>\n<p>If true, this is already sufficient incentive for miners to consolidate their UTXOs.</p>\n<p>If indeed true, what’s preventing miners from consolidating their UTXOs today? Is it purely the lack of automatic mechanism to consolidate?</p>\n<p>If so, I propose a flag in the miner app, that when set, remembers the one consolidated or <code>n</code> consolidated UTXOs, and then adds to the mining template a transaction in the same block that consolidates it with the coinbase or even a special transaction that consolidates it into the coinbase.</p>\n<p>If always set to run this way, this mechanism would ensure there would be just one large UTXO the miner rolls on like a snowball (good name for the flag).</p>\n<p>A parameter <code>n</code> can be used to define how many snowball UTXOs to keep (instead of one), and it can round robin between them.</p>\n<p>A “migration” would need to happen before, where miners consolidate all their UTXOs into <code>n</code> snowball UTXOs.</p>\n<hr>\n<p>The con is that the miner would have to call the wallet to sign a transaction when it finds a block.</p>\n<p>To circumvent this, we could have new blocks invalidate the consolidated coinbase txs and  validate the consolidating coinbase tx whenever there is a new block that asks to be paid to the same address as an existing block.</p>",
        "raw_content": "",
        "author": "ey51",
        "created_at": "2022-02-16T00:43:02.745Z",
        "updated_at": "2022-02-16T01:27:31.707Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/miner-funds-consolidation/122/6",
        "category_id": 7
      },
      {
        "post_id": 227,
        "post_number": 7,
        "topic_id": 122,
        "topic_title": "Miner funds consolidation",
        "topic_slug": "miner-funds-consolidation",
        "content": "<p>thanks for the awesome information.</p>",
        "raw_content": "",
        "author": "alexsunny123",
        "created_at": "2022-12-16T06:57:57.264Z",
        "updated_at": "2022-12-16T06:57:58.756Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/miner-funds-consolidation/122/7",
        "category_id": 7
      },
      {
        "post_id": 179,
        "post_number": 1,
        "topic_id": 117,
        "topic_title": "Updated survey of the data growth in kaspad",
        "topic_slug": "updated-survey-of-the-data-growth-in-kaspad",
        "content": "<p>All and all we store three components: full header data above the pruning block, the UTXO set of the pruning block, and a proof of correctness for the UTXO set.</p>\n<p>We have a fancy <a href=\"https://research.kas.pa/t/some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning/95\">pruning mechanism</a> that allows us to remove old block data. At full capacity the size of a block payload is bound by 100kB and the size of a block header is bound by <span class=\"math\">100 +32\\cdot \\log_2(\\mbox{past size})</span> bytes. In the distant future where we have a trillion blocks in the network (this will take about 30 thousand years of one block per second) we will have that <span class=\"math\">\\log_2(\\mbox{past size})\\le 40</span>, so let us assume henceforth that <span class=\"math\">\\log_2(\\mbox{past size})\\le 40</span> forever (being mindful that the following analysis is “only” good for the coming 30 thousand years). This means that the header size is bound by <span class=\"math\">(100+32\\cdot40)</span> bytes which is just below 1.5kB. We store three days worth of full block data which, at a rate of one block/second, accumulates to about 26GB (note that this bound assumes that all blocks are at maximum capacity, no assumptions on average number of txns per block).</p>\n<p>The UTXO <a href=\"https://github.com/kaspanet/research/issues/3\">correctness proof</a> requires that we keep additional <span class=\"math\">\\log_2(\\mbox{number of blocks in the network})</span> headers (not full blocks). Using again the assumption <span class=\"math\">\\log_2(\\mbox{past size})\\le 40</span> this adds about 60kB of data, which is completely negligible. Currently we store all block headers, as it requires some care to remove them without accidentally removing headers required for the proof and our dev team hasn’t got around to this yet, this is a completely technical issue which will be resolved in the near future. (There is another detail I swept under the rug, which is that we also have to store the headers of all pruning blocks. This means one header per day. While this technically grows at a rate <span class=\"math\">O(n\\cdot\\log n)</span> the constant is ridiculously small: it is bound by 1.5kB/day, which are about 570kB a year).</p>\n<p>The only thing that grows linearly is the pruning block UTXO set itself. It currently requires a field of a fixed size for every unspent output in the network. It is hard to predict how fast this set grows as this heavily depends on user behavior. We intend to resolve this in the future by means of <a href=\"https://en.wikipedia.org/wiki/Accumulator_(cryptography)\">cryptographic accumulators</a>. An accumulator is a way to represent a large set succinctly such that it is impossible to recover the set itself (due to information theoretic compression bounds), but it is possible to verify that an element is in the set given a proof. This means that every user will only need to store the (proofs of) their own unspent outputs, and the nodes will only have to verify this proof against the accumulator, which is much smaller than the actual number of unspent outputs. The sizes of the accumulator and the proofs depends on the exact solution we will choose.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2021-12-01T08:33:45.860Z",
        "updated_at": "2021-12-01T08:40:17.020Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/updated-survey-of-the-data-growth-in-kaspad/117/1",
        "category_id": 1
      },
      {
        "post_id": 160,
        "post_number": 1,
        "topic_id": 98,
        "topic_title": "An atlas of the various constants and dependencies thereof",
        "topic_slug": "an-atlas-of-the-various-constants-and-dependencies-thereof",
        "content": "<p>The purpose of this post is to list and describe some of the various constants which parametrize the system and the relations thereof, provide sources to more in depth discussion when available and fill the gaps when not.</p>\n<p>Some of the constants described below appear in the code, whereas the others are defined for the sake of theoretic discussion. In the former case I will state how the variable under discussion is used in the codebase.</p>\n<p><strong>The maximal network delay <span class=\"math\">d_\\max</span></strong></p>\n<p>A bound on the time it takes for the entire network to learn of a block since it was broadcasted by its miner.</p>\n<p><strong>The tolerance parameter <span class=\"math\">\\delta</span></strong></p>\n<p>A lot of the structural properties of the DAG are only guaranteed with high probability. Adjusting some parameters allows us to control that probability to make it so desired properties hold “most of the time”, when we say “most of the time” we actually mean <span class=\"math\">1-\\delta</span> of the time.</p>\n<p>In the entire system the only such hard assumption we make is on the anticone size (cf. <span class=\"math\">k</span> below), and the rest is derived. Hence, <span class=\"math\">\\delta</span> designates a bound on the expected fraction of blocks whose anticone is larger than <span class=\"math\">k</span>.</p>\n<p>When discussing parameters which exist as variables in the codebase, I will state the name of the variable.</p>\n<p>All of the descriptions and properties above are assumed in the honest scenario unless stated otherwise. This does not mean we assume that the network is honest, only that some of the variables have meaningful interpretation in the honest settings. All security guarantees are proven in the honest majority setting.</p>\n<p><strong>The block rate <span class=\"math\">\\lambda</span></strong><br>\nName in codebase: for implementation reasons, it is more convenient to use the reciprocal <span class=\"math\">1/\\lambda</span>, which represents block <em>delay</em> rather than block <em>rate</em>. This appears in the code as <code>defaultTargetTimePerBlock</code>.</p>\n<p>Conceptual meaning: The expected number of blocks created in a period of time of length <span class=\"math\">D</span> is <span class=\"math\">D/\\lambda</span>.</p>\n<p>Operational meaning: the difficulty adjustment algorithm is parametrized to adjust the difficulty such that if <span class=\"math\">H</span> is the global hash rate then the expected number of hashes required to mine a block is <span class=\"math\">H\\lambda</span>.</p>\n<p><strong>The parent cap <span class=\"math\">C</span></strong><br>\nName in codebase: <code>defaultMaxBlockParents</code></p>\n<p>Operational meaning: a cap on the number of blocks a given block could point to.</p>\n<p>This parameter was introduced as a means to control the size of a block. An unlimited number of parents implies that an attacker could inflate the blocksize by posting many parallel blocks, thus causing the network delay of the block to exceed <span class=\"math\">d_\\max</span>, violating the assumptions on which the security of PHANTOM relies.</p>\n<p><strong>The effective merge delay <span class=\"math\">d_\\mbox{eff}</span></strong></p>\n<p>The effective merge delay is a period of time <span class=\"math\">d_\\mbox{eff}</span> such that if block <span class=\"math\">B'</span> was created at least <span class=\"math\">d_\\mbox{eff}</span> after the block <span class=\"math\">B</span> was reported to the network then <span class=\"math\">B\\in B'.Past</span> with probability at least <span class=\"math\">1-\\delta</span>.</p>\n<p>Dependence on other variables: If <span class=\"math\">C\\ge \\lambda d_\\max</span> then we have <span class=\"math\">d_\\mbox{eff} = d_\\max</span>. If <span class=\"math\">C = \\alpha \\lambda d_\\max</span> then, in the worst case in which there are <span class=\"math\">\\lambda d_\\max</span> tips it would take at least <span class=\"math\">1/\\alpha</span> blocks to merge them, making <span class=\"math\">d_\\mbox{eff} \\ge d_\\max + \\alpha/\\lambda</span>.</p>\n<p>It is impossible to upper bound <span class=\"math\">d_\\mbox{eff}</span> without further assumptions. Generally speaking, if there are more than <span class=\"math\">C</span> tips it could be that one of them would be indefinitely starved. This could be mitigated in a variety of ways, either by refining the definition of <span class=\"math\">d_\\mbox{eff}</span> to allow starvation of some blocks, by incentivizing miners to point at slightly older blocks by giving (some or all) of the block rewards of red blocks to the merging block, by forcing miners to randomly choose the pointed tips etc… This is beyond the scope of this post. We assume for now that <span class=\"math\">d_\\mbox{eff} \\le d_\\max + \\eta\\alpha/\\lambda</span>, leaving the specification of <span class=\"math\">\\eta</span> to whoever specifies the chosen approach.</p>\n<p><strong>The blue anticone size limit <span class=\"math\">k</span></strong></p>\n<p>Name in codebase: <code>defaultGHOSTDAGK</code></p>\n<p>Conceptual meaning: most of the time, the anticone size should be less than <span class=\"math\">k</span></p>\n<p>Operational meaning: Let <span class=\"math\">B</span> be a block, and let <span class=\"math\">C\\in B.BlueSet</span>, then <span class=\"math\">|C.AntiCone \\cap B.BlueSet| \\le k</span></p>\n<p>Dependence on other constants: Once <span class=\"math\">d_\\max</span>, <span class=\"math\">\\lambda</span> and <span class=\"math\">\\delta</span> above are decided upon, <span class=\"math\">k</span> is chosen such that, in expectation, a fraction of at least <span class=\"math\">1-\\delta</span> of the blocks admit anticones of size at most <span class=\"math\">k</span>. For <span class=\"math\">d_\\max = 50</span>, <span class=\"math\">\\lambda = 1/sec</span> and <span class=\"math\">\\delta = 0.05</span> this evaluates to <span class=\"math\">k=18</span>.</p>\n<p>The calculation is based on the formula which appears in the PHANTOM paper, subsection 4.2. The analysis therein assumes <span class=\"math\">d_\\mbox{eff} = d_\\max</span>. <strong>If the system is parametrized such that the equality does not hold, <span class=\"math\">d_\\mbox{eff}</span> should be used rather than <span class=\"math\">d_\\max</span>.</strong></p>\n<p><strong>The merge set size limit <span class=\"math\">L</span></strong><br>\nName in codebase: <code>defaultMergeSetSizeLimit</code></p>\n<p>Conceptual meaning: A limit of the number of blocks a new block can introduce to the state.</p>\n<p>Operational meaning: Any block <span class=\"math\">B</span> must satisfy that <span class=\"math\">|B.Past \\setminus B.SelectedParent.Past|\\le L + 1</span> (the <span class=\"math\">+1</span> is because the set in the LHS conains <span class=\"math\">B.SelectedParent</span>).</p>\n<p><strong>The finality depth <span class=\"math\">\\phi</span> and pruning depth <span class=\"math\">\\pi</span></strong></p>\n<p>Name in codebase: <span class=\"math\">\\phi</span> is called <code>defaultFinalityDuration</code>, <span class=\"math\">\\pi</span> is not defined but rather calculated in the function <code>PruningDepth()</code>.</p>\n<p>Conceptual meaning of <span class=\"math\">\\phi</span>: represents a period of time after which the selected chain may not be changed. That is, if a block in the selected chain is old enough, it is promised that it is in the selected chain.</p>\n<p>Conceptual meaning of <span class=\"math\">\\pi</span>: represents a period of time such that blocks which were created in the anticone of a selected chain block older than this period of time will never affect the state of the UTXO set.</p>\n<p>Operational meaning of <span class=\"math\">\\phi</span>: <span class=\"math\">\\phi</span> is used to define the pruning block <span class=\"math\">F</span> as the newest block in <span class=\"math\">Virtual.SelectedChain</span> whose blue score is not larger than <span class=\"math\">Virtual.BlueScore - \\phi</span>. In the event where including a new block would cause the selected chain to reorganize such that it does not include the current <span class=\"math\">F</span>, we alert the community that the network is split. The split is then manually resolved.</p>\n<p>Operational meaning of <span class=\"math\">\\pi</span>: <span class=\"math\">\\pi</span> is used to define the pruning block <span class=\"math\">P</span> as the newest block in <span class=\"math\">Virtual.SelectedChain</span> whose blue score is not larger than <span class=\"math\">Virtual.BlueScore - \\pi</span>. The validation rules, thoroughly discussed <a href=\"https://research.kas.pa/t/some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning/95\">here</a>, are designed such that if at the time <span class=\"math\">B</span> was discovered it was in the anticone of <span class=\"math\">P</span>, it would never be in the past of the virtual block (almost certainly, barring a more-than-half attack).</p>\n<p>Dependence on each other and variables: <span class=\"math\">\\phi</span> is chosen to represent an arbitrary real time length deemed reasonable by external considerations, hence it does not strongly depend on other variables. The only form of weak dependence is that it must be orders of magnitude larger than the expected number of blocks during the effective convergence time, i.e. <span class=\"math\">\\phi \\gg \\lambda d_\\mbox{eff}</span>.</p>\n<p>The discussion in the linked post describes validation rules, relative to which it was formally proven that the pruning mechanism is secure given that <span class=\"math\">\\pi = 2\\phi + 4Lk + 2k + 2</span>. The dependence of <span class=\"math\">\\pi</span> on <span class=\"math\">k</span> also implies dependence on <span class=\"math\">d_\\mbox{eff}</span> and  <span class=\"math\">\\lambda</span>, However, this dependence is accounted for in the calculation of <span class=\"math\">k</span>.</p>\n<p><strong>The difficulty window size <span class=\"math\">\\mathcal{N}</span>, the timestamp deviation tolerance <span class=\"math\">\\mathcal{F}</span></strong><br>\nName in codebase: <span class=\"math\">\\mathcal{N}</span> is called <code>defaultDifficultyAdjustmentWindowSize</code>, <span class=\"math\">\\mathcal{F}</span> is called <code>defaultTimestampDeviationTolerance</code>.</p>\n<p>Conceptual meaning: <span class=\"math\">\\mathcal{N}</span> describes how deep into the past of a block we should look into when calculating its difficulty target. <span class=\"math\">\\mathcal{F}</span> describes how tolerant we are to timestamps deviating from the expected.</p>\n<p>The operational meaning is thoroughly discussed <a href=\"https://research.kas.pa/t/handling-timestamp-manipulations/97\">here</a>. The only dependence on other variables lies in the fact that <span class=\"math\">\\mathcal{F}</span> relies on measuring a real time period (currently two minutes) by block count, which depends on <span class=\"math\">\\lambda</span>.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-29T10:55:43.572Z",
        "updated_at": "2021-11-30T12:40:54.769Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/an-atlas-of-the-various-constants-and-dependencies-thereof/98/1",
        "category_id": 1
      },
      {
        "post_id": 85,
        "post_number": 1,
        "topic_id": 58,
        "topic_title": "Manipulating difficulty to spam the UTXO set is ineffective",
        "topic_slug": "manipulating-difficulty-to-spam-the-utxo-set-is-ineffective",
        "content": "<p>The growth of the UTXO set poses a challenge to the design of any cryptocurrency protocol, as it bloats the state of the clients. An attacker might want to bloat this set by flooding the network with many cheap transactions in order to artificially increase the size of the state. In this post I am not addressing these issues directly, but merely trying to argue that a mining attacker could not abuse the difficulty adjustment protocol to increase the UTXO set.</p>\n<p>This is in response to the following attack (proposed by <a class=\"mention\" href=\"/u/ori\">@ori</a> and Elichai): create a side chain where you artificially decrease the difficulty, and then create a huge graph with many transactions for cheap.</p>\n<p>I argue that this attack does not actually allow a &lt; 50% attacker to spam the UTXO set, if we add a simple consensus rule (which was conceived in an entirely different context, which serves my position that it is a natural and reasonable consensus rule, and not a ad-hoc hacky solution).</p>\n<p>The consensus rule is: For some constant <strong>maxDiff</strong> (which should be about k), reject any blocks which has two immediate parents whose blue score difference is more than maxDiff. (my “philosophical” argument for this rule is that the PHANTOM DAG should be thought of as a generalization of Nakamoto chains. In this case, it makes sense to ask what replaces the maximal block in this generalization. The most easy answer is “the tips”, but I argue that “the maximal tip, and other tips with similar blue score” is better. The reason the top block in a Nakamoto chain is interesting is because it is supposed to contain the most current data. This could not be said about a week old block, even if it is technically a tip).</p>\n<p>Now, when calculating the UTXO set of the virtual block, it will simply not include tips with low scores, and their content will not affect the nodes UTXO set.</p>\n<p>Coming back to the attack above. In order for an attacker to decrease difficulty they must use time stamps which go into the future. While it is true that the attacker may create many blocks this way, they must also wait for the clock to catch up with the futuristic time stamps for them to be included, and by that time the network will have created more blocks. The difficulty adjustment is <strong>designed</strong> such that at any point in time (up to a small constant) the attacker can not create a lot of blocks <strong>which will be accepted</strong>. While it is true that the attacker can create many blocks this way, at any point in times, the amount of blocks they created <strong>which will be accepted by honest nodes</strong> is proportional to their hashrate. This (along with the consensus rule described above) implies that these blocks will not affect the nodes UTXO set.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-03-05T16:26:21.353Z",
        "updated_at": "2020-03-05T16:26:21.353Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/manipulating-difficulty-to-spam-the-utxo-set-is-ineffective/58/1",
        "category_id": 1
      },
      {
        "post_id": 159,
        "post_number": 2,
        "topic_id": 58,
        "topic_title": "Manipulating difficulty to spam the UTXO set is ineffective",
        "topic_slug": "manipulating-difficulty-to-spam-the-utxo-set-is-ineffective",
        "content": "<p>Should be noted that this approach has not proven itself in the long run. <em>tipDiff</em> could be seen as a very early precursor to the more mature idea of a “kosherizing block” which is presented in the <a href=\"https://research.kas.pa/t/some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning/95\">pruning algorithm</a>.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-21T16:18:13.011Z",
        "updated_at": "2020-12-21T16:18:13.011Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/manipulating-difficulty-to-spam-the-utxo-set-is-ineffective/58/2",
        "category_id": 1
      },
      {
        "post_id": 158,
        "post_number": 1,
        "topic_id": 97,
        "topic_title": "Handling timestamp manipulations",
        "topic_slug": "handling-timestamp-manipulations",
        "content": "<p>DAA relies on time stamps for retargeting, and this is a problem as timestamps are not reliable, both for adversarial reasons and due to natural clock drift across large networks.</p>\n<p>This poses an inherent tension: being too lax means giving more power to difficulty attacks, whereas being too strict would cause honest blocks to be dropped.</p>\n<p>There should be a mechanism for determining and enforcing a leeway for time signature deviation. I describe one here.</p>\n<p>The mechanism relies on the ability to decide, given two blocks, which one is “older”. This decision should not be based on time stamps as this leads to circular reasoning. From our point of view, we can assume that the design of the difficulty adjustment algorithm already contains a way to decide block chronology, and we only require that the DAA and the timestamp verification use the same way. (At the time this post was written, the selected metric was blue accumulated work, you can read about the reasoning behind this decision in <a href=\"https://research.kas.pa/t/switching-to-a-baw-based-daa/87\">this post</a>.)</p>\n<p>Say we decide our deviation tolerance should be <span class=\"math\">N\\lambda</span> where <span class=\"math\">\\lambda</span> is the expected block delay. The mechanism acts differently for checking if a block is too much in the past, and for checking if a block is too much into the future. There is also a difference in how we chose to handle both this case.</p>\n<p>To verify that a block is not too much into the past, we check that its time signature is at least as late as the median timestamp of the last <span class=\"math\">2\\mathcal{F}-1</span> blocks in its past. If it is, then the block is considered invalid and is rejected indefinitely. The reason we can be confidant about rejecting the block is that the information required to avoid this situation is completely known to the miner of the block.</p>\n<p>To verify that a block is not too much into the future, we have to rely on the system clock. If the timestamp on the block is later than <span class=\"math\">t + \\mathcal{F}\\lambda</span> where <span class=\"math\">t</span> is the local clock, the block is dropped. In this case, we do not consider the block rejected, and we trust that if it is an honest block it would be transmitted again (another approach would be to hold on to the block, but delay its inclusion into the DAG until the condition is satisfied).</p>\n<p>The reason this is crucial is because we rely on system clock, which is not in consensus. In particular, if we invalidated blocks which are too much into the future, an adversary could use well timed blocks to split the network. Also, it would cause nodes with too much of a negative clock drift to reject honest blocks.</p>\n<p>How <span class=\"math\">\\mathcal{F}</span> should be chosen depends on the particular DAA algorithm. The current algorithm allows a simple bound on the factor by which timestamp manipulation could affect the difficulty adjustment.</p>\n<p>We quantify this with regards to the particular DAA algorithm at the time this post was written, which is described <a href=\"https://research.kas.pa/t/switching-to-a-baw-based-daa/87\">here</a> and <a href=\"https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93\">here</a>.</p>\n<p>In this algorithm, we have a difficulty window size <span class=\"math\">N</span>, the correction factor to the difficulty is the ratio between the following two quantities:</p>\n<ul>\n<li>The expected time it should take to create <span class=\"math\">N</span> blocks, <span class=\"math\">N\\lambda</span>, and</li>\n<li>the approximate time it took to create the blocks in the window, which is the difference between the minimal and maximal timestamps in the window, call this quantity <span class=\"math\">R</span>.</li>\n</ul>\n<p>The adjustment factor is thus <span class=\"math\">\\alpha = R/N\\lambda</span>.</p>\n<p>The choice of <span class=\"math\">\\mathcal{F}</span> also relies on a bound <span class=\"math\">e</span> on the drift between clocks in the network (which could be a very good approximation to the assumption that clock delay distributes like <span class=\"math\">\\mathcal{N}(0,e^2)</span>).</p>\n<p>Assuming that in days of peace <span class=\"math\">R\\approx N\\lambda</span>, the worst an attacker can do is to push the least timestamp about <span class=\"math\">\\lambda F</span> into the past, and the latest timestamp exactly <span class=\"math\">\\lambda F</span> into the future, getting at <span class=\"math\">R\\approx N\\lambda + 2\\lambda\\mathcal{F}=\\lambda(N+2\\mathcal{F}</span>.</p>\n<p>In this case we get that <span class=\"math\">\\alpha \\approx N/(N+2\\mathcal{F})</span> or <span class=\"math\">\\frac{1}{\\alpha} \\approx 1 + 2\\frac{\\mathcal{F}}{N}</span>.</p>\n<p>This means we can’t make <span class=\"math\">\\mathcal{F}</span> irresponsibly large, e.g. if <span class=\"math\">\\mathcal{F} = N/2</span> then an attacker can potentially make the difficulty half as easier than necessary by placing just two blocks. (Of course, this attack will only be as effective for a single block, and will have a declining effect for the next <span class=\"math\">\\mathcal{F}</span> blocks.</p>\n<p>If we want to limit timestamp attacks to a create a multiplicative error of at most <span class=\"math\">(1+\\delta)</span> we need to have <span class=\"math\">2\\frac{F}{N}\\le \\delta</span> or <span class=\"math\">2\\mathcal{F}{\\delta}\\le N</span>.</p>\n<p>In can be crudely stated that large <span class=\"math\">\\frac{F}{N}</span> protects us from synchronization problems at the expense of allowing some difficulty manipulation, while choosing small <span class=\"math\">\\frac{F}{N}</span> protects us from such manipulations while making the algorithm less responsive. It might be possible to ameliorate this lack of responsiveness by means other than decreasing <span class=\"math\">N</span>, such as choosing different averaging mechanisms.</p>\n<p>Let <span class=\"math\">\\delta</span> be the maximal timestamp manipulation we allow. Assuming that the DAA works properly then (during times in which the global hash rate does not change too frantically) the block delay will be at least <span class=\"math\">\\lambda/(1+\\delta)</span>. If we want to prevent honest nodes from dropping honest blocks we should require that <span class=\"math\">\\mathcal{F} \\ge (1+\\delta)e/\\lambda</span>.</p>\n<p>In particular, if we allow <span class=\"math\">\\delta=10\\%</span>, and assume <span class=\"math\">e=120</span> seconds and <span class=\"math\">\\lambda = 1</span> seconds, we get that we need <span class=\"math\">\\mathcal{F} \\ge (1+\\delta)e/\\lambda=132</span> and <span class=\"math\">N\\ge 2\\mathcal{F}/\\delta = 2640</span>.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-21T15:00:15.085Z",
        "updated_at": "2020-12-21T15:43:31.909Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/handling-timestamp-manipulations/97/1",
        "category_id": 1
      },
      {
        "post_id": 146,
        "post_number": 2,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<p>I want to emphasize a big difference between the under-estimation in blue score vs. the over-estimation in past size.</p>\n<p>But before that some context: one of the reasons that we use a clock is to implement statements such as “the finality block should be a week old”. Currently we use blue score to measure it. That is, we define that the difference <strong>in blue score</strong> between the finality block and the virtual is the expected increase in blue score during a week of operation. When we discuss a different metric of time, we also implicitly discuss the consequential changes in the implementation of choice of the finality block (and consequently, the pruning block).</p>\n<p>The under-estimation in the former case is bound by the computational power of the adversary, and in particular could be manipulated at most by a factor of two (that is, an almost 50% adversary can convince us that time passes almost twice as slow as it really does).</p>\n<p>On the other hand, the over-estimation in the latter case is unbounded*. In particular, if we use past size to measure time it is very cheap to convince us that time passed 1000 times faster than it actually does.</p>\n<p>Another point is the ramifications of the attack: by increasing our clock speed, an attacker could force us to choose finality (and consequentially, pruning) blocks which were made very late. Technically, they could make so that the finality block is the selected parent of the selected tip! On the other hand, I fail to see any dire consequence of decreasing our clock speed, especially not when it is increased by a factor of at most two, and at a very considerable PoW expanse.</p>\n<p>*That’s not completely true due to the bounded merge rule. Recall that there is a parameter L which bounds the maximal merge set size of a valid block. This implies that the past size can not increase by more than L. The implications for the attack is that the attacker can’t actually make our clock arbitrarily faster, but they can still increase it by a factor of almost L (a more exact expression is [L - average merge set size in the rest of the network], but since L &gt;&gt; k this is pretty much the same). So maybe an attacker can’t force that the finality block is the selected parent of the selected tip, but for, say, L=1000, it can convince us that a block is a week old when it is actually ten minutes old.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-09T11:40:36.361Z",
        "updated_at": "2020-12-09T11:40:36.361Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/2",
        "category_id": 1
      },
      {
        "post_id": 149,
        "post_number": 4,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<p>I always kind of assumed this is the case. When we outlined the finality/pruning we always used real world time terminology e.g. “the finality window is a week long”.</p>\n<p>The way pruning/finality are implemented is under the implicit assumption that blue score is a viable approximation of time. If we decide that other metrics are more accurate and less attackable, then I think we should adjust pruning accordingly.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-10T11:13:24.713Z",
        "updated_at": "2020-12-10T11:13:24.713Z",
        "reply_count": 1,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/4",
        "category_id": 1
      },
      {
        "post_id": 150,
        "post_number": 5,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<p>On a second thought, I am not sure if the implementation complexity doesn’t outweigh the benefit. Are you bothered by the fact that a 33% attacker can make finality blocks 1.5 <strong>older</strong> than they should be? To an extent which justifies altering the finality/pruning block selection mechanism? I am not fluent enough in the engineering implications to make the call.</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-10T11:20:10.622Z",
        "updated_at": "2020-12-10T11:20:44.862Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/5",
        "category_id": 1
      },
      {
        "post_id": 153,
        "post_number": 8,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<aside class=\"quote no-group\" data-username=\"elichai2\" data-post=\"6\" data-topic=\"93\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/elichai2/48/59_2.png\" class=\"avatar\"> elichai2:</div>\n<blockquote>\n<p>AFAIR in finality/pruning blueScore is used for time approximation as a side effect, the main reason is for the pruning proof which heavily leans on <code>k</code> which is only true under blueScore.</p>\n</blockquote>\n</aside>\n<p>It is a bit more nuanced than that. The proof uses <code>k</code> in several different ways. Let w’(B) be the “accumulated difficulty window size” of B. That is, the size of the union of difficulty windows of B and the blocks in its selected chain (equivalent recursive defn: w’(B) = w’(B.SelectedParent) + |B.DifficultyWindow / B.SelectedParent.Past|, w’(Genesis) = 0). It is no longer true that w’(B) - w’(B.SelectedParent) &lt;= k, but this is not a real issue because we can still say that w’(B) - w’(B.SelectedParent) &lt;= diffWindowSize.</p>\n<p>The other assumption that we make, that there would not be a split below a certain depth, can still be stated in terms of k.</p>\n<p>The upshot, I think, is that the security proof should carry over but some of the k factors in the pruning block depth would have to be changed to diffWindowSize.</p>\n<p><a class=\"mention\" href=\"/u/msutton\">@msutton</a> what do you think?</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-10T11:54:08.410Z",
        "updated_at": "2020-12-10T11:54:08.410Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/8",
        "category_id": 1
      },
      {
        "post_id": 154,
        "post_number": 9,
        "topic_id": 93,
        "topic_title": "Difficulty adjustment and time measurement in DAGs",
        "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
        "content": "<aside class=\"quote no-group\" data-username=\"hashdag\" data-post=\"7\" data-topic=\"93\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://research.kas.pa/user_avatar/research.kas.pa/hashdag/48/45_2.png\" class=\"avatar\"> hashdag:</div>\n<blockquote>\n<p>Lower bound is sufficient. If I understand correctly, an attacker can manipulate these points by a factor of 2 at most (assuming an attacker &lt; 50%)</p>\n</blockquote>\n</aside>\n<p>I am fairly convinced that this is indeed the case. An alpha attacker can slow down the clock by a factor of at most (1 - alpha).</p>",
        "raw_content": "",
        "author": "Deshe",
        "created_at": "2020-12-10T11:58:44.749Z",
        "updated_at": "2020-12-10T11:58:44.749Z",
        "reply_count": 0,
        "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/9",
        "category_id": 1
      }
    ],
    "news_articles": [],
    "github_activities": [
      {
        "type": "github_commit",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "Remove temporary dust prevention mechanism (#698)",
        "author": "Ori Newman",
        "url": "https://github.com/kaspanet/rusty-kaspa/commit/fcd9c28f9b211b9872a0c90a8c94f79466ffb063",
        "date": "2025-06-30T10:18:30+00:00",
        "content": "Commit: Remove temporary dust prevention mechanism (#698)\n\n* Remove temporary dust prevention mechanism\n\n* Disable uninlined_format_args lint\n\n* Apply workspace lints to all crates\n\n* clippy\nFiles changed: 53\nAdditions: 155\nDeletions: 26",
        "metadata": {
          "sha": "fcd9c28f9b211b9872a0c90a8c94f79466ffb063",
          "stats": {
            "additions": 155,
            "deletions": 26,
            "total": 181
          },
          "files_changed": 53
        },
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "type": "github_pull_request",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "Remove temporary dust prevention mechanism",
        "author": "someone235",
        "url": "https://github.com/kaspanet/rusty-kaspa/pull/698",
        "date": "2025-06-26T07:38:52+00:00",
        "content": "PR #698: Remove temporary dust prevention mechanism\nThe [dust-prevention mechanism](https://github.com/kaspanet/kaspad/pull/2223) that was introduced in September 2023 to deal with a dust attack is now considered too restrictive, and preventing legitimate players from using multi outputs transactions with ease. Now that [KIP 9](https://github.com/kaspanet/kips/blob/master/kip-0009.md) is active on mainnet, we don't need any other measurements against such attacks.\nState: closed\nFiles changed: 53\nAdditions: 155\nDeletions: 26",
        "metadata": {
          "number": 698,
          "state": "closed",
          "draft": false,
          "merged_at": "2025-06-30T10:18:30+00:00",
          "stats": {
            "additions": 155,
            "deletions": 26,
            "changed_files": 53
          }
        },
        "signal": {
          "strength": "high",
          "contributor_role": "core_developer",
          "is_lead": false,
          "is_founder": false
        }
      },
      {
        "type": "github_pull_request",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "IBD Handle Syncer Pruning Movement",
        "author": "freshair18",
        "url": "https://github.com/kaspanet/rusty-kaspa/pull/702",
        "date": "2025-06-27T19:26:42+00:00",
        "content": "PR #702: IBD Handle Syncer Pruning Movement\nAddresses https://github.com/kaspanet/rusty-kaspa/issues/679\r\n\r\n**IBD Type determination**\r\nA new IBD type is added, currently called pruning_catchup. This IBD type is triggered when all the following are fulfilled:\r\n1) the syncer's and node's pruning points do not match,\r\n2) the node does know the header of the syncer's pp and can tell it's in the  future of its own pp\r\n3)the node does not have the block body of the syncer's pp  - if it does have that block body, it means vanialla syncing can carry on as normal and the node will prune on its own in due time.\r\n\r\nConveniently, negotiate_missing_syncer_chain_segment allows for an easy way to derive the syncer's current pruning point hash.\r\n\r\n**Validation Before Movement**\r\nBefore any sensitive and irreversible part, the node first downloads and validates headers from the Syncer until its declared sink. \"Destructive\" changes would only occur when :\r\n\r\n 1)the  syncer pp is a valid pruning sample (it satisfies the blue_score requirements to be a pp)\r\n 2)there are sufficiently many headers built on top of it, specifically, the syncer's sink validated header blue_score is greater than P.b_score+pruning_depth.\r\n 3) the syncer pruning point is on the selected chain from the syncer's sink, and any pruning points declared\r\n on headers on its path must be consistent with those already known\r\n\r\n**Transitional States During Catchup**\r\nUpdating to a new pruning point, conceptually consists of three stages:\r\n1) Updating various stores ,  most prominently pruning_point store,  but also virtual store, past_pruning_points, pruning samples, selected chain store, and body_tips store. All those could be updated in a batch. (ignoring pruning samples for which it does not matter). I will refer to this stage as the \"pruning point movement\".\r\n2) Downloading the new pruning utxo set from a peer, and verifying it matches the header\r\n3) Downloading the block bodies of the new pruning point and its anticone - these blocks should only undergo trusted validation as their parents will forever miss block bodies. Hence they require special attention.\r\n\r\nDuring IBD_with_headers_proof (as it previously was), these three stages are performed atomically, using a \"discardable\" staging consensus, which either goes through all of them and only then is commited, or the current consensus remains active.\r\n\r\nUnlike an IBD with headers proof, pruning_catchup inherently consists of building on the information of the current consensus rather than starting from scratch.\r\n\r\nThe current implementation hence introduces transitional states, with corresponding \"flags\" for the intermediary cases where the pruning point movement occured but a new pruning utxo set is yet to be downloaded, and or the anticone's block bodies have not all went through verification. The required anticone in particular is maintained by computing and storing it already during the pp movement, with it being computed in relation to the syncer's sink (In theory this maintained set  could be shrunk on the fly as more bodies are synced, but at the moment this set is maintained in an all or nothing manner - since sending validated blocks to validation causes no harm and is fast enough).\r\n\r\nGiven the easy recognition, these intermediary states could just be handled in future syncs. These transitional states are unabusable given the standard security assumption of an honest majority at every pruning period: as we synced sufficiently many headers on top of the pruning point, we know the syncee's Dag on top of it represents the honest network, and hence its PP represents a valid pruning utxo set, and the blocks on the anticone must have had a block_body - or the honest network would have \"rejected\" this Dag (more precisely, the pp would not have been on the selected chain of it). It is remarked the same assumption was used previously when choosing to commit a staging consensus before all blocks synced underwent validation. \r\n\r\n* decoupling utxo download from pruning_movement also allows for sync_with_headers_proof to commit prior to downloading the utxo set, greatly improving the UI experience of many users who disconnect during the long UTXO download and have to start fresh syncing from anew. \r\n\r\n**Transitional States Security**\r\nPruning: generally pruning is not activated unless a virtual task is completed, and hence would not be called while in the limbo state of a missing utxo set. To be on the safe side it is confirmed we are not in a transitional state before attempting to naturally advance the pruning_utxo_set. Could perhaps be turned to an assert\r\nBlock Relay: a check is added if the consensus is in a transitional state to immediately send it to IBD if it is.\r\nHandleRelayBlockRequests: The node will ignore requests to send over its sink if it is in a transitional state, to avoid log cluttering and disconnecting due to  a potential missing block error\r\n\r\nFor simplicity both transitional states are checked in all the above, though at times a distinction could be made between them.\nState: open\nFiles changed: 22\nAdditions: 618\nDeletions: 145",
        "metadata": {
          "number": 702,
          "state": "open",
          "draft": true,
          "merged_at": null,
          "stats": {
            "additions": 618,
            "deletions": 145,
            "changed_files": 22
          }
        }
      },
      {
        "type": "github_pull_request",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "Python bindings initial feature set",
        "author": "smartgoo",
        "url": "https://github.com/kaspanet/rusty-kaspa/pull/602",
        "date": "2024-11-22T20:23:09+00:00",
        "content": "PR #602: Python bindings initial feature set\nPython Bindings for Rusty Kaspa - using [PyO3](https://pyo3.rs/v0.22.5/) and [Maturin](https://www.maturin.rs) to build a native Python extension module named `kaspa`. \r\n\r\nThis initial version exposes wRPC Client, transaction creation, and key management functionality to Python.\r\n\r\nWASM interface is mirrored as much as possible.\r\n\r\n---\r\n\r\nAdded workspace member crates:\r\n- `kaspa-python` - main entry point for building Python package, and example usage from Python\r\n- `kaspa-python-core` - custom types used for Python interface\r\n- `kaspa-python-macros`\r\n- `kaspa-wrpc-python` - Python wRPC client\r\n\r\nAdded external dependencies: `pyo3`, `pyo3-async-runtimes`, `pyo3-log`, `serde-pyobject`\r\n\r\nA (very) high-level overview of bindings:\r\n- All Python bindings code is gated behind feature flag `py-sdk`. This feature was added to multiple existing RK crates.\r\n- Structs (and Enums) exposed to Python have attribute `#[pyclass]`.\r\n- Methods exposed to Python exist inside impl blocks that have attribute `#[pymethods]`.\r\n- Functions are exposed to Python using attribute `#[pyfunction]`.\r\n- Where possible, exposed existing RK native/WASM code (structs, enums, functions, etc.). Where not possible (due to Python interface limitations/requirements), separate Python-compatible implementations were added.\r\n- In situations where both `python` and `wasm` directories exist at same level, created `bindings` directory and moved both under that directory.\r\n\r\nCI:\r\n- On release - builds Python wheels for matrix of OS, architecture, and Python versions. End result is single zip that contains all wheels that is part of release.\r\n- On push/pull - builds Python wheels for only Linux x86_64.\nState: open\nFiles changed: 156\nAdditions: 6532\nDeletions: 72",
        "metadata": {
          "number": 602,
          "state": "open",
          "draft": false,
          "merged_at": null,
          "stats": {
            "additions": 6532,
            "deletions": 72,
            "changed_files": 156
          }
        }
      },
      {
        "type": "github_pull_request",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "Deep wiki badge",
        "author": "freshair18",
        "url": "https://github.com/kaspanet/rusty-kaspa/pull/699",
        "date": "2025-06-26T13:41:09+00:00",
        "content": "PR #699: Deep wiki badge\n\"Placing the badge (anywhere in the README) does two things:\r\n\r\n**Auto-refresh** – every time you push to main, DeepWiki’s webhook re-indexes changed files.\r\n\r\n**Ranking boost** – repos that carry the badge get a small weighting bump in DeepWiki’s retrieval layer, so code snippets from your repo are more likely to be surfaced.\"\nState: open\nFiles changed: 1\nAdditions: 1\nDeletions: 1",
        "metadata": {
          "number": 699,
          "state": "open",
          "draft": false,
          "merged_at": null,
          "stats": {
            "additions": 1,
            "deletions": 1,
            "changed_files": 1
          }
        }
      },
      {
        "type": "github_issue",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "Cannot start a node with --utxoindex",
        "author": "9igeeky",
        "url": "https://github.com/kaspanet/rusty-kaspa/issues/696",
        "date": "2025-06-25T14:33:52+00:00",
        "content": "Issue #696: Cannot start a node with --utxoindex\n[ERROR] thread 'main' panicked at kaspad/src/daemon.rs:226:32: attempt to multiply with overflow\n\nthread 'main' panicked at kaspad/src/daemon.rs:226:32:\nattempt to multiply with overflow\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nExiting...\n\nkaspad/src/daemon.rs:\npub fn create_core_with_runtime(runtime: &Runtime, args: &Args, fd_total_budget: i32) -> (Arc<Core>, Arc<RpcCoreService>) {\n    let network = args.network();\n    let mut fd_remaining = fd_total_budget;\n    let utxo_files_limit = if args.utxoindex {\n        let utxo_files_limit = fd_remaining * 10 / 100;\n        fd_remaining -= utxo_files_limit;\n        utxo_files_limit\n    } else {\n        0\n    };\n\nLine 226: let utxo_files_limit = fd_remaining * 10 / 100;\n\n\nState: open\nComments: 7",
        "metadata": {
          "number": 696,
          "state": "open",
          "comments": 7,
          "labels": [],
          "assignees": [
            "freshair18"
          ]
        }
      },
      {
        "type": "github_issue",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "SDK - Generator - Mass Exceed on a single utxo set evaluation round should not exit if there are other utxos that could fit",
        "author": "IzioDev",
        "url": "https://github.com/kaspanet/rusty-kaspa/issues/701",
        "date": "2025-06-26T18:07:35+00:00",
        "content": "Issue #701: SDK - Generator - Mass Exceed on a single utxo set evaluation round should not exit if there are other utxos that could fit\n**Describe the bug**\nUnder certain conditions (more below), the Generator could have picked a valid (according to KIP-9 rules) UTXO to spend according to the request conditions, but has failed while supposedly only attempting one of them.\n\nMethod: `generator.next()`\n\nInput parameters:\n* payload\n  ```\n  636970685f6d73673a01b09eadce2f05f00289a1de1603383e2b8283bc8e74c37bed75a0ce0db369628860d5d2e6b1243f9685809f5889d0f8cab14313432b19059baa0621e6a5d8e9888415b5884e7ba44e483ebd72e089d617aab093bdeb590e38ec1b30d6bf0083f4780e0b351caab91af44d11fd85b0291fcff75b5d87445ba1709858cc89ea121b86304249e63b8311283c6fa8be683e1ee1c786629e79fd4d469634072006d3f8cdfc74b4d6732b41a76919071e4ebb488900297d757abe73fb8d6d8b012fa2a038414c6cea7a14cd2e07ad3bd9083e0b1976b0c59cb3b64566250455ab4d37562a839e1f6ee0e665b535255a233a0ade5d9205e52cc5ab6cb4fe74cb1d736c33619e1401c8e1cd75e519e138db4b7b101d509d1fbf7820a4a3223129038e1f3bc2064dae148f36dac0b4d53ab82fb1c76d2b9add63574662169cba7b30797b896f7aa1c79d9ed5c1be48112e1c2ac4ab0fe84b6194aa3c94ff4cdf6798f3c217c8\n  ```\n* address: `kaspatest:qqfxqnutdqycengg2w3ch36djg20y9ws5kyvjhzflgxw5ledzwt9ulkhlny0x` (as an `Address`)\n* priorityFee: `0n`\n* outputs: `[new PaymentOutput(destinationAddress, 20000000n)]` (0.2 TKAS)\n\nUTXO Context: (sorry about the format)\n```json\n{\"utxos\":[{\"entry\":{\"address\":{\"version\":\"PubKey\",\"prefix\":\"kaspatest\",\"payload\":\"qzadn9tyvela5tc9rpn5tnrlkmdq9988wu8nh57pa4d62nw92pspsd740qlkv\"},\"outpoint\":{\"transactionId\":\"3017aa20bfe904407a214d81997468b8defef0617cef9007c7fa12b322a22c05\",\"index\":0},\"amount\":\"29624618\",\"scriptPublicKey\":{\"version\":0,\"script\":\"20bad99564667fda2f05186745cc7fb6da0294e7770f3bd3c1ed5ba54dc5506018ac\"},\"blockDaaScore\":\"183065193\",\"isCoinbase\":false},\"outpoint\":{\"transactionId\":\"3017aa20bfe904407a214d81997468b8defef0617cef9007c7fa12b322a22c05\",\"index\":0},\"address\":{\"version\":\"PubKey\",\"prefix\":\"kaspatest\",\"payload\":\"qzadn9tyvela5tc9rpn5tnrlkmdq9988wu8nh57pa4d62nw92pspsd740qlkv\"},\"amount\":\"29624618\",\"isCoinbase\":false,\"blockDaaScore\":\"183065193\",\"scriptPublicKey\":{\"version\":0,\"script\":\"20bad99564667fda2f05186745cc7fb6da0294e7770f3bd3c1ed5ba54dc5506018ac\"}},{\"entry\":{\"address\":{\"version\":\"PubKey\",\"prefix\":\"kaspatest\",\"payload\":\"qzadn9tyvela5tc9rpn5tnrlkmdq9988wu8nh57pa4d62nw92pspsd740qlkv\"},\"outpoint\":{\"transactionId\":\"7a973447dd13e0c1375f98e669794f87c7421f9976d54a09e1bcf8443bbe961a\",\"index\":0},\"amount\":\"50000000\",\"scriptPublicKey\":{\"version\":0,\"script\":\"20bad99564667fda2f05186745cc7fb6da0294e7770f3bd3c1ed5ba54dc5506018ac\"},\"blockDaaScore\":\"185277525\",\"isCoinbase\":false},\"outpoint\":{\"transactionId\":\"7a973447dd13e0c1375f98e669794f87c7421f9976d54a09e1bcf8443bbe961a\",\"index\":0},\"address\":{\"version\":\"PubKey\",\"prefix\":\"kaspatest\",\"payload\":\"qzadn9tyvela5tc9rpn5tnrlkmdq9988wu8nh57pa4d62nw92pspsd740qlkv\"},\"amount\":\"50000000\",\"isCoinbase\":false,\"blockDaaScore\":\"185277525\",\"scriptPublicKey\":{\"version\":0,\"script\":\"20bad99564667fda2f05186745cc7fb6da0294e7770f3bd3c1ed5ba54dc5506018ac\"}}]}\n```\n\nTo make it simpler, here are the UTXO:\n![Image](https://github.com/user-attachments/assets/d9b43d8c-14c5-41f1-adce-5b1337e299a7)\n\nconsole error: `Storage mass exceeds maximum`\n\n**Expected behavior**\nGenerator should potentially fail on the utxo that has `0.29624618 TKAS` but should eventually try another combination for the request, in this case the utxo that has `0.5 TKAS`.\n\n**SDK Information**\n - wasm web kaspa v1.0.0, taken from built wasm package on the releases page.\n\nState: open\nComments: 3",
        "metadata": {
          "number": 701,
          "state": "open",
          "comments": 3,
          "labels": [],
          "assignees": []
        }
      },
      {
        "type": "github_issue",
        "repository": "kaspanet/rusty-kaspa",
        "repository_url": "https://github.com/kaspanet/rusty-kaspa",
        "title": "Add tx.payload support for PSKT constructions",
        "author": "danwt",
        "url": "https://github.com/kaspanet/rusty-kaspa/issues/700",
        "date": "2025-06-26T13:45:39+00:00",
        "content": "Issue #700: Add tx.payload support for PSKT constructions\nIt's misssing\nState: open\nComments: 0",
        "metadata": {
          "number": 700,
          "state": "open",
          "comments": 0,
          "labels": [],
          "assignees": []
        }
      }
    ],
    "onchain_data": {},
    "documentation": []
  },
  "metadata": {
    "total_items": 146,
    "processing_time": "0.00s",
    "pipeline_version": "2.0.0",
    "sources_processed": [
      "medium: 10 items",
      "forum: 128 items",
      "github_activities: 8 items",
      "signal_analysis: 77 high-signal items (3 from lead developer, 25 from founder)"
    ],
    "signal_analysis": {
      "total_items": 146,
      "high_signal_items": 77,
      "lead_developer_items": 3,
      "founder_items": 25,
      "contributor_roles": {
        "founder_researcher": 25,
        "core_developer": 52
      },
      "signal_distribution": {
        "high": 77,
        "standard": 69
      },
      "sources_with_signals": {
        "medium_articles": {
          "total": 10,
          "high_signal": 10,
          "lead_developer": 0,
          "founder": 10,
          "roles": {
            "founder_researcher": 10
          }
        },
        "forum_posts": {
          "total": 128,
          "high_signal": 65,
          "lead_developer": 3,
          "founder": 15,
          "roles": {
            "founder_researcher": 15,
            "core_developer": 50
          }
        },
        "github_activities": {
          "total": 8,
          "high_signal": 2,
          "lead_developer": 0,
          "founder": 0,
          "roles": {
            "core_developer": 2
          }
        }
      }
    }
  }
}