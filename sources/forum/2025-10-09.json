{
  "date": "2025-10-09",
  "generated_at": "2025-10-09T03:17:34.826145+00:00",
  "source": "discourse_forum",
  "status": "success",
  "forum_posts": [
    {
      "post_id": 696,
      "post_number": 6,
      "topic_id": 429,
      "topic_title": "Random Linear Network Coding For Scalable BlockDAG",
      "topic_slug": "random-linear-network-coding-for-scalable-blockdag",
      "content": "On redundancy for parents versus transactions: You are correct that parent hashes in Kaspa include indirect parents, often several hundred per block with higher concurrency under Crescendo. That is a major source of redundancy. To keep this focused I will treat headers and parents as propagated as usual for now and explain RLNC only for block data, meaning transactions. The same ideas can be extended to header factorization later. What RLNC is in plain English for transactions: RLNC turns a large, overlapping set of missing transaction bytes into a stream where any useful packet helps until you finish. Instead of asking peers for specific transactions which risks duplicates, stalls, and repeated request rounds, peers send small coded shreds that are linear combinations of the chunks you are missing. Your node keeps only shreds that add new information and drops the rest as soon as they arrive. Once you collect enough innovative shreds, you recover all original chunks at once and reassemble the transactions. Why this helps Kaspa even when links are not very lossy: It is primarily about coordination and duplication across many peers and many parallel blocks, not only about packet loss. Cross block duplication becomes a single download. If the same transaction appears in multiple parallel blocks, RLNC treats it as one source item inside a small sliding window. You fetch it once and it satisfies all those blocks. Any useful packet means no per tx choreography. With many peers, classic pulls waste bandwidth on duplicates and bookkeeping about who has which transaction. Under RLNC, whichever peer sends a coded shred, it is very likely useful until you finish. Tail removal and endgame elimination. The last one to five percent of transactions often dominate wall clock time. RLNC adds a small planned overcode, typically five to ten percent, so you finish without hunting for specific transactions. Partial peers still help. Even if a peer has only some of the window, its shreds are still useful. RLNC harvests partial availability automatically. What exactly are the packets, for transactions only: Build a sliding window of recent levels for IBD or catch up, for example four to eight levels. Compute the node\u2019s unique set of missing transactions across that window. Duplicates across blocks collapse to one. Split each transaction into fixed size chunks, for example 512 to 1024 bytes. Across the window you get K chunks that are the sources. A coded shred equals a tiny seed, for example four bytes, plus a payload that is the bytewise linear combination of those K chunks over GF(256). The seed deterministically expands through a public PRF to the coefficient row, and the receiver recomputes it for verification. What the receiver does, and why memory remains bounded: For each shred, regenerate coefficients from the seed, then run one elimination step. If it increases rank, keep it. If it is dependent, drop it immediately. You do not buffer junk. When rank reaches K, solve once, recover all chunks, reassemble transactions, and validate. Memory is bounded at roughly K multiplied by chunk size for the innovative rows plus a small coefficient structure per active window, with strict caps and timeouts. Why RLNC is not only for lossy networks and fits inherent redundancy in Kaspa: The main win is removing the coordination cost that comes from overlap. In a multi peer pull, different peers often resend the same last few transactions. RLNC turns those overlaps into independent information, so they still push you toward completion. The tail disappears because you do not need those specific last transactions. You need enough degrees of freedom. A small overcode in the range of five to ten percent provides that cushion with no per tx negative acknowledgments. Scheme in practice: Window selection on the receiver: choose recent levels, list the transactions you are missing, deduplicate, chunk, and count K. Coefficient rule that is safe against DoS: each shred carries a seed. Coefficients derive from a public PRF of window id, stream id, and shard index. The receiver can dictate the stream id so peers cannot choose pathological schedules. Schedules can be prechecked for rank growth if desired. Sending by peers: stream ratelessly about K times one plus epsilon coded shreds, with epsilon around five to ten percent. Multiple peers can send, no need to coordinate who has which transaction. A short systematic phase is optional on very clean links. Receiving by the node: innovative or drop, stop at rank K, decode once, reassemble transactions, validate. Cache recovered transactions so later blocks do not re download them. Bounds and fallback: keep a small fixed number of windows in flight, apply hard caps on rows and time. If progress stalls, request a small top up or fall back to per tx or per chunk for the last few, which is the same safety net used today. Peer quality control: track an innovation ratio per peer, namely innovative over received. Peers that send mostly non innovative or invalid shreds are deprioritized or banned. What this solves for Kaspa: Cross block transaction duplication becomes a single download inside the window. Multi peer overlap stops costing you since any helpful packet advances you. The long tail and endgame vanish since a small overcode replaces rounds of requests for specific transactions. IBD and catch up time becomes bandwidth proportional rather than RTT and coordination dominated. Memory stays bounded using innovative only buffering with sliding window caps. Partial peers still help without any per tx scheduling. Non goals and expectations: RLNC does not compress transactions and does not change consensus or validation. It is a transport and scheduling upgrade. With a single perfect peer, zero loss, and an ideal scheduler, an oracle style unique pull can match or beat RLNC in bytes. RLNC earns its keep in Kaspa\u2019s real conditions with parallel blocks, many peers, overlapping availability, and some loss. TL;DR RLNC helps by streaming rateless coded shreds so progress isn\u2019t gated by per-tx NACK/REQ exchanges; completion becomes closer to bandwidth-proportional rather than RTT-bound. For headers: If we want to tackle header redundancy, we can factor the header into a core part and a set of parent hash items, both direct and indirect, then code those items as well. That would exploit the several hundred hashes you highlighted. Starting with transactions keeps integration simple and already yields the tail removal and de duplication gains.",
      "raw_content": "",
      "author": "Gordon_Murray",
      "created_at": "2025-10-09T01:38:57.393Z",
      "updated_at": "2025-10-09T01:38:57.393Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/random-linear-network-coding-for-scalable-blockdag/429/6",
      "category_id": 1
    }
  ],
  "metadata": {
    "forums_processed": 1,
    "total_posts_fetched": 1,
    "credential_status": "configured",
    "processing_mode": "topic_centric"
  }
}