{
  "date": "full_history",
  "generated_at": "2025-07-10T13:49:39.185795+00:00",
  "source": "discourse_forum",
  "status": "success",
  "forum_posts": [
    {
      "post_id": 344,
      "post_number": 3,
      "topic_id": 62,
      "topic_title": "Including Transactions Of Red Blocks",
      "topic_slug": "including-transactions-of-red-blocks",
      "content": "Sorry for resurrecting this thread. I have a few questions along these lines and didn\u2019t want to create a new thread when there was already one with minimal discussion. I just want to say first that argument about having protection against temporary 51% attacks is a compelling one for including txns within red blocks. It\u2019s not mentioned above, but to include txns within red blocks necessarily means including red blocks in a block\u2019s mergeset (mentioning it here in case it\u2019s unclear to anyone reading). I\u2019m wondering about a few things: Are red blocks ordered the same way as blue blocks would be (except they go after all the blue blocks in the same mergeset)? My intuition tells me it must be since they\u2019d still have blue work info even if they\u2019re red so they can be ordered that way. And since ordering is consensus sensitive then so ordering them must also be done in a consistent, stable manner. How deep of a red block (and txs within) are we allowing for inclusion here? I\u2019m thinking about a few boundaries like the DAA window, the merge depth bound under which we (maybe?) no longer accept red blocks.",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2024-12-10T16:49:11.887Z",
      "updated_at": "2024-12-10T16:49:11.887Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/3",
      "category_id": null
    },
    {
      "post_id": 349,
      "post_number": 4,
      "topic_id": 62,
      "topic_title": "Including Transactions Of Red Blocks",
      "topic_slug": "including-transactions-of-red-blocks",
      "content": "IIRC red blocks receive no special treatment within the merge set\u2013the rules of inclusion and the ordering do not explicitly take into account the colour of a block (re inclusion, it so happens that merge_depth violating blocks will be red, since merge_depth>>k). From the \u201clocal\u201d merge-set pov, the only different treatment they receive is that their rewards are sent to the merging block, in order to impose a clear cost on double-spending attempts (similarly to Bitcoin orphans not receiving rewards; also related: the uncle mining attack on Ethereum 1.0). The real impact of blue-red discrimination is the contribution to a block\u2019s score (aka blue score), which impacts the chain selection rule, and in turn the entire ordering. Since only blue blocks contribute to a block\u2019s ability to compete for chain membership, red blocks do not contribute to the security of the DAG\u2019s ordering. BTW, I\u2019ll try to explain in a later comment the answer to another question (which i believe you once raised): if red blocks do not contribute to security, why are they still counted in the DAA window",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-11T05:44:48.783Z",
      "updated_at": "2024-12-11T05:44:48.783Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/4",
      "category_id": null
    },
    {
      "post_id": 339,
      "post_number": 1,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "This post aims to provide a possible picture of how based zk (zero-knowledge) rollups can be designed to operate over Kaspa\u2019s UTXO-based L1 (see @hashdag\u2019s post for broader context). This is by no means a final design, but rather the accumulation of several months of discussions with @hashdag, @reshmem and Ilia@starkware, which is presented here for aligning the core R&D discussion in the Kaspa community around common ground before finalizing (the L1 part of) the design. I\u2019m making a deliberate effort in this post to use the already established jargon and language of the broader SC community. To this end I suggest that unfamiliar readers review documents such as eth-research-based, eth-docs-zk and nested links therein before proceeding with this post. Brief description of based zk rollups A zk rollup is an L2 scaling solution where L2 operators use succinct zk validity proofs to prove correct smart contract execution without requiring L1 validators to run the full computations themselves. A based zk rollup is a type of rollup design where the L2 is committed to operations submitted to L1 as data blobs (payloads in Kaspa\u2019s jargon), and cannot censor or manipulate the order dictated by L1. Based zk rollups as an L1 \u2194 L2 protocol Since based zk rollups represent an interaction between the base layer (L1) and the rollup layer (L2), the L1 must expose certain functionalities to support rollup operations. Conceptually, this defines a protocol between L1 validators and L2 provers. Logical functionalities expected from L1: Aggregate rollup transactions: L1 aggregates user-submitted data blobs in the order received, providing a reliable anchoring for L2 validity proofs. Verify proof submissions: L2 operators submit zk proofs that confirm correct processing of these transactions, ensuring L2 execution follows the agreed protocol and updating the L2 state commitments stored on L1. Entry/Exit of L1 funds: The protocol must enable deposits and withdrawals of native L1 funds to and from L2, ensuring consistent state and authorized spending. Key point: L1 acting as the zk proof verifier (point 2) is crucial for enabling the native currency (KAS) to serve as collateral for L2 financial activity, underscoring the importance of point 3. Additional benefits of having L1 verify the proofs include establishing a clear, single point of verification (enforced by L1 consensus rather than requiring each interested party to perform it individually) and providing proof of state commitment for new L2 validators. VM-based vs. UTXO-based L1 When the L1 is a fully-fledged smart contract VM, the establishment of this protocol is straightforward: the rollup designer (i) publishes a core contract on L1 with a set of rules for L1 validators to follow when interacting with the rollup; and (ii) specifies a program hash (PROG) that the L2 provers are required to prove the execution of (via a zk proof, ZKP). This two-sided interplay establishes a well-defined commitment of the rollup to its users. For UTXO/scripting-based L1s like Kaspa, a more embedded approach is required in order to support various rollup designs in the most generic and flexible way. Technical design of the L1 side of the protocol in UTXO systems To begin with, I present a simplified design which assumes a single rollup managed by a single state commitment on L1. Following this minimum-viable design I discuss the implications of alleviating these assumptions. Kaspa DAG preliminaries A block B \\in G has a selected parent \\in parents(B) as chosen by GHOSTDAG. The mergeset of B is defined as past(B) \\setminus past(\\text{selected parent of } B). The block inherits the ordering from its selected parent and appends its mergeset in some consensus-agreed topological order. Block C is considered a chain block from B 's pov if there\u2019s a path of selected parent links from B to C (which means the DAG ordering of B is an extension of C \u2019s DAG ordering). Detailed protocol design Recursive DAG ordering commitments: A new header field, called ordered_history_merkle_root, will be introduced to commit to the full, ordered transaction history. The purpose of this field is to maintain a recursive commitment to the complete sequence of (rollup) transactions. The leftmost leaf of the underlying Merkle tree will contain the selected parent\u2019s ordered_history_merkle_root, thereby recursively referencing the entire ordered history. The remaining tree leaves correspond to the ordered sequence of mergeset transactions, as induced by the mergeset block order. ZK-related opcodes: OpZkVerify: An opcode that accepts public proof arguments and verifies the correctness of the zk proof (our final design might decompose this opcode into more basic cryptographic primitives; however, this is out of scope for this post and will be continued by @reshmem). OpChainBlockHistoryRoot: An opcode that provides access to the ordered_history_merkle_root field of a previous chain block. This opcode expects a block hash as an argument and fails if the block has been pruned (i.e., its depth has passed a threshold) or is not a chain-block from the perspective of the merging block executing the script. It will be used to supply valid anchor points to which the ZKP can prove execution. State-commitment UTXO: The UTXO responsible for managing the rollup state will appear as an ordinary UTXO from L1\u2019s perspective, with no special handling or differentiation at the base layer. The UTXO spk (script_public_key) will be of type p2sh (pay to script hash), which will represent the hash of a more complex structure. Specifically, it will be the hash of the following pre-image: PROG (the hash of the permanent program L2 is obligated to execute) state_commitment (the L2 state commitment) history_merkle_root (the ordered_history_merkle_root from L1\u2019s header, representing the point in the DAG ordering up to which L1 transactions have been processed to produce the corresponding L2 state commitment) Additional auxiliary data required to verify a ZKP (e.g., a well-known verification key) The remaining execution script (will be specified below) Proof transaction: In its minimal form, a proof transaction consists of an incoming state-commitment UTXO, an outgoing updated state-commitment UTXO and a signature revealing the pre-images and the ZKP. Assuming such in, out UTXOs and a signature script sig, the following pseudo code outlines the script execution required to verify the validity of this signature (and logically verify the L2 state transition): // // All data is extracted from sig // Some operations below might use new Kip10 introspection opcodes // // Skipping the part where the script itself is proven to be in the preimage // (which is standard p2sh processing) // // Prove preimages show (prog_in , commit_in , hr_in , ...) is the preimage of in.spk show (prog_out, commit_out, hr_out, ...) is the preimage of out.spk verify prog_in == prog_out // verify prog is preserved // Verify L1 history-root anchoring extract block_hash from sig // the chain block we are claiming execution to hr_ref <- OpChainBlockHistoryRoot( block_hash ) // fails if anchoring is invalid verify hr_ref == hr_out // Verify the proof extract zkp from sig OpZkVerify( proof: zkp, proof_pub_inputs: [commit_in, commit_out, hr_in, hr_out]) // ^ omitting prog and other auxiliary verification data L2 program semantics: In order to provide the desired based rollup guarantees, the execution specified by the (publicly known) L2 PROG must strictly adhere to the following rules: Reveal (through private program inputs) the full tree diff claimed to be processed: T(hr_out) \\setminus T(hr_in) Execute the identified transactions in order, without any additions or removals Observe that the first rule, combined with the OpChainBlockHistoryRoot call within the script, ensures that the state commitment always advances to a valid state: The script verifies that hr_out is a valid chain block history commitment. The PROG verifies that hr_out recursively references hr_in and, as a result, must be an extension of it. This verification by PROG is enforced by L1 through OpZkVerify. Operational flow: Tx_1, Tx_2, ..., Tx_n with data payloads are submitted to the DAG, included in blocks, and accepted by chain blocks C_1, C_2, ..., C_n, respectively. The transaction hashes are embedded into the ordered_history_merkle_root fields of the corresponding headers, enforced as part of L1 consensus validation. An L2 prover chooses to prove execution up to block C_i A proof transaction referencing the initial state-commitment UTXO and producing a new state-commitment UTXO (encoding the new history root ordered_history_merkle_root(C_i)) is created. The proof is validated by L1, and the new state-commitment UTXO replaces the previous one in the UTXO set, recording the L2 state transition on L1. Soundness and flexibility of the proposed design The design presented supports fully based zk rollups by embedding ordered history Merkle roots into block headers and introducing new zk-related opcodes into Kaspa\u2019s script engine. The history roots provide modular and complete evidence of DAG ordering, enabling provers to select granularity over any period of consecutive chain blocks (while still requiring processing in mergeset bulks). The combination of L1 script opcodes and L2 PROG programmability provides substantial flexibility for rollup design. Points for subsequent discussion Despite its focus on the most baseline design, this post is already becoming rather long, so I\u2019ll wrap up by briefly outlining some key \u201czoom-in\u201d points for further discussion and refinement. Uniqueness of the state-commitment UTXO 1.1 Challenge: Proving that a specific UTXO represents \u201cthe\u201d well-known authorized state commitment for a given rollup. Although state transitions do not require such proof, it is important, for instance, for proving L2 state to a newly syncing L2 node. 1.2 Solution Direction: L2 source code can define a \u201cgenesis\u201d state (encoded in the initial UTXO), and zk validity proofs can recursively attest that the current state originated from that genesis. Entry/Exit of L1 funds 2.1 Allow deposits to static addresses representing the L2. 2.2 Allow withdrawals back to L1 (as additional proof transaction outcomes). 2.3 The N-to-const problem: Address the challenges arising from local limits on transaction size and the potentially many outcomes resulting from a batched proof operation. Extension to many rollups \u00b9 3.1 Requirement/Desire: Each L2 rollup prover should only need to execute O(\\text{rollup activity}) within their PROG proof execution. 3.2 Solution Direction: Manage the L1 history Merkle tree by grouping by rollup and further dividing into activity/inactivity branches. Note: Applying the grouping recursively might result in long-term storage requirements per rollup, which has broader implications. Multiple state commitments per rollup 4.1 Challenge: Allowing L1 to manage multiple state commitments for a single rollup in order to balance scalability and validity (allowing different provers to partially advance independent segments/logic zones of L2 state). 4.2 Solution Direction: Implement partitioned state commitments on L1, representing dynamic cuts of the L2 state tree. If taken to the extreme, this solution could allow a user to solely control their own L2 account via a dedicated state commitment on L1. Another major aspect not discussed in this post is the zero-knowledge technology stack to be supported and its implications for L1 components (e.g., the hash function used to construct the Merkle history trees). Laving this part to @reshmem, @aspect and others for full follow-up dives. [\u00b9] The introduction of many rollups (or subnets in Kaspa\u2019s jargon) touches on conceptual topics such as L2 state fragmentation and atomic composability, which are beyond the scope of this post and were preliminarily discussed in @hashdag\u2019s post. Here, I\u2019m referring merely to the technical definitions and consequences.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-09T13:09:26.728Z",
      "updated_at": "2025-01-09T12:48:11.413Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/1",
      "category_id": null
    },
    {
      "post_id": 347,
      "post_number": 2,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "I know that for zkEVM, a single account holds the balances of all users in the rollup. It looks like this design makes that account a UTXO which contains a commitment in the form of a merkle root of a merkle tree that commits to all the current balances of existing accounts in the rollup. Re 1: Highly cogent and reasonbale. Re 2: Will the entire tree be posted on-chain or just the root of the tree? If it is just the root, how do users get their branch in the tree in order to be capable of exiting without permission when they want to? Re 3: Not sure how you gonna do that based on your description. It sounds like some clustering. Personal thought is not sure if this is a high priority issue (do you need that many rollups on Kaspa or having one that works is more important?) Re 4: Not sure if understood correctly, but if the concern is about scalability and validity, I think fully distribututed ZKPs may be something interesting to think of. The scheme distributes proof generation across multiple machines and require minimal communication among them; it allows us to distribute ZKP generation in zkRollups and zkEVM among multiple participants as mining pools. Participants may share the reward for proof generation, akin to miners in PoW chains like Kaspa. This also is related to 3 since then groupin may be unecessary. One more thing: do we have a role like an aggregator (such as one in CKB)?",
      "raw_content": "",
      "author": "YesComrade",
      "created_at": "2024-12-10T21:26:43.666Z",
      "updated_at": "2024-12-10T21:26:43.666Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/2",
      "category_id": null
    },
    {
      "post_id": 351,
      "post_number": 3,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "In the basic design - yes, the UTXO will contain a single state root representing all existing rollup accounts. Re 2. Only the root. Any user operation on his L2 account will mutate the root, and the prover reporting this root mutation (via a proof tx) will be obligated to apply the outcome in L1 in the form of an additional tx output going to the user\u2019s L1 address. This will be enforced as part of the PROG. Re 3. As mentioned, the above description applies to the basic design. In a more advanced/dynamic design, we can have the L2 state committed to L1 in multiple, fragmented state commitments\u2014where at the extreme, a single account can have its own L1 state-commitment UTXO. My mental picture of this is keeping the image of L2 state as a single tree, but drawing a tree cut representing the subtree roots which are reported to L1 (see image) image1072\u00d7352 35.8 KB Notes: Such L2 split strategies can be static (as in scenario 2), or dynamic (scenario 3) in which case they will be supported via an L2 \u201cdetach\u201d command which locally breaks a commitment to its tree-child commitments (on L1 this will look like a proof tx with a single incoming UTXO representing the previous subtree root and multiple output UTXO entries representing its children) In scenario 3, accounts 13, 14 can be solely controlled by their sole owner, perhaps even requiring that each operation must be provided with an immediate inline ZKP. Think krc20 token holders performing a send\u2014the transaction can consume the state-commitments UTXOs of both the sender and recipient and can output the updated UTXOs with the updated state commitments, where the signature is an inline ZKP (credit: @reshmem, @aspect). Of course there are numerous subtleties and complexities to this dynamic design which I\u2019m neglecting here, one major one being \u201chow to prove a subtree state commitment can be advanced w/o needing to execute unrelated rollup transactions (for showing non of them touched this part of the state)?\u201d. This requires some form of \u201cexclusion proof\u201d or an explicit way to state read/write dependencies (cc: @aspect). Re 4. I totally agree, I think proof generation should be a distributed effort. Imho it can be a coordinated collective effort, and even centralized to some degree, since decentralization and censorship-resistance are enforced by L1 due to the based design guarantees. Re the final remark. Afaiu the aggregator role in CKB is exactly the non-based part where an L2 operator is required to collect off-chain batches, hence it\u2019s irrelevant to this based design. That being said, there\u2019s no way to enforce such a thing from L1 (unless you define a single METAPROG which all rollups must extend)\u2014the point is to allow based rollups and to make them the default way to go, not to forbid non-based approaches.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-11T11:01:55.226Z",
      "updated_at": "2024-12-11T11:01:55.226Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/3",
      "category_id": null
    },
    {
      "post_id": 354,
      "post_number": 4,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "This has been an interesting read so far. I\u2019m still trying to catch up to be able to properly parse and understand the content fully. For now, I do have some preliminary questions based on my first reading here: Entry/exit funds - will these use p2sh exclusively? Or p2pk for entry and exit but p2sh for state transitions? What does a state_commitment look like? Regarding the use of p2sh here referencing a single state commitment - does this mean that each state transition will necessarily use a different p2sh address each time? I\u2019m wondering about the impact of this on services like the explorer who will maintain several address entries that will keep getting used just the one time. With the statement: \u201cThe proof is validated by L1, and the new state-commitment UTXO replaces the previous one in the UTXO set, recording the L2 state transition on L1.\u201d - what happens during a re-org in L1? This will regularly happen around the tips. How is the L2 tolerant to such re-orgs? The post focuses on state transitions, but I\u2019m curious about how much funds would actually move between each transaction from p2sh to new p2sh? Each tx will incur a fee also. What are your thoughts on these amounts moved/paid on the base layer as state transitions in the L2?",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2024-12-12T06:56:20.319Z",
      "updated_at": "2024-12-12T06:56:20.319Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/4",
      "category_id": null
    },
    {
      "post_id": 355,
      "post_number": 5,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "I\u2019ll try to address the questions to some degree, though I think some of the answers will be fully clarified only after in-depth follow-up posts. Note: for simplicity, my points below still assume the baseline design with a single state-commitment UTXO per rollup instance, but can easily be extended to the multiple-commitment scheme described in my previous comment. Re 1. First - this area is definitely still not finalized and you (or any other reader) should join the brainstorming. My current thinking is as follows: Think of L2 (a rollup instance) as a \u201cvirtual\u201d wallet which might own many UTXOs on L1. All spks (addresses) for this wallet will be p2sh; however, some will be dynamic and some static. Like you mention in point 3, the state-commitment UTXO is an everchanging dynamic p2sh address. In addition to that there will be a set of static p2sh addresses which can be driven from PROG \u00b9. The static addresses will be used by users for deposit/entry. Spending these UTXOs must be done through a proof transaction and the spending sig must reveal the preimage script which will delegate verification to the primary input sig of the proof tx (the one containing the ZKP) \u00b2. Re 2. The L2 state_commitment would usually be a Merkle root of its state (e.g., the root of a Patricia tree). But our L1 design should not rely on specific assumptions about it. Re 3: As mentioned, yes, it means a different p2sh address each time. Explorers will learn how to track these addresses and treat them as a continuous entity by following the specific L2 encoding. Re 4. Great question. This is precisely why OpChainBlockHistoryRoot \u00b3 is set to fail if the block hash isn\u2019t a chain block from pov of the executing merging block. If a reorg invalidates a previously used anchoring chain block, then the proof tx using that anchor will be invalidated as well (when executed through the new chain), thus effectively \u201cunspending\u201d the spent state-commitment UTXO. This means that following a reorg, L2 provers will need to resubmit proofs proving execution according to the order dictated by the new chain segment. Your mental picture here should be a proof chain following the DAG selected chain. Note that if done correctly, L2 provers can reuse hierarchic zk proofs used to compose the previous proof-chain. I.e., a reorg does not necessarily mean full re-computation of all reversed proofs. Re 5. It can be the full amount deposited to L2 all concentrated in the \u201cdynamic\u201d state-commitment UTXO. I don\u2019s see this as an issue. [\u00b9] These addresses can incorporate KIP10-style additive schemes for improved management of the L2 UTXO subset and for compliance with KIP9 [\u00b2] The advanced reader might notice that this scheme requires the \u201cUniqueness of the state-commitment UTXO\u201d property I mentioned at the end of the post. [\u00b3] Unrelated note on OpChainBlockHistoryRoot. The chain depth we allow access to here will affect syncing of new L1 nodes. We will need to sync a chain segment of that length below the pruning point in order for the syncee to be able to process all transactions above the pruning point deterministically.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-12T11:10:23.989Z",
      "updated_at": "2024-12-22T15:17:24.401Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/5",
      "category_id": null
    },
    {
      "post_id": 394,
      "post_number": 6,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "It took me a while to find this ancient post in Bitcoin Forum: Storing UTXOs in a Balanced Merkle Tree (zero-trust nodes with O(1)-storage). I think it answers the basic questions of storing UTXOs plus that its #7 reply (starting with \u201cNode-deletion is not the inverse of node-insertion\u201d) is related to your Re 1. (addressing the entry/exit question) above.",
      "raw_content": "",
      "author": "LostandFound",
      "created_at": "2024-12-18T13:52:22.577Z",
      "updated_at": "2024-12-18T13:52:22.577Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/6",
      "category_id": null
    },
    {
      "post_id": 398,
      "post_number": 7,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "@michaelsutton then I think the biggest thing what this post wasn\u2019t addressing is the \u201chow to be based\u201d question? I can think of execution of a zkSNARK verification program pre-configured with a verification key (like a SNARK VM) . It can be run and implemented in a really simple way with only two challenge scenarios and just 3 instructions ADD, SUB and MULKAS, running any zk-snark generator lib targeting groth16, for example. Certainly bridge that allows users to transfer assets between L1 and L2 would be needed. That should also easily solve the problem of deposit and withdrawal. There may be two types of proofs that will be needed: one for transaction inclusion and another one for state transition of bridge sc on the L2 side. For both a zkSNARK groth16 proof can be a good fit and proof recursion should be supported to allow Plony2 and STARKs proofs if needed. But if a role of operator is introduced then I guess this is agian non-based, right? As based-approach may be challenging and taking some time and saying \u201cnot to forbid non-based approaches\u201d, will Kaspa welcome third party teams to make a non-based rollup plan on Kaspa? [Question 1] Also what happens if the target block with the target TX is invalidated by reorg? The OpChainBlockHistoryRoot looks like a revert. Practically, for an unconfirmed transaction does the transaction remain in the mempool and is reprocessed? Would this end up with a new TX hash? How might we be notified of this new TX? For a confirmed (multiple confirmations) transaction (though may not be likely) will it be marked as reverted? I guess the answer is Yes as you mentioned about to resubmit. Could this potentially lead to a situation in which a TX1 is confirmed on L1 while another TX2 is later confirmed on L1 on the canonical, which means TX2 is final but TX1 is reverted after TX2 is submitted? [Question 2] An additional thought on reorg: is it possible to figure out a way maybe just do experiements? I have seen people do it for Opstack by Setting up an L2 replica \u2013 Bridge OAS from L1 to L2 using three different accounts (A, B, C) \u2013 Fork L1, and then roll up L2 to the miner\u2019s side \u2013 Transfer \u2013 Merge L1\u2019s minor chain with the majority \u2013 Set up another replica. Then just see what happens to the accounts. Would it be possible to figure out ansuwers to such questions just empirically maybe? So different L2s will at least know what will happen at least. [Question 3]",
      "raw_content": "",
      "author": "YesComrade",
      "created_at": "2024-12-21T20:50:12.322Z",
      "updated_at": "2024-12-21T20:50:53.006Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/7",
      "category_id": null
    },
    {
      "post_id": 400,
      "post_number": 8,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "AFAIK Andrew\u2019s post deals with UTXO commitments (more accurately: accumulator), not sure what connection to this post you are suggesting here (?)",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-22T09:31:12.200Z",
      "updated_at": "2024-12-22T09:31:12.200Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/8",
      "category_id": null
    },
    {
      "post_id": 521,
      "post_number": 9,
      "topic_id": 208,
      "topic_title": "On The Design Of Based Zk Rollups Over Kaspas Utxo Based Dag Consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "michaelsutton: OpChainBlockHistoryRoot: An opcode that provides access to the ordered_history_merkle_root field of a previous chain block. This opcode expects a block hash as an argument and fails if the block has been pruned (i.e., its depth has passed a threshold) or is not a chain-block from the perspective of the merging block executing the script. It will be used to supply valid anchor points to which the ZKP can prove execution. I want to emphasize the fact that this opcode is stateful, which breaks the assumption that a script validity is dependent only on the transaction itself and its previous UTXOs. This means that a transaction that spends a UTXO with such opcode might be invalidated without an intentional double spend, which might harm the UX of the recipient. This can be dealt by introducing a concept similar to coinbase maturity for such transactions, or developing an off-chain mechanism to help users identify such transactions (and dependent transactions) so they can set a higher confirmation time.",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2025-02-23T15:17:32.886Z",
      "updated_at": "2025-02-23T15:17:32.886Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/9",
      "category_id": null
    },
    {
      "post_id": 356,
      "post_number": 1,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "This post was deeply inspired by ideas shared with me by @hashdag, @michaelsutton, and @proof. It assumes a strong foundational understanding of ZK concepts and the workings of ZK-protocols. This blog post is far from being a detailed design proposal, but the core concept is accurately outlined. A previous post by @michaelsutton outlined the basic L1<\u2013>L2 interaction. Here I dive in further into the technical bits: About Proving System Choice for L1 zk verification L1 Enhancements / Additions a. Low level elliptic curve opcodes b. L1 zk-verify function pseudo code as it should be implemented in Kaspa script c. ZK-friendly hash function choice (partly finished, require separate blogpost) d. UTXO ( input / output ) Anchoring via KIP10 (so-called canonical bridge) Possible L2 design examples a. Single proving system b. zkVM base solution c. Pseudocode for zk-SNARK Verification for Groth16 Additionally, the general guideline for implementing L2 solutions on top of Kaspa L1 should prioritize minimizing L1 modifications, ensuring support for a wide range of L2 solutions, and maintaining a strong guarantee of bounded performance impact for zk-verification executed by L1 nodes. This post serves as an attempt and example of how zk-L2s can be supported with minimal changes to the BTC-like Kaspa script language and L1 architecture, while still offering flexibility for diverse implementation options. Terminology and concepts adopted from @michaelsutton post: Recursive DAG ordering commitments Merkle root State UTXO / State commitment UTXO L1 based rollups (zk) ZK-related opcodes (OpZkVerify) 1. About Proving System Choice for L1 zk verification Groth16 / Plonk proving systems are proposed to be utilized due to their small proof size and constant-time verification complexity, which are critical for maintaining L1 performance. Groth16/ Plonk proving systems are highly stable, with thoroughly debugged technology stacks and rigorously audited, well-established mathematics. Despite not being recent developments, they remain state-of-the-art in terms of proof size, verifier efficiency, and overall stability and security. Moreover, these proving systems are the only ones natively supported by major blockchain networks such as Ethereum. Proving System zk-stacks & stability: Codebases for zk-proving systems like Groth16 and Plonk are mature, extensively tested, and battle-hardened across various blockchain ecosystems. This reduces the risk of bugs or implementation errors, ensuring a stable and secure integration. Elliptic Curve Choice: Groth16 / Plonk zk-proving systems are based on the BN254 elliptic curve (a Barreto-Naehrig curve with a 254-bit prime field). While these systems can operate with other pairing-friendly elliptic curves, BN254 offers a security level of approximately 100 bits, which is considered sufficient for many practical applications. Additionally, it is already supported by Ethereum-like chains, making it a strong candidate for adoption. 2. L1 Enhancements / Additions: To enable zk-proof verification and interaction between L1 and L2, the following enhancements are proposed: 2.a. Low level EC operations: Additions to BTC like Kaspa-script language: ADD: Modular addition for elliptic curve field arithmetic. MUL: Modular multiplication for elliptic curve field arithmetic and scalar multiplications. PAIRING: Executes elliptic curve pairing operations, required for verifying Groth16 / Plonk proofs. Source of Opcode Implementations: Implementations for `ADD`, `MUL`, and `PAIRING` opcodes will be derived from the stable and widely adopted blockchains, such as Ethereum. Most stable implementations are zCash, Geth (and maybe reth) Ethereum\u2019s opcode implementations for BN254 are well-tested and optimized for performance, providing a reliable foundation. 2.b. Details about Groth16 / Plonk ZK-Verify-Function (executed by L1 nodes): The verify function will be implemented directly in terms of the BTC-like Kaspa scripting language, along with the verification key and state hash. The exact definition of public parameters can vary based on the application, but this design ensures flexibility and trustless execution across use cases. The verification process requires only two public inputs *** enabling support for practically any desired logic. The verify function can be defined with four arguments: Verification Key: Less than 1kB. Public Parameter 1: 256-bit. Public Parameter 2: 256-bit. zk-Proof: Less than 1kB. The verification itself involves dozens of elliptic curve (EC) operations (ADD, MUL, and PAIR) and can be efficiently implemented within a Bitcoin-like Kaspa script if these operations are supported as was proposed earlier in this post. *** In general, more than two public inputs can be supported; however, the number should always be bounded, as the verification complexity increases linearly with the size of the public inputs. 2.c. ZK-friendly hash function choice (partly finished, require separate blogpost) L1 must provide additional support for zk-based L2 systems to prove transaction ordering as observed by L1. This ensures consistency and enables verifiable state updates. zk-L2s will leverage Merkle Inclusion Proofs, using Merkle Roots computed by L1 to verify transaction inclusion and ordering without requiring direct access to the complete transaction history during L1 verification. L1 will construct a Merkle Tree, where each leaf corresponds to an individual transaction. The Merkle Root will represent the set of all transactions in a block, and will be computed by L1 with a zk-friendly hash function (e.g., Poseidon2, Blake2s, or Blake3) ensuring compatibility with zk systems. This structure is referred to as Recursive DAG ordering commitments in a previous post by @michaelsutton Hash Function Considerations Efficiency: L1 requires a highly efficient hash function optimized for standard CPUs, as it must be executed by all Kaspa nodes. zk-Friendliness: L2 requires zk-friendly hash functions to compute inclusion proofs for each L2 transaction during proof generation. Security: The selected hash function must ensure robust security across both L1 and L2. For example, while the Blake2s/Blake3 family of hashes is field-agnostic and suitable for L1 (cpu friendly), it is significantly less performant than zk-friendly options like Poseidon2, which offers better compatibility with zk-proofs but is finate field dependent. Blake2s / Blake3 zk performance currently about at least 20 times slower than this of Poseidon2, moreover major zkVMs should be extended with Blake hash as its precompile circuit that is not a big problem but still should be done accurately and properly audited ( which can delay mainnet L2s ) Most major zkVMs (such as SP1, CairoVM, Risc0, Jolt, and Nexus) would / should be compatible with the proposed choice of hash function. \u201cCompatible\u201d means prove generation performance or number of circuit constraints per hash per byte of hashed data etc \u2026 An alternative approach could involve selecting a hash function optimized for Groth16 / Plonk, but this comes with at least two significant drawbacks: Fixed Tree Size: Groth16 and Plonk require a fixed tree size. While this limitation is manageable, it imposes a strict proving interval, meaning the number of transactions in the tree must remain constant for single zk-proof. Performance Impact: A more critical issue arises because transaction order and inclusion would not be proven by the zkVM itself. Instead, these transactions would become public inputs for the second-stage proving system (i.e., the zkVM verifier implemented using Groth16/Plonk). This approach results in increased zk-proof generation times, negatively impacting performance. The relationship between hash function choice, cpu vs zk performance and the dependency on finite field will be explored in a future blog post. Sub-Nets Support To enhance scalability and parallel processing, transactions can be categorized into sub-nets. The Merkle Tree\u2019s leftmost leaf will store the Merkle Root of the previous block and its merge-set (all transactions), ensuring consistent chain linkage. 2.d. UTXO ( input / output ) Anchoring via KIP10 (so-called canonical bridge) By utilizing KIP10 introspection opcodes, UTXO-input and UTXO-output anchoring can be incorporated into the spending script. This mechanism enables any party to provide a valid zk-proof to spend a State UTXO, ensuring that the newly created UTXO complies fully with the rules defined by the L2 state transition logic. It guarantees verifiability and security of L2 state transitions in a fully trustless manner, as KIP10 enforces the creation of specific UTXO-outputs when spending UTXO-inputs. This topic will be explored in greater detail in a separate blog post. For now, it suffices to say that KIP10 facilitates the linking of UTXO-inputs and UTXO-outputs within the same transaction through commitments. Summary for L1 additions: Math OpCodes: ADD, MUL, PAIR - allow implementation of zk-verifier (aka OpZkVerify) Merkle-Tree structure with zk-friendly hash function - allow TX inclusion proofs and ordering during state transaction (open question: hash function choice) Open Question: A dedicated opcode for constructing a Merkle root from public inputs may be required\u2014this will be addressed separately. In any case, the aim is to minimize additions to L1. The current Bitcoin-like Kaspa script capabilities are sufficient to support basic hash composition of public inputs. However, for more advanced schemes, L1 may need to provide additional support in the future. 3. Possible L2 Implementation examples How to Store L2 State in L1 UTXO The ScriptPubKey will store a hash representing a specific verification function, including its verification key and the L2 state hash. Zk-proof public parameters act as commitments to the previous and next states, dag recursive commitments and output UTXO (and maybe input UTXO), ensuring that the spending process is fully trustless. The ScriptPubKey field in L1 UTXOs will store the following information related to L2 state: Recursive DAG ordering commitments Merkle root State-Hash: Represents the Merkle root of the L2 state Program-Hash: Specifies the zk-circuit or state transition program. Verification-Key: Enables zk-proof verification on L1 ZK-Verification code 3.a. Example of a Single zk-proof system: State transition programs can be implemented directly using Groth16 or Plonk proving systems. This approach is well-suited for smaller, predefined, or existing solutions, for example zCash ZK core is implemented in terms of this proving system and at least theoretically can be deployed as is. In such cases, a single zk-proof is generated and verified on-chain. 3.b Example of a zkVM based solution: Another potential implementation for L2 is a zkVM-generated proof of state transitions. A zkVM enables the efficient execution of arbitrary smart contracts or virtual machine code (e.g., EVM, SVM) with state transitions verified through zk-proofs. This approach ensures program correctness while maintaining efficiency in proof generation and verification. Leveraging zkVM technology offers significant development flexibility and numerous essential features. Currently, zkVMs represent the dominant approach in the industry. They are continually improving, becoming more user- and developer-friendly, as well as increasingly efficient. Furthermore, two-stage proving can be seamlessly extended to N-stage proving, as zkVMs are capable of supporting additional zk-schema implementations on top of their existing framework. zkVM Codebase Compatibility: This approach enables integration with major zkVM codebases, including Risc0, SP1, and CairoVM and others, ensuring broad interoperability and ecosystem growth. Example of using 2 zk-proof systems (zkVM\u2192Groth16/Plonk): zkVM (e.g., Risc0, CairoVM, SP1): This system will prove the execution of a state transition program (e.g., EVM) alongside transaction ordering and inclusion proofs. Groth16 / Plonk: This proving system will implement a zk-verifier for the zkVM. zkVM proof will be feeded inside Groth16 / Plonk circuit that implement zkVM verifier. This will generate another zk-proof that going to be verified on-chain by L1. It will generate a zk-proof that is sent on-chain and verified by all Kaspa L1 nodes. 3.c. Pseudocode for zk-SNARK Verification for Groth16. Constants - PRIME_Q (256 bits, 32 bytes): Prime field size for alt_bn128 (aka BN254) elliptic curve. Data Structures 1. G1Point (512 bits, 64 bytes) - Represents a point on the curve with: - x (256 bits, 32 bytes). - y (256 bits, 32 bytes). 2. G2Point (1024 bits, 128 bytes): - Represents a point on the extension field with: - x[0], x[1] (each 256 bits, 32 bytes). - y[0], y[1] (each 256 bits, 32 bytes). 3. VerifyingKey (640 bytes): - alfa1 (G1Point, 64 bytes). - beta2 (G2Point, 128 bytes). - gamma2 (G2Point, 128 bytes). - delta2 (G2Point, 128 bytes). - ic (array of G1Points, size depends on the number of public inputs + 1; each G1Point = 64 bytes). 4. SnarkProof (192 bytes): - a (G1Point, 64 bytes). - b (G2Point, 128 bytes). - c (G1Point, 64 bytes). Elliptic Curve Math 1. P1 and P2: - P1 (512 bits, 64 bytes): Generator point for G1. - P2 (1024 bits, 128 bytes): Generator point for G2. 2. Negate Point: - Input: G1Point (64 bytes). - Output: Negated G1Point (64 bytes). 3. Addition: - Inputs: Two G1Point values (each 64 bytes). - Output: Resultant G1Point (64 bytes). 4. Scalar Multiplication: - Inputs: G1Point (64 bytes), scalar s (256 bits, 32 bytes). - Output: Scaled G1Point (64 bytes). 5. Pairing: - Inputs: Arrays of G1Point and G2Point (each G1Point = 64 bytes, each G2Point = 128 bytes). - Output: Boolean (1 byte). 6. Pairing Check: - Input: Four pairs of G1Point and G2Point (4 x 64 bytes + 4 x 128 bytes = 768 bytes). - Output: Boolean (1 byte). Verification Logic (logic that will be supported directly by L1) 1. Verify Function: - Inputs: - vk (Verification Key, 640 bytes). - input (Array of public inputs, each 256 bits, 32 bytes; total size = number of inputs \u00d7 32 bytes). - proof (SNARK proof, 192 bytes). - Steps: 1. Input Validation: - Ensure input.length + 1 == vk.ic.length. - Ensure all input[i] values are less than FIELD_SIZE (256 bits, 32 bytes). 2. Compute vk_x: - Start with vk_x as G1Point (0, 0) (64 bytes). - For each public input: - Scale vk.ic[i + 1] (64 bytes) by input[i] (32 bytes) using scalar_mul. - Add the result to vk_x (64 bytes) using addition. - Add vk.ic[0] (64 bytes) to finalize vk_x. 3. Pairing Check: - Use pairingProd4 (extended pairing function for 4 arguments) to verify the pairing: - Inputs: Negated proof a (64 bytes), proof b (128 bytes), vk.alfa1 (64 bytes), vk.beta2 (128 bytes), vk_x (64 bytes), vk.gamma2 (128 bytes), proof c (64 bytes), and vk.delta2 (128 bytes). - Output: Boolean (1 byte). - Output: - Returns true (1 byte) if the proof is valid, false (1 byte) otherwise. Summary of Sizes 1. Key Sizes: - Verification Key: 640 bytes. (example for 2 public inputs) - Public Inputs: Each 256 bits, 32 bytes. - SNARK Proof: 192 bytes. 2. Operations: - Addition (G1Point): Input = 64 bytes \u00d7 2, Output = 64 bytes. - Scalar Multiplication (G1Point): Input = 64 bytes + 32 bytes, Output = 64 bytes. - Pairing Check: Input = 768 bytes, Output = 1 byte.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2024-12-12T14:44:57.329Z",
      "updated_at": "2024-12-12T20:46:09.126Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/1",
      "category_id": null
    },
    {
      "post_id": 359,
      "post_number": 2,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Are any of the zk-friendly hash functions considered sufficiently safe by cryptanalysts? And is there a reasonable chance that some new zkp system variant would require choosing a different hash function?",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-13T04:54:10.344Z",
      "updated_at": "2024-12-13T04:54:10.344Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/2",
      "category_id": null
    },
    {
      "post_id": 390,
      "post_number": 3,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Re sufficiently safe: Each of zk-friendly hashes should be addressed separately. For example Poseidons family functions, are widely used by zk community in production projects and by zk-community it considered sufficiently safe, but by wider cryptographers / research / cryptanalysts community, the question reminds open. I think for proper answer to this question we need help of @proof & Elihai. And if my understanding is correct, re Blake family hash functions are considered to be safe ! Re zkp system variant would require choosing a different hash function: The short answer, most probably yes. Slightly more unfolded answer: for example for hash function like Poseidon, which is finite field dependent, it is clear that each zk system will prefer that chosen Poseidon hash to be based on its (zk-system) native finite field. This way the performance will be better since it will be highly aligned with specific proving system. This is one of the reasons we need to choose it carefully. I will try to address these things in separate post. Blake family on the other hand can be presumably \u201cgood\u201d for all / most of existing zkp systems, but not the best. Other perspective is the CPU friendliness, which is good for Kaspa-L1, that should be considered as well. To summarize: the final choice is multi dimensional and part of the requirements contradict with each other.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2024-12-16T11:10:30.362Z",
      "updated_at": "2024-12-16T11:12:27.002Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/3",
      "category_id": null
    },
    {
      "post_id": 395,
      "post_number": 4,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Regarding hashes (after consulting with smart folks): Preliminary remark: \u201czk-friendliness\u201d of a hash function is not a purely intrinsic property - it depends on design/efficiency of the underlying AIRs (at least in the context of STARKs). AIR efficiency can vary by orders of magnitude, and of course AIR design depends on the particular proving protocol and implementation (it\u2019s a bit of an art). To start with some names, Blake, Pedersen, and Poseidon are all used in production rollups (e.g Starknet). Blake (like keccak and sha256) consists of many bitwise operations so it\u2019s very execution friendly. From the zk perspective, bitwise operations are only native to binary fields, which are not commonly used at the moment (neither for EC nor for STARKs). Hence Blake isn\u2019t zk-native. As remarked above, zk-friendliness depends on the ambient proving protocol and implementation. There\u2019s a very efficient AIR for the Stwo prover for STARKs over Mersenne 31. Pedersen (roughly) takes two felts, concatenates them, and returns the associated multiple of some EC generator. It\u2019s relatively zk-friendly if your proving protocol works over the same field. Pedersen lacks entropy in small fields such as Mersenne 31 and consequently isn\u2019t secure in the naive approach. On the other hand, proving is much more efficient over small fields. There\u2019s a middle ground if you use ECs over an extension field, but then you\u2019re losing proving efficiency due to the overhead of representing EC ops using base felts. Note EC ops are also relatively heavy to execute. Poseidon takes a vector of felts and returns another via sequences of matrix multiplications and pointwise powers and additions. Hence it\u2019s very execution-friendly and also quite zk-friendly assuming efficient AIRs. It also has plenty of entropy so it can be used over smaller fields. If you want a secure zk-friendly hash compatible with small fields, I think this is the best option. I\u2019d choose between Blake and small-field Poseidon depending on whether you want to optimize for execution time or proving time.",
      "raw_content": "",
      "author": "proof",
      "created_at": "2024-12-19T13:54:23.515Z",
      "updated_at": "2024-12-19T13:57:10.296Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/4",
      "category_id": null
    },
    {
      "post_id": 399,
      "post_number": 5,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Thanks for this info @proof! Optimizing for execution of L1 ops should be primary over prover efficiency, so let\u2019s aim for Poseidon, unless someone provides new arguments. Poseidon over what field do you recommend we use? (Elichai, any chance to get your input on Poseidon safety?)",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-22T08:37:28.784Z",
      "updated_at": "2024-12-22T08:37:28.784Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/5",
      "category_id": null
    },
    {
      "post_id": 401,
      "post_number": 6,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "@hashdag, @proof indeed suggested that Poseidon takes \u2026 hence it\u2019s very execution-friendly (where by \u201cexecution-friendly\u201d I\u2019m assuming he means L1 friendly), however afaik from @reshmem it is still an ~order of magnitude more expensive to compute than Blake-class hash functions. So it seems like we need exact benchmark numbers before making any conclusion here?",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-22T13:44:02.976Z",
      "updated_at": "2024-12-22T13:44:02.976Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/6",
      "category_id": null
    },
    {
      "post_id": 402,
      "post_number": 7,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Sorry, my post was unclear. Blake will probably be faster to run on x86 than Poseidon (even on a small field). Perhaps the execution time will be comparable if the field is sufficiently small, in which case Poseidon will be the best of both worlds (as it\u2019s much more efficient to prove with Circle STARKs over a small field). In my opinion such a core level decision warrants benchmarks.",
      "raw_content": "",
      "author": "proof",
      "created_at": "2024-12-22T15:52:19.405Z",
      "updated_at": "2024-12-22T15:52:19.405Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/7",
      "category_id": null
    },
    {
      "post_id": 405,
      "post_number": 8,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Here are benchmarks (CPU not ZK) for Blake3 & Poseidon2. Apple MacBook Pro M4, single threaded. chatgpt only helped to organise the results in table telegram-cloud-photo-size-4-5999079653719000034-y1071\u00d7833 93.2 KB Few comments: Blake3 perf stays the same for 32 bytes and 64 bytes input since pre-image length of Blake3 is 64 bytes Small input as 64bytes is still important for Kaspa L1 since during Merkle Tree construction ~1/2 hashes will be with 64bytes input For Poseidon2 bench I used Plonky3 lib. Blake3 is blake3 v1.5.3 (official rust implementation) These are preliminary results, that maybe good enough for discussion but still far from being extremely accurate. Question: @proof - do I understand it correctly that number of AIR constraints for Blake3 for n-rounds = 7, is 2976 + 900 ~= 4k ? *taken from the link provided by you.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2024-12-23T20:47:25.975Z",
      "updated_at": "2024-12-24T08:27:34.606Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/8",
      "category_id": null
    },
    {
      "post_id": 406,
      "post_number": 9,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Am I correct in my understanding that groth and plonk will both require a trusted set-up phase for the L2s? I consider it desirable to allow L2 to avoid trusted set ups if they so choose, even if it comes at the price of something else. Do the suggested op codes allow for Starks and (best) transparent snargs to be implemented?",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2024-12-24T13:43:39.350Z",
      "updated_at": "2024-12-24T13:43:39.350Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/9",
      "category_id": null
    },
    {
      "post_id": 413,
      "post_number": 10,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "@proof In continuation to our conversation, could you describe here the different methods you see in which Starks can be integrated?",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-04T21:46:52.348Z",
      "updated_at": "2025-01-04T21:46:52.348Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/10",
      "category_id": null
    },
    {
      "post_id": 440,
      "post_number": 11,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Zero-knowledge proof is a rapidly developing technology. Even for Plonky, there are multiple versions like Plonky2, Plonky3, and FFLONK. In Ethereum, this can be implemented through smart contract updates. How does Kaspa ensure that it can quickly support more advanced proof systems when they become available on the market? Or must it undergo another hardfork?",
      "raw_content": "",
      "author": "Dash",
      "created_at": "2025-01-19T14:40:02.682Z",
      "updated_at": "2025-01-19T14:48:19.647Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/11",
      "category_id": null
    },
    {
      "post_id": 465,
      "post_number": 12,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Indeed both Plonk & Groth16 require trusted-setup ceremony, but not for L2s. For each version of zkVMs in case of chain of 2 proof systems. If L2 decides to implement its logic directly in terms of Plonk/Groth16 then yes.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T12:09:43.679Z",
      "updated_at": "2025-01-20T20:35:27.112Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/12",
      "category_id": null
    },
    {
      "post_id": 466,
      "post_number": 13,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "We\u2019re implementing the EC curve ops which will allow sufficient flexibility without direct need for hard-forks.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T12:12:37.520Z",
      "updated_at": "2025-01-20T12:12:37.520Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/13",
      "category_id": null
    },
    {
      "post_id": 481,
      "post_number": 15,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "reshmem: We\u2019re implementing the EC curve ops which will allow sufficient flexibility without direct need for hard-forks. this is an assumption of a EC-based zk system? hash-based zk system would still be a hard fork?",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-23T20:55:26.788Z",
      "updated_at": "2025-01-23T20:55:26.788Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/15",
      "category_id": null
    },
    {
      "post_id": 482,
      "post_number": 16,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Not in current construction. Any zk-verify function (hash-bases, EC) can be implemented in terms of EC-based Math-Opcodes ( Groth16 / Plonk).",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-24T11:57:11.071Z",
      "updated_at": "2025-01-24T11:57:11.071Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/16",
      "category_id": null
    },
    {
      "post_id": 484,
      "post_number": 17,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "reshmem: can be implemented in terms of EC-based Math-Opcodes ( Groth16 / Plonk). is there secondary proving going on? \u201cWhile traditional zk-SNARKs rely on cutting-edge cryptographic hard problems and assumptions, the only cryptographic ingredient in a STARK proof system is a collision-resistant hash function.\u201d Anatomy of a STARK Anatomy of a STARK, Part 0: Introduction STARK tutorial with supporting source code in python.",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-24T14:42:11.717Z",
      "updated_at": "2025-01-24T14:42:11.717Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/17",
      "category_id": null
    },
    {
      "post_id": 487,
      "post_number": 18,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Dear @superMainnet - can I kindly ask you to re-read the whole blogpost ? Key Points to Note A chain of zk proofs (STARK/any other new proof system \u2192 SNARK) leverages the best of both worlds: \u2022 STARK for efficient, transparent proof generation of large-scale computations. \u2022 SNARK for cost-effective on-chain verification (small proof size, constant-time checks). This architecture is increasingly discussed in modern zero-knowledge ecosystems, as it alleviates the on-chain verification bottleneck of large STARK proofs while still enjoying the transparency and scalability that STARKs offer off-chain. Proof Size STARK proofs grow (roughly) polylogarithmically/log with circuit size, which is still quite efficient for large n (especially with continual optimizations). Groth16 has some of the smallest proof sizes (~128\u2013192 bytes) and constant verification time, but each circuit needs its own trusted setup. PLONK typically produces a constant-size proof \u2014 often a few hundred bytes. This is larger than Groth16\u2019s but still considered \u201csmall\u201d in absolute terms. Verification Complexity STARK verification is generally O(log n) or a small polynomial in log n. Groth16 verification is constant (independent of n)\u2014only a handful of pairings (e.g., 3 pairings total). PLONK also has a constant number of pairings/polynomial checks and is therefore constant time in circuit size. Trusted Setup STARK systems avoid a trusted setup entirely (fully transparent). Groth16 requires a circuit-specific setup (each new circuit needs a new setup). PLONK uses a universal (or updatable) setup that can be reused across many different circuits. Security Assumptions STARK: Relies on collision-resistant hash functions and no heavy number-theoretic assumptions\u2014often touted as plausibly or partially post-quantum secure, though current standardization is still evolving. Groth16 and PLONK: Rely on elliptic-curve pairings and discrete-log assumptions, which are not believed to be secure against large-scale quantum computers. In summary, STARKs offer transparency and good asymptotic scalability but come with larger proof sizes. Groth16 remains popular for minimal proof sizes and fastest verification, while PLONK offers a middle ground of small (but slightly larger) proofs, constant verification, and a single universal trusted setup that simplifies deployment across many circuits. Observations Tens to Hundreds of Kilobytes \u2022 Real-world STARK proofs often land in the tens/handreds of kilobytes range for moderate-scale applications (batching thousands of trades, or verifying mid-sized programs). \u2022 Even not extremely large circuits (millions of constraints) can push proofs into the **hundreds of kilobytes / megabytes **. Ongoing Optimizations \u2022 Projects like StarkWare continue to optimize the FRI protocol, polynomial commitment schemes, and Cairo\u2019s AIR (algebraic intermediate representation). These improvements may reduce proof sizes further or keep them manageable as circuit complexity grows. Comparison to Groth16 / PLONK \u2022 STARK proofs are significantly larger than the constant-sized proofs typical of Groth16 or PLONK (a few hundred bytes to sub-kilobyte). \u2022 In exchange, STARKs offer transparency (no trusted setup) and have good scaling properties for very large circuits. Here is a recent comparison for zkVMs ( SP1, risc0 are stark-based ). image887\u00d71001 56.5 KB In addition, stark-based verifiers are not easily implementable with BTC-like kaspa scripting language, hence: Each version of such verifier per specific version of zk-VM requires a hard-fork Kaspa L1 should commit upfront to specific zkVMs which is not the case with basic EC math op-codes. Math op-codes are not in any sense are equal to this. zkVM codebases are extremely complex and new, with ton of bugs inside and hundreds of thousands lines of code. They are safe in a way that people put real money in these L2s / zk-rollups but solid chains like Ethereum still are far from accepting it inside ( requirement is at least formal verification of code vs math ) And many more other \u2026 \u201cbuts\u201d \u2026",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-26T09:45:38.766Z",
      "updated_at": "2025-01-26T09:49:52.461Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/18",
      "category_id": null
    },
    {
      "post_id": 488,
      "post_number": 19,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Just to put here for the reference EIPs related to these math-ops: EIP-196: Precompiled contracts for addition and scalar multiplication on the elliptic curve alt_bn128 EIP-197: Precompiled contracts for optimal ate pairing check on the elliptic curve alt_bn128 EIP-1108: Reduce alt_bn128 precompile gas costs Here is in review status of EC ops related to BLS12-381: EIP-2537: Precompile for BLS12-381 curve operations Here is a ton of information about ECs accepted in Ethereum, why and how and why others were rejected \u2026 https://ethresear.ch https://ethereum-magicians.org",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-26T13:21:57.998Z",
      "updated_at": "2025-01-26T13:34:47.364Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/19",
      "category_id": null
    },
    {
      "post_id": 492,
      "post_number": 21,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "reshmem: A chain of zk proofs (STARK/any other new proof system \u2192 SNARK) leverages the best of both worlds: the wall of ai generated background info is appreciated, but in the interest of time a simple \u201cyes\u201d would have saved me 20 minutes superMainnet: is there secondary proving going on?",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-28T16:22:54.145Z",
      "updated_at": "2025-01-28T16:23:14.616Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/21",
      "category_id": null
    },
    {
      "post_id": 493,
      "post_number": 22,
      "topic_id": 219,
      "topic_title": "Additional Practical Considerations Re Hash Function And Zk Opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "fyi groth16 seems to be falling out of favor in the Ethereum community x.com vitalik.eth (@VitalikButerin) on X @VitalikButerin One of the many things that we need to acceeeeeeelerate is abolishing groth16 Per-application trusted setups are just not ok in the 2020s. Universal setup at the minimum, ideally no setup at all This requires big improvements on infra and standardization for newer SNARK algos x.com Mauro Toscano \ud83d\udfe9 (@MauroAligned) on X @MauroAligned It's incredible that Groth16 is still widely used today. It's not the fastest, requires a trusted setup, it's not even universal, and most of the new tech have to do a lot of work to convert from their proving system to it. But not a surprise, since it's the cheapest proving",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-28T16:26:15.555Z",
      "updated_at": "2025-01-28T16:26:15.555Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/22",
      "category_id": null
    },
    {
      "post_id": 404,
      "post_number": 1,
      "topic_id": 237,
      "topic_title": "Updateable List Of L1 L2 Topics To Flesh Out Before Finalizing Design",
      "topic_slug": "updateable-list-of-l1-l2-topics-to-flesh-out-before-finalizing-design",
      "content": "For the sake of visibility and coordination of research core, dumping here an updateable list of research topics that must be fleshed out before we present an initial holistic design for L1<>L2. All topics and terms will be defined and explained clearly in separate independent posts. Published high level op_codes; L1<>L2 conceptual protocol: see @michaelsutton\u2019s post. cryptographic primitives, low-level op_codes: see @reshmem\u2019s post. Needs to be written/published: Benchmarks towards final decisions on zk-friendly hash; see comments to @reshmem\u2019s post State commitments: uniqueness of the state commitment utxo. Done proving race rules; liveness attacks (proof construction to prevent ramifications). Done multiple state commitments per rollup (related to intra subnet parallelism) Sequencing commitments; hierarchical data structure (linear; consider usecase for log). Done Entry/exit mechanisms; virtual wallet management. Done Throughput regulation; fee market; gas vs fees; global vs per subnet gas limit; (related: multidimensional knapsack problem). Done L2\u2019s interop: messaging protocol; design for sync atomic composability",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-23T13:49:35.720Z",
      "updated_at": "2025-01-20T11:31:42.541Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/updateable-list-of-l1-l2-topics-to-flesh-out-before-finalizing-design/237/1",
      "category_id": null
    },
    {
      "post_id": 417,
      "post_number": 1,
      "topic_id": 247,
      "topic_title": "Fees And Throughput Regulation Dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "Context: based rollups. Since L2\u2019s are permissionless, they too need a mechanism to cap their throughput. As per the custom, let\u2019s refer to the computational load on L2 nodes in gas units. This load applies to any L2 operator \u2013 entities parsing the state, eg L2 explorers, wallets, etc. \u2013 but more importantly to provers. The latter\u2019s load is 2-3 order of magnitude heavier than entities merely executing the state. What entities should regulate L2 throughput? One could imagine a setup where an L2 instance caps its own throughput by encoding a gas limit rule in its program, but this could lead to an undesirable outcome where users\u2019 transactions are included in L1 blocks but ignored by the corresponding L2. If we want to avoid such horrible UX, then L1 sequencers are the only relevant entities to take on the responsibility of regulating L2 throughput. Let\u2019s start by defining the requirements for a regulation mechanism: L2 throughput must be capped. L1 shouldn\u2019t typically include transactions that exceed the throughput cap. L2 quality of service (QoS) should be preserved: Users with higher urgency should be able to prioritize their txns. (This crucial property is often overlooked, ignoring this requirement can lead to practical DoS for urgent transactions, similarly to the havoc on KAS, June '24, due to the lack of RBF to deal with txn congestion.) The mechanism shouldn\u2019t rely on users\u2019 honesty or miners\u2019 goodwill. The mechanism shouldn\u2019t require L1 actors executing L2 logic. L2 provers should be compensated for sustainability. One simple approach we propose (@FreshAir08 +the usual suspects) is: i. Transaction issuers declare the maximum gas their transactions consume. ii. Miners ensure no more than X gas units are confirmed per block per L2 instance (enforced in L1 consensus). iii. Transactions that exceed their declared maximum gas are reverted but still pay their fees, similar to Ethereum L1 rules. Note: Wallets can typically provide reasonable gas estimates. Even for complex transactions, users can often give tight bounds, so they won\u2019t usually suffer from gross misestimates. iv. Transactions that don\u2019t exhaust their declared gas still pay the full gas fee to L2 from the perspective of L1. Whether this is refunded to the user within L2 logic is outside this discussion. [A riddle for the reader: If users are refunded the gas change, can they overestimate gas just to be lazy? Why not?] It is easy to verify that this design satisfies all aforementioned requirements. Observe an important implication of this design: When L2 demand is high, revenue and fees go to L1 sequencers, not to L2 provers. I argue this is not a problem, and the preservation of L1\u2019s security budget outweighs L2 layer biz model concerns. An alternative proposal will be be described at the end of the post. Another observation is that introducing gas units and limits per block (point ii above) adds complexity for sequencers (miners) optimizing their blocks. It effectively creates a multi-knapsack problem for sequencers, which is NP-hard. However, this computation isn\u2019t done on-chain or in consensus, and simple heuristics can achieve a sufficient approximation of the optimal solution. Importantly, this challenge isn\u2019t unique to gas units; similar considerations apply to storage-mass (KIP9) units. See KIP13 by @coderofstuff. Importantly, each L2 instance can define its own interpretation and scale for gas\u2014there\u2019s no implication in L1 consensus about what gas specifically means. If one instance computes gas differently \u2014 whether in a simple or complex manner \u2014 that\u2019s fine. L2 instances can also dynamically adjust gas scales based on load (@proof note this). However, this dynamic adjustment should be carefully designed to avoid situations where users\u2019 transactions are included by miners but ignored by L2 programs, causing fee loss and poor UX. Finally, since we proposed that all transaction fees \u2014 regardless of rising demand \u2014 flow to sequencers, one might ask how L2 provers are paid and what guarantees their compensation and sustainability. One alternative advocated by @someone235 is that a fraction of L1 fees associated with a certain L2 instance flow to its P2SH address and be distributed to provers via its PROG. Hopefully others can evaluate this direction or provide alternative L2 sustainability schemes",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-01-07T17:09:52.136Z",
      "updated_at": "2025-01-08T21:10:55.183Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/1",
      "category_id": null
    },
    {
      "post_id": 420,
      "post_number": 2,
      "topic_id": 247,
      "topic_title": "Fees And Throughput Regulation Dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "A couple of notes and thoughts. Many of these might protrude a bit into the realm of the L2 design itself which may be out of scope but I want to mention them for the reader. First, there is no real problem in the L2 requiring another inner L2 fee in proportion to the L1 miner fee (if the inner fee is not sufficient, txs are included but considered invalid from the L2 perspective) is there? I think of the inner fee moving to a SC with its locked money only released by a proof tx. That\u2019s a cleaner way to reward L2 provers for high activity in their subnet imo than forcing L1 to deal with the distribution. Upon reread that might be what was suggested in the post but I wouldn\u2019t use the term \u201cL1 fees\u201d for that. Second, without stepping too much into the L2 shoes, I believe L2 rewards should encourage fast proving in some manner, i.e. rewarding higher fees for proofs that come \u201cfast enough\u201d. Could be handled in a similar manner to what I suggested above, with the excess money going back to the original payer if proofs didn\u2019t arrive fast enough. Of course this does not apply to all L2: some may just be service providers and will provide fast proofs to their clients as part of their service. Lastly one might ponder what happens if the monetary policy of L2 ends up being unsustainable due to price drop, or unjust due to price increase. L2s can and arguably should have some update mechanism for the gas scale (and potentially other things). Obviously this update mechanism and the extents of what is allowed to be updated, presents many questions of L2 governance, which L2 creators will need to address according to their specific needs and ethea.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-08T18:29:57.808Z",
      "updated_at": "2025-01-08T20:20:21.364Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/2",
      "category_id": null
    },
    {
      "post_id": 444,
      "post_number": 3,
      "topic_id": 247,
      "topic_title": "Fees And Throughput Regulation Dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "Do I understand correctly why \u201crevenue and fees go to L1 sequencers, not to L2 provers\u201d \u2013 is it precisely because it \u201cadds complexity for sequencers (miners) optimizing their blocks\u201d? Before, miners were incentivized to perform only the optimization to select transactions based on storage-mass, now, in addition to that, they will also perform transaction selection based on gas amount. While economics and profitability of miner business very much depends on the demand and the competition, we can think of ZK provers as almost having constant costs to operate - therefore, they are not in danger if they are paid just enough? Or maybe you say a more general thing, since you seem to downplay the actual amount of additional work they will do in practice, that it is only miners that are incentivised in the system for increased demand for block space, no matter where the demand is coming from?",
      "raw_content": "",
      "author": "oudeis",
      "created_at": "2025-01-19T22:00:02.770Z",
      "updated_at": "2025-01-19T22:00:02.770Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/3",
      "category_id": null
    },
    {
      "post_id": 474,
      "post_number": 4,
      "topic_id": 247,
      "topic_title": "Fees And Throughput Regulation Dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "Even if L2 txns incur no special costs to miners, no added complexity etc., still the miners are inevitably the entity to receive users\u2019 txn fees and to enjoy surge in demand for L2 txns, b/c they are the (only) gatekeepers to the entire system, so the user must incentivize them \u2013 bribe, if you may \u2013 in order to be sequenced sooner rather than later. The more pressure and demand for L2 txns \u2013 the more impatient users are willing to pay to enter the sequencing in a timely manner \u2013 and if this willingness to pay does not transfer to the miners themselves in the form of greater revenue from these impatient txns then the miner would ignore these high priority txns, which destroys the QoS and practically leads to a DoS for L2. Whether you need another L2-dictated form of payment or mechanism to satisfy the economics or sustainability of provers \u2013 I did not form a strong opinion on that, there are several options that come to mind. One could argue, as you mentioned, that provers\u2019 load is const and therefore they should require no extra compensation, others can argue that a healthy macro dynamic is one where service providers receive higher compensation in case of higher interest/demand in their services. L2\u2019s can decide their own mechanism here, as long as they comprehend that fees for prioritization under demand must inherently flow to the miners.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-01-22T22:04:59.535Z",
      "updated_at": "2025-01-22T22:04:59.535Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/4",
      "category_id": null
    },
    {
      "post_id": 422,
      "post_number": 1,
      "topic_id": 251,
      "topic_title": "Conflicting Proofs Policy",
      "topic_slug": "conflicting-proofs-policy",
      "content": "In Kaspa based rollups design, entities called provers will regularly submit proofs of an ordered aggregation of txs that occurred since the last proof was submitted \u2013 a point in time which I will refer to as the latest settlement point. More precisely, txs are proven in batches according to the chain blocks that accepted them. These proofs are treated as regular txs from the point of view of L1. Currently there is no set bound planned on how late or how early can these proofs arrive. Provers may cooperate completely on the proving effort \u2013 there is no clear danger in them cooperating as the results of their computation are deterministically decided by the contents of the L1. Nevertheless for reasons of lack of coordination, or for plain avarice, two provers may end up submitting two distinct proofs, starting from the same L2 latest settlement point. The two proofs will naturally agree on a prefix of the proven txs, but one will usually contain a suffix missing from the second. I refer to such events as conflicting proofs. If the longer of the two is the first included in the L1 ledger, then it is clear that the second proof is to be ignored. But if the shorter is included first, it is less obvious what to do. Naively, the longer proof is treated as a double spend attempt/ invalid tx and ignored. However there are several issues with it, first, in terms of quality of service, the longer proof contains txs that the L1 has yet to see the proof of. Ideally, these proofs can be included immediately, but the naive design foregoes that. Second, the prover has worked tirelessly to create their longer proof, but now will have to start from anew, leading to losses, potentially making proving financially unsustainable if this occurs routinely. Third, this paves the way for a liveness attack on the L2: a malicious party can regularly submit proofs proving 1 block of txs to scoop all other provers (as supplying long proofs takes more time for the benevolent provers), resulting in the settlement to L1 progressing very slowly. Two possible solutions come to mind: the first, perform major logical changes to allow miners to include a more inclusive proof even if its settlement point has already been deprecated. I believe this is possible to engineer in some manner, but will likely demand a further level of utxos abstraction, more opcodes, and might end up introducing other complications. The second solution is the empty solution where we will accept this phenomenon as a fact of life, yet explain away why the problems are not as fatal as they superficially seem: a property of Snark/Stark proofs as I understand them is that they are componentized. i.e., proofs are not necessarily created as one massive indivisible black box block, rather the different stages of the computation could be broken apart and proven separately, then combined together. If this supposition is true, then even if scooped, a prover only loses the work put to prove the subset of txs proven by its competitor, and some constant metadata. Thus they can relatively quickly rebuild a new proof from the surplus txs they previously proved but their competitor did not. It is self apparent that if this supposition is true it will mitigate the effects of all 3 of the issues discussed above, without the need to take drastic measure. However the \u201cif\u201d here is crucial: I address the zk audience (@reshmem and @proof are explicitly called, but please others join in) with a question: is this supposition about the way snark/stark proofs behave true? As a sidenote, it is worth mentioning that \u201cscooping\u201d is not all bad: if the fear of being scooped rules the ring, one might expect provers to compete within themselves on proving as fast as possible, vastly improving quality of service. However the exact mechanics required to ensure such competition without collapsing to cannibalism and monopoly require more thought, and seem out of scope for this post.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-08T18:34:30.099Z",
      "updated_at": "2025-01-09T10:07:02.213Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/conflicting-proofs-policy/251/1",
      "category_id": null
    },
    {
      "post_id": 483,
      "post_number": 2,
      "topic_id": 251,
      "topic_title": "Conflicting Proofs Policy",
      "topic_slug": "conflicting-proofs-policy",
      "content": "Please don\u2019t treat my answer as a complete one because it will be too long to answer and in any case I think that even with a very long detailed answer there is not possible to cover everything since every particular choice of ZK system should be considered separately. ZK schemes / systems in general support technics as recursion / folding and other smart technics on parallel proof generation but, this topic is very depends on particular system, privacy requirements and many other factors and choices made for this particular zk-system. It looks like it is up to the L2s to choose what is good for them. For example STARK based proving systems with recursion and / or folding are able to support it. Packing stage ( G16 / Plonk ) does not support recursion / folding natively, but MSMs can be paralleled, for example via Pippenger\u2019s algo and FFTs parallel nature is obvious. Especially given the fact that witness in this case has no visible private requirements. Privacy protocols that have this type of requirements will use known technics how to accomplish what the need. Even without such support ( no deep dive to the details here ), it looks possible to control desired behaviour via L2s logic. For example L2s can choose the provers\u2019 queue and anchor it inside spending script via ZK itself ( or via BTC-like kaspa script + Merkle-Root )\u2026 Bottom line, I think L1 should not care about it at all ( at least for the next 1-2 years . I highly support your second solution ) and let L2s to decide what is good for them and how to do it in a best possible / available way ( hence every week we see new game-changing zk scheme comes out. But, I still think that this problematic should be reflected in our overall design suggestions in order to provide (at least) some guidelines to these L2s that are new to ZK concepts. Auditors (ZK) on the other hand will catch these points very easily and it is a responsibility of each L2 to conduct such audit.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-24T12:30:59.310Z",
      "updated_at": "2025-01-24T12:37:19.472Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/conflicting-proofs-policy/251/2",
      "category_id": null
    },
    {
      "post_id": 429,
      "post_number": 1,
      "topic_id": 258,
      "topic_title": "L1 L2 Canonical Bridge Entry Exit Mechanism",
      "topic_slug": "l1-l2-canonical-bridge-entry-exit-mechanism",
      "content": "Context/prerequisite: part 1 \u2013 based rollups design. A primary objective of the based rollup design is to support the entry and exit of L1 KAS funds to and from the L2\u2014a mechanism often referred to as a canonical bridge. The end result should be a bridged KAS token (henceforth referred to as BKAS), which can be used on L2 and has a 1:1 relation with native L1 KAS. From the perspective of L1, this means there will be a \u201cpool\u201d of native KAS allocated across several UTXOs, which is locked and owned by the L2. The internal distribution of this pool is managed by the L2 state. A user should be able to initiate an \u201centry\u201d by sending KAS to their L2 account via an L1 operation\u2014sending KAS to a designated, well-known L2 address while specifying (via the payload) their destination address within L2. Complementarily, they should be able to initiate an \u201cexit\u201d request, demanding that the L2 send their BKAS back from L2 to an L1 address. Distinguishing semantics of Entry and Exit operations From the perspective of L1, entry and exit operations serve fundamentally different purposes and feature distinct behaviors. An entry operation is an L1 transaction that is validated inline by L1 itself, making it effective immediately for both the transfer of funds on L1 and the corresponding state update on L2. In contrast, an exit operation requires internal L2 authorization, which cannot be directly validated by L1. Instead, it depends on the submission of a zero-knowledge proof (ZKP) to confirm its validity and take effect on L1. This semantic distinction highlights a key operational difference: entry funds can be used immediately on L2, even before the entry operation has been formally proven via ZKP. For example, an entry operation could provide the necessary collateral for a subsequent proof and exit request by another user, even if its own proof is still pending. This immediate usability of entry funds reflects the simpler integration of entry operations, contrasting with the more complex requirements of exit operations. Below, we dive into the technical details and complexities of designing a bridge to support entry and exit operations. Specifically, canonical bridges have been successfully designed in the industry and are relatively easy to come up with when the L1 is an SC layer as well. Our goal here is constructing such a bridge above the UTXO/scripting model. We focus on two key aspects: Using Kaspa\u2019s scripting capabilities to form user-friendly entry addresses that can be easily used by users. This set of addresses and their corresponding UTXOs will effectively define an \u201cL2 virtual wallet\u201d which can be used by provers for managing L2-owned KAS funds on L1. Addressing the issue of many exit operations resulting from a single proof transaction, which might surpass local L1 transaction size limits. Note: Exit operations are one type of outcome that must be transmitted from L2 to L1. More generally, this primitive is also required for asynchronous messages sent over L1 between rollup instances. These outcomes, verified by L1 as part of the ZKP check, attest to state reads within L2 and can be shared in plain text with other L2s. This process can be viewed as part of an abstract \u201coutbox\u201d that the prover is obligated to deliver to L1. Static entry addresses and the primary state commitment UTXO As detailed in part 1, the state commitment UTXO of a rollup instance contains an ever-changing dynamic p2sh address. Conceptually, this address cannot be used as the receiving address for entry operations, both because it is short-lived (expiring with the next proof) and because spending it inherently requires a full ZKP. Instead, we define static addresses that delegate their spending authorization to an existing ZKP provided by primary proof scripts. This motivates the following introduction of static \u201cdelegation\u201d p2sh addresses, which directly resolve both issues: the address is no longer dynamic, and its spending signature no longer requires full ZKP verification. Specification of static delegation scripts To focus on solving the foundational problem, I assume a single state commitment UTXO representing the full rollup state. Extending the discussion below to multiple state commitment UTXOs per rollup introduces additional complexities that require careful consideration and is therefore out of scope for this post. Preliminaries To establish a foundation for the discussion, let us revisit the definition of key components related to the state commitment process: Denote SCU_i as the i'th State Commitment UTXO of a given rollup instance since inception. In other words, [SCU_0, \\dots, SCU_i, \\dots, SCU_n] forms a chain of UTXOs where the i'th proof transaction spends SCU_{i-1} and creates SCU_i. Denote key(SCU_i) as the key identifier of this UTXO on L1. As per standard UTXO management, this key is composed of the following tuple (often referred to as the outpoint): (\\text{prev tx id}, \\text{output index}). In other words, the UTXO entry is uniquely identified by the transaction that outputs it and the index of that specific output within the transaction. Similarly, denote script(SCU_i) as the p2sh script specified within SCU_i. Define the rollup\u2019s canonical L1 script as CANON (see pseudo code below). Recall that PROG is the hash of the permanent program L2 is obligated to execute. Canonical state commitment script To provide a more rigorous explanation of the canonical script\u2019s role in verifying state commitments, its process is detailed below in pseudo-code (reiterating** parts of the code described in part 1 under \u2018Proof transaction\u2019). This expanded explanation also integrates the p2sh preimage verification step, ensuring a complete picture: ** Note that I\u2019m adopting @hashdag\u2019s terminology (here) and referring to the previously known history_merkle_root as the sequencing_commitment (seq_in in the code below). Inputs: script_hash, sig_script ------------------------------- [<sig_data>, <redeem_script>] = sig_script // Standard p2sh preimage verify verify OpBlake2b(input: redeem_script) == script_hash // Decompose redeem_script into its canonical script (only opcodes) and // its data arguments. // The L2 PROG will appear as a push data opcode in the canonical // script, creating a strong binding between them. Additionally, the // script encodes the incoming L2 state commitment (state_in), and // the incoming L1 sequencing commitment (seq_in) [<CANON>, [<PROG>, <state_in>, <seq_in>]] = redeem_script // Execute the verified preimage script (converted into pseudo code for // brevity). // I.e., CANON is the actual pseudo code listed below in its stack-based // script language format (this transfer to execution of the inner script // is part of the standard p2sh processing) Execute CANON(PROG, state_in, seq_in, sig_data): // Extract arguments from signature data [<out_script>, <chainblock_hash>, <zkp>] = sig_data // Verify the output script preimage. OpTxOutputSpk is a new Kip10 // introspection opcode and is a crucial component in defining // this \"covenant\" between the spending and the output scripts verify OpBlake2b(input: out_script) == OpTxOutputSpk(index: 0) // Decompose the out script, which is expected to be in the same format // as the input script and deviate only in the dynamic elements (state // and seq). Note that this decomposition might require new data-masking // opcodes. [<out_canon>, [<out_prog>, <state_out>, <seq_out>]] = out_script // Verify PROG is preserved verify out_prog == PROG // Verify the canonical script itself is preserved (excluding // the extracted variables state_out and seq_out). This can be // thought of as the \"covenant\" by which the output script must // follow the input script verify out_canon == CANON // Verify L1 sequencing root anchoring verify OpChainBlockHistoryRoot(hash: chainblock_hash) == seq_out // Verify the state transition ZK proof OpZkVerify(prog: PROG, proof: zkp, proof_pub_inputs: [state_in, state_out, seq_in, seq_out]) // ^ omitting other auxiliary verification data The canonical script enforces the correctness of state transitions, ensuring both the input and output states, as well as sequencing commitments, are consistent with the L2 program (PROG ) and the ZKP. Static delegation script To support entry operations without directly relying on dynamic state commitments, the following delegation script is proposed. This static script delegates certain responsibilities to the canonical script while remaining independent of the current state or history. Inputs: script_hash, sig_script ------------------------------- [<sig_data>, <redeem_script>] = sig_script // Standard p2sh preimage verify verify OpBlake2b(input: redeem_script) == script_hash // Decompose redeem_script into its script part and data arguments [<DELEGATE>, [<PROG>, <CANON>]] = redeem_script // Execute the delegation script Execute DELEGATE(PROG, CANON, sig_data): [<primary_script>] = sig_data // Verify primary script preimage verify OpBlake2b(input: primary_script) == OpTxInputSpk(index: 0) // Decompose primary script [<canon_primary>, [<prog_primary>, ...]] = primary_script // Verify PROG is preserved verify prog_primary == PROG // Verify the delegation targets a correct canonical script verify canon_primary == CANON The delegation scheme builds on the inherent property that a transaction is only accepted when all its inputs are validated as correctly spent. Through this mechanism, the delegator ensures that the primary input script meets specific requirements, authorizing its own spending if those conditions are satisfied. The delegation script described above achieves a static structure by avoiding reliance on specific sequencing or dynamic state commitments, as shown by the redeem_script preimage in the code snippet. However, without showcasing further properties, the scheme remains vulnerable to the following attack vector. Attack vector. Alice sends funds from a standard L1 Schnorr address to a script structured correctly with CANON and PROG but using fabricated state and seq commitments. She then constructs a proof transaction, using the resulting UTXO as the primary input and adding additional inputs from static delegation addresses, redirecting the KAS funds to the primary state-commitment output. The result is that the funds are locked in a malformed state-commitment UTXO distinct from the authentic proof chain. Even if she uses a valid (state, seq) pair, the problem persists as she has effectively created a separate proof chain. The challenge. Addressing this attack requires incorporating information beyond the script itself. This is because L1 cannot enforce restrictions on the \u201centrance\u201d to a script covenant\u2014for instance, any user can send funds to an opaque p2sh address. Consequently, authentic and forged state-commitment UTXOs are indistinguishable at the script level. Solutions based on static registration of rollup-associated information outside the UTXO set are currently ruled out, as they would introduce significant complexity and compromise the cleanliness and soundness of L1 state management. Key-based authentication. An alternative solution involves leveraging UTXO keys to differentiate between authentic and non-authentic state-commitment UTXOs. A UTXO key is uniquely derived from the transaction that creates it, allowing the authenticity of a UTXO to be tied directly to its creating transaction. One possible approach relies on defining a genesis UTXO in the L2 source code and proving that the current UTXO input belongs to a transaction chain originating from this genesis. While requiring the entire proof chain as a witness in each new proof transition is impractical, a feasible alternative involves using a constant-length suffix of the proof chain, as outlined in the following section (in collaboration with @FreshAir08; based on preliminary discussions with @reshmem & @hashdag as well). Key-based state-commitment UTXO authentication We propose the following scheme as in illustrative example of a construction providing state-commitment authenticity. A GENESIS UTXO is hardcoded in L2 PROG via its source code The CANON part of the canonical script is hardcoded in L2 PROG as well. Note that this is a pure script without any dependency on PROG (thus no hash cycles) The ZKP receives as public input the key of the currently spent state-commitment UTXO, i.e., key(SCU_{i-1}) The corresponding L2 prover program (producing this ZKP) receives as private witness the full i-2, i-1 proof transactions and the preimage of script(SCU_{i-2}) The following logic is executed as part of the program: // L2 program running for each proof transition // // Arguments: input_key - the UTXO key of the currently // spent state-commitment UTXO // source_tx - the source transaction of input_key // (i.e., proof transaction i-1) // source_script - the preimage of the script hash used as // input for source_tx // script_affirming_tx - proof transaction i-2; provided for // affirming source_script (since the // input of i-1 does not specify it) PROG(...,pub_inputs: [..., input_key], private_inputs: [..., source_tx, source_script, script_affirming_tx]): ... // First, verify that input_key is indeed an output of source_tx verify (blake2b(source_tx), 0) == input_key if source_tx.inputs[0].outpoint == GENESIS: // Allow entering the covenant only via the hardcoded genesis UTXO pass else: // Otherwise, we must verify that input_key was produced as part of the // covenant. To do that we acquire the incoming script through the i-2 // transaction, decompose it, and verify that it follows the expected // canonical script // Verify the linkage between the two transactions (i-2 -> i-1) verify (blake2b(script_affirming_tx), 0) == source_tx.inputs[0].outpoint // Verify that source_script is the preimage verify blake2b(source_script) == script_affirming_tx.outputs[0].script // Decompose the script [<canon>, [<_prog>, <_state>, <_seq>]] = source_script // Verify it follows the expected canonical script verify canon == CANON ... Definition 1: A state commitment UTXO is well-formed if its script preimage can be decomposed into [<CANON>, [<PROG>, <*>, <*>]]. Definition 2: A state commitment UTXO is forged if it hasn\u2019t originated from the GENESIS UTXO, i.e., GENESIS was never part of a transaction chain leading to it. Claim 1: A well-formed forged state commitment UTXO is unspendable. Proof: Assume for contradiction that such a UTXO exists. Then there must be a maximal transaction X on the chain creating it, where the first input script S_1 is not well-formed (or at the very least missing altogether, since root transactions are always coinbase). By the maximality of X, it follows that the output of X, used as the first input for the following transaction, is well-formed. Denote this output as S_2. There are two cases to consider: Case 1: S_1 is partially malformed Here, S_1 can be decomposed into [<CANON>, [<PROG'>, <*>, <*>]], but it uses an incorrect PROG'. In this case, X would fail L1 verification because S_2 will decompose into [<CANON>, [<PROG>, <*>, <*>]], and the L1 CANON execution would attempt to verify that PROG' is preserved, resulting in failure. Case 2: S_1 is completely malformed Here, S_1 cannot even be decomposed into [<CANON>, [<*>, <*>, <*>]]. However, S_2 is well-formed. From our contradiction assumption, S_2 must be spendable. The spender must provide a valid ZKP based on the real PROG. However, the prover must include X as a witness, and during PROG execution, both branches will fail: (i) X's first input key cannot be GENESIS, as it is forged; and (ii) S_1 cannot be decomposed into CANON, so it will fail the final line of execution. Corollary: Delegation scripts cannot be redirected to a forged proof chain. Proof: The last line of DELEGATE verifies that the primary input script is well-formed, thus by Claim 1 it can only be spent if it is not forged. L2 initialization procedure To initialize an L2 system with this scheme, the process begins by sending KAS on L1 to an ordinary address (e.g., Schnorr) controlled by L2 initiators. The resulting UTXO key from this transaction is then added to the L2 source code and designated as GENESIS. This key serves as the starting point for the rollup\u2019s transaction chain. The next step is to create the canonical script CANON and add it as a constant to the L2 source code. While CANON is not directly part of the L2 system, it is essential for enabling chain link verification between transactions. Once CANON is defined, the L2 program is compiled to produce PROG, which serves as the main logic for generating proofs. Using CANON, PROG, and the initial state and seq, the initial state-commitment script is composed. This script represents the starting point for the rollup\u2019s state and sequencing commitments. An L1 transaction is then performed to transfer funds from the GENESIS UTXO to the newly created state-commitment script, marking the rollup\u2019s formal initiation. After the transaction is confirmed, a proof of its acceptance on L1 should be saved as part of the L2\u2019s integrity data. This proof can also be shared with new L2 nodes to establish trust in the L2 initialization process. The combination of L1-approved proof transactions and L2-verified chain links ensures that only authenticated state-commitment UTXOs pass ZKP verification on L1, effectively mitigating the attack vector of redirecting delegated address funds. Syncing new L2 nodes This scheme provides a trustless mechanism for fully syncing new L2 nodes from the recent state. By verifying the correct state-commitment UTXO and the suffix of the transaction chain leading to it (e.g., the last two transactions), new nodes can use the authenticated L2 state commitment on L1 to confirm the newly synced L2 state is consistent with this commitment. Exit operations as proof outputs (Thx @FreshAir08 for writing the majority of this section.) After a user issues a \u201cwithdrawal\u201d transaction to L2, the associated funds are no longer available in L2 but remain in the L2-owned addresses. When a proof is submitted, it must ensure and enforce the transfer of these funds from the addresses to the requested L1 address. Conceptually, these pending withdrawal transactions form an outbox of exit operations, which can be inferred from the executed L1 transactions within the proved period. To support this mechanism, we propose the following additions: L1 CANON script change: modify CANON to compute the cumulative hash of all outputs in the current transaction except the primary state commitment output (similar to the Schnorr sighash process). This hash will be passed to OpZkVerify as an additional public proof input. L2 PROG change: update PROG to compile a list of expected proof outputs from the outbox and compute their cumulative hash using the same method employed by CANON. This hash is then verified against the public input provided in the proof. N to Const problem So far, we have assumed that the prover can include all pending exit outputs in a single proof transaction. However, due to mass limitations (e.g., KIP9), these added outputs could potentially take up a significant amount of block space. While higher fees for the prover or slower L2 progress are concerns, the main issue is that the transaction might exceed the block size limit, causing a deadlock in the L2. In extreme cases, even a single mergeset could congest an entire block. We explore two possible solutions to address this issue: Instead of directly including all L1 addresses in outputs, public keys (or more general scripts) can be used. Provers would transfer funds to special-purpose p2sh addresses that represent all public keys associated with the withdrawals. These addresses would allow each public key to spend only its share of the funds, similar to a KIP10-like mechanism. The public keys could be stored in a Merkle tree within the redeem script, and an additional opcode might be added to verify Merkle witnesses efficiently. If funds are not immediately repatriated to L1, they can remain in a designated L2 outbox, which acts as an extension to the L2 state. The program would enforce that the active L2 state cannot advance until the outbox has been cleared. If the outbox becomes too congested, the L1 sequencing commitment would remain unchanged, and only the outbox would be updated. Adapting the hash commitments discussed earlier to this structure would be straightforward. A more lenient variation of this idea could allow state advancement with partial outbox clearing. For example, provers might be required to clear the outbox at twice the rate they add to it. This restriction could activate only once the outbox exceeds a predefined congestion limit.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-13T12:49:19.647Z",
      "updated_at": "2025-01-13T20:28:30.653Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258/1",
      "category_id": null
    },
    {
      "post_id": 490,
      "post_number": 2,
      "topic_id": 258,
      "topic_title": "L1 L2 Canonical Bridge Entry Exit Mechanism",
      "topic_slug": "l1-l2-canonical-bridge-entry-exit-mechanism",
      "content": "michaelsutton: Key-based state-commitment UTXO authentication Worth mentioning that after writing this post I bumped into a similar authenticity proposal by starkware: To address this, we perform what is called a \u201cgenesis check.\u201d Essentially, we make the aggregation covenant check its previous transaction and the transaction preceding that one \u2014 that is, its ancestor transactions. The covenant verifies that these transactions contain the same covenant script and perform the same checks. In this way, we achieve an inductive transaction history check. Because both of the previous transactions performed the same checks as this covenant does, we know that the ancestors of that transaction did the same, all the way back to the leaf (i.e., the genesis transaction).",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-28T10:40:09.751Z",
      "updated_at": "2025-01-28T10:40:09.751Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258/2",
      "category_id": null
    },
    {
      "post_id": 438,
      "post_number": 1,
      "topic_id": 267,
      "topic_title": "Optimal Transaction Selection A Multidimensional Knapsack Problem",
      "topic_slug": "optimal-transaction-selection-a-multidimensional-knapsack-problem",
      "content": "KIP9 (Persistent Storage Mass) and KIP13 (Transient Storage Mass) introduced new types of mass which represent some resource consumption within a node. Originally there was only compute mass (usage of compute resource) but now there is also persistent storage mass (usage of storage space that persist between pruning periods) and transient storage mass (usage of storage space only within a pruning period). The KIPs above also propose to independently track each mass when checking for block mass limit on the consensus level. This allows fo optimal block space allocation for transactions that consume different resources. The block mass limit is 500,000 grams. So such independent tracking allows for including two transactions that have masses (compute:490,000g; persistent_storage_mass: 10,000g) and (compute:10,000g; persistent_storage_mass: 490,000g) in the same block. Now, an interesting question arises: On the mempool level, when constructing the block template, how can the transaction selection logic be updated to optimize for block space consumption?",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2025-01-18T21:42:56.325Z",
      "updated_at": "2025-01-18T21:42:56.325Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/optimal-transaction-selection-a-multidimensional-knapsack-problem/267/1",
      "category_id": null
    },
    {
      "post_id": 463,
      "post_number": 1,
      "topic_id": 274,
      "topic_title": "Subnets Sequencing Commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "Context/prerequisite: Based rollup design Notation glossary: H - a hash function, e.g. SHA256, or blake, (see discussion here re zk-friendly hashes) MT(x_1,x_2...,x_k) - the Merkle tree derived from x_1,...,x_k, with some implicit hash function H MR(x_1,x_2...,x_k) - the root of the above Merkle Tree SQC(B) - the \u201csequencing commitment\u201d field of block B B.sp - the selected parent of the block B Introduction In Kaspa\u2019s based rollup design we suggested introducing a new field, which serves as a sequencing commitment (previously referred to as the history root). The new field is recursively defined as follows: let B be a block, and let t_1...t_n be all of B's accepted transactions (more accurately, the tx-hash or tx-id of those, to be determined at a later stage), in the order they were accepted in. Ignoring initialization of the first block in the smart contracts hardfork, the new field SQC(B) is: SQC(B)=H(SQC(B.sp),t_1, t_2...t_n),^1 as illustrated below: image1209\u00d7679 149 KB To prove the progression of this field between a selected chain block Src and a selected chain block Dst matches the progression of the rollup, the prover requires all txs that were accepted between both of them as private inputs - inputs that the prover of a statement requires to produce a proof, but the verifier of that proof does not. More formally, these are all txs that were accepted on a selected chain block in the future of Src and in the past of Dst, including Dst itself. This scheme is perfectly functional, but it has the following inefficiency: a prover of a certain rollup, is really only interested in those txs that are directed to their rollup. All other txs, and any block that accepted no tx associated with this rollup, are \u201cnoise\u201d from their point of view. Yet in order to prove they are not associated with the rollup, provers are forced to take those as private inputs, increasing their runtime complexity. In particular, if a rollup is only active once every 5 minutes, it would still have to go through the entire selected chain and all transactions accepted on it. I explore below a method to represent a particular rollup history in a more concise manner. Separate Subnet Histories A rollup is represented by a field in the transaction called subnetId, and we refer to each rollup as a \u201csubnet\u201d. Our suggestion (courtesy of @michaelsutton) is that SQC have an inner structure representing the different subnets\u2019 histories. For a block B, and for each subnet i, let t_{i_1}...t_{i_k} be those transactions within B's accepted txs t_1, t_2...t_n that belong to subnet i. We say a subnet i is active at block B, if there ever in history was an accepted transaction belonging to subnet i prior or at block B (this permanent definition of activeness will be relaxed below). We can recursively define an abstract field: if block B accepted no txs of subnet i this field takes the same value as its selected parent, i.e. SQC_i(B)=SQC_i(B.sp). Otherwise, SQC_i(B)=H(SQC_i(B.sp),t_{i_1},...t_{i_k}), or simply SQC_i(B)=H(t_{i_1},...t_{i_k}), if SQC_i(B.sp) was previously undefined. Lastly, SQC(B)=MR(SQC_i(B)|\\textit{i is active as of block } B), meaning the Merkle root of the sequencing commitments of all active subnets. The prover of subnet i now only requires the accepted txs between Src and Dst that belong to subnet i, as well as witnesses to prove the inclusion of SQC_i(Src), SQC_i(Dst) in SQC(Src) and SQC(Dst). These two witnesses are of logarithmic size in the number of subnets active in the network. This prover now has O(A_i+\\log R) private inputs, where A_i is the number of txs in subnet i at the span in between Src and Dst, and R is the number of active subnets. For some concreteness, with current parameters, \\log R is at most 160. The solution as is requires each L1 node to keep track of and compute SQC_i for all R subnets that are active, and subnets never become deactivated. Effectively this creates a \u201cregistry\u201d of subnets. Thought must be devoted to prevent unlimited growth of this registry or spamming attacks. I propose implementing pruning of the subnets: nodes will also store the last (selected chain) DAA score at which subnet i was updated, and subnets will only be considered active if the current DAA score is less than finality-depth greater than the last update on the subnet. If subnets become indisputably inactive, their stored data can be thrown away completely during pruning. This ensures a bound on the number of active subnets at any given time. Naive use of Merkle trees may result in recalculation of the entire tree at every block. To prevent this constant recalculation, I advocate using sparse Merkle trees. For ease of syncing and general data tracking, SQC_i(B) should be updated to explicitly contain the last DAA score subnet i was updated at, e.g., SQC_i(B)=H(DAA(B),H(SQC_i(B.sp),t_{i_1},...t_{i_k})) whenever an update occurs. A sketch of the structure of subnets history is provided below: image1196\u00d7698 172 KB The pruning means that a subnet should make at least a single tx every finality-depth blocks if it wishes not to be pruned. Finality depth is planned to be about half a day after the 10bps hard fork. It appears to me like a reasonable requirement of a subnet to demonstrate its activity by issuing a tx every 12 hours. Nevertheless, there is value in allowing a fallback mechanism in case that for whatever reason a subnet has been pruned prematurely. Hence I suggest we add another branch to SQC(B)'s sparse Merkle tree, called SQC_{TOT} (B)=H(SQC(B.sp),t_1,...,t_n) which stores the total history as originally proposed, without division to subnets and without pruning. That way pruned subnets could still supply proofs - at the cost of waiving all optimizations.^2 Another advantage of maintaining this divisionless branch, is that it maintains the original ordering of the accepted txs in their entirety, which may be required for some applications that choose to track all txs. [1] Previously @michaelsutton suggested H(MR(SQC(B.sp),t_1, t_2...t_n)). The current reasoning to use a Merkle tree instead of a cumulative hash mostly reduces to overloading the new field with the responsibility previously kept by the Accepted Transactions Merkle Tree field, or possibly, the responsibilities of the Pchmr field proposed in KIP6 - if we later choose to implement a \u201clogarithmic jumps\u201d lookout into history instead of the linear one described here. This kind of decisions are to be made at a later stage and I see fit to decouple them from the subject at hand. [2] It is worth emphasizing that while a proof end point must refer to a relatively recent block for it to be valid, its starting point can be arbitrarily old.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-20T11:22:14.519Z",
      "updated_at": "2025-01-20T11:32:28.541Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/1",
      "category_id": null
    },
    {
      "post_id": 464,
      "post_number": 2,
      "topic_id": 274,
      "topic_title": "Subnets Sequencing Commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "I think that some L2s will be interested to know the order between 2 transactions from 2 different subnets. SQC-TOT will allow to extract this information as well, but maybe it worth considering to extend definition of the leaves to be subnet transaction and global sequence number ( sequence number of the transaction in the global acceptance order in this block ).",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T12:04:33.393Z",
      "updated_at": "2025-01-20T12:04:33.393Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/2",
      "category_id": null
    },
    {
      "post_id": 470,
      "post_number": 3,
      "topic_id": 274,
      "topic_title": "Subnets Sequencing Commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "You\u2019re right, that\u2019s probably the better way to go. For the fallback mechanism we only really need SQC(B.sp) rather than anything else, and hashing the global index alongside the tx will prevent the need to hash all txs twice.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-20T15:08:10.221Z",
      "updated_at": "2025-01-20T15:08:10.221Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/3",
      "category_id": null
    },
    {
      "post_id": 471,
      "post_number": 4,
      "topic_id": 274,
      "topic_title": "Subnets Sequencing Commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "FreshAir08: Naive use of Merkle trees may result in recalculation of the entire tree at every block. To prevent this constant recalculation, I advocate using sparse Merkle trees. @FreshAir08 - can you please unfold this a bit \u2026 I can see how SMT helps for proof of non-inclusion ( very useful in some zk cases \u2026), but where / how SMT will decrease CPU side computation done by Kaspa L1 ?",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T21:09:24.676Z",
      "updated_at": "2025-01-20T21:09:24.676Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/4",
      "category_id": null
    },
    {
      "post_id": 500,
      "post_number": 5,
      "topic_id": 274,
      "topic_title": "Subnets Sequencing Commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "Following the conversation, I\u2019d like to make a final proposal/recap combining all: hierarchic subnets; subnet expiry; global mergeset sequence number and elements of KIP-6. I\u2019ll use \\psi, \\psi_i as short for SQC, SQC_i. Notation: mergeset(B) := [t_1, ..., t_n] denotes the sequence of n txs accepted by block B in order for each subnet i, let t_{i_1}...t_{i_k} be those transactions within mergeset(B) that belong to subnet i. Note that i_j is the original index within mergeset(B) of the j'th subnet tx. we (abuse notation and) say i \\in mergeset(B) iff there exists a tx in mergeset(B) which belongs to subnet i B.sp_i is the i'th selected parent starting from B. I.e., B.sp = B.sp_1, (B.sp).sp = B.sp_2 and so on. Denote expiry(i, B) := bluescore(B) - bluescore(B.sp_m); where B.sp_m is the most recent chain ancestor of B such that i \\in mergeset(B.sp_m); or genesis if no such block. Definition 1: \\psi_i(B) := \\psi_i(B.sp); iff i \\notin mergeset(B) H(\\psi_i(B.sp), (t_{i_1}, i_1), ..., (t_{i_k}, i_k)); otherwise Definition 2: \\psi(B) := MR(\\psi(B.sp_1), \\psi(B.sp_2), ..., \\psi(B.sp_{2^m}), \\psi_i(B) \\text{ }| \\text{ for all } i \\text{ s.t. } expiry(i, B) \\le F) where F is some expiry constant, and m is set in a way detailed later. The above definition of \\psi(B) accomplishes the following: Subnets can prove their execution in O(\\text{subnet activity}) time as long as there\u2019s at least a single subnet tx every F epoch (nit: each proof requires additional O(log(\\text{#non-expired-subnets}) + log(\\text{mergeset-tx-limit}))) The size of the maintained subnet tree (supporting the construction of \\psi(B)) is bounded by the number of non-expired subnets which in turn is bounded by F\\cdot \\text{mergeset-tx-limit} (assuming a worst-case scenario where each tx in the epoch belongs to a unique subnet). By setting F\\approx \\text{pruning-period-len} we get a balance where tree storage requirements are only a fraction of the storage required for keeping header and transaction data in the pruning period on the one hand, while maintaining efficient subnet proving requires only a single tx per F epoch on the other hand (which can be thought of as minimal tax paid by any reasonable non-spam subnet). The addition of the global indices i_1, ..., i_k hashed within def. 1 allows proving cross-subnet DAG order relations. The series \\psi(B.sp_1), \\psi(B.sp_2), ..., \\psi(B.sp_{2^m}) embedded within \\psi(B) is set such that B.sp_{2^{m+1}} would be below the previous pruning/posterity point (see KIP-6 by @Deshe2). This construction can be used to prove arbitrary transaction acceptance in the L1 DAG throughout history with the same \u03b8(log(N)loglog(N)) complexity suggested in the KIP (technically this might require switching H to MR in def. 1). Note that we do this without going through the header-chain, thus not providing PoChM explicitly, nor the possibility to prove transaction inclusion. The mention of using SMR\u2019s @reshmem is not in relation to non-inclusion proofs but rather for efficiently managing a mutating key-value tree with log depth and with in-consensus semantics. The key here is the subnet id and \\psi_i(B) is the mutating value. Keeping a dense tree for all active (non-expired) subnets, would require mutations and tree rebalancing strategies which are hard to form in-consensus. Reinvestigating this, I guess we can also use a Patricia Tree (with another hashing layer over keys) which might provide better density tradeoffs.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-30T15:18:12.735Z",
      "updated_at": "2025-01-31T11:46:30.681Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/5",
      "category_id": null
    },
    {
      "post_id": 503,
      "post_number": 6,
      "topic_id": 274,
      "topic_title": "Subnets Sequencing Commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "michaelsutton: Subnets can prove their execution in O(\\text{subnet activity}) time as long as there\u2019s at least a single subnet tx every F epoch I noticed that expired subnets can prove inactivity by providing a non-inclusion proof every F epoch. I.e., by showing subnet i is not included in \\psi(B), you essentially prove there was no activity for the F epoch prior to B. So a subnet inactive for T time will only require O(T/F) proof steps.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-31T11:58:41.571Z",
      "updated_at": "2025-01-31T11:58:41.571Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/6",
      "category_id": null
    },
    {
      "post_id": 473,
      "post_number": 1,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork Discussion Thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "This is a discussion thread for KIP-14 \u2014 The 10-BPS Crescendo Hardfork: kips/kip-0014.md at master \u00b7 kaspanet/kips \u00b7 GitHub",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-21T21:11:53.611Z",
      "updated_at": "2025-01-22T19:04:34.518Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/1",
      "category_id": null
    },
    {
      "post_id": 506,
      "post_number": 2,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork Discussion Thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "RE The pruning depth formula provides a lower bound, yet the actual pruning period can be set longer. Plugging in the scaled parameters, the lower bound is calculated to be 627,258 blocks, representing approximately ~17.4238 hours. We suggest rounding this up to 30 hours for simplicity and practical application. A 30-hour period is closer to the current mainnet pruning period (~51 hours) and aligns closely with the value used and benchmarked throughout TN11 (~31 hours). I think setting it as some (integer) multiplication of finality depth (something like 3\u03d5) can simplify pruning point calculation, since the difference between pruning points is at least the finality depth.",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2025-02-03T16:41:51.158Z",
      "updated_at": "2025-02-03T16:41:51.158Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/2",
      "category_id": null
    },
    {
      "post_id": 507,
      "post_number": 3,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork Discussion Thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "36 hours will slightly increase the storage upper bound (from 190GB for transient storage to ~230GB), but it does seem reasonable.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-02-03T17:31:29.374Z",
      "updated_at": "2025-02-03T17:31:29.374Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/3",
      "category_id": null
    },
    {
      "post_id": 520,
      "post_number": 4,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork Discussion Thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "Dear community - I propose to consider including KIP-15 to the Crescendo HF. The change is tiny (literally few lines of code) but still IMHO very powerful.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-02-23T12:54:08.596Z",
      "updated_at": "2025-02-23T12:54:08.596Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/4",
      "category_id": null
    },
    {
      "post_id": 525,
      "post_number": 5,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork Discussion Thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "Any decision about KIP-15?",
      "raw_content": "",
      "author": "Jacek_Kozicki",
      "created_at": "2025-02-27T22:12:19.277Z",
      "updated_at": "2025-02-27T22:12:19.277Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/5",
      "category_id": null
    },
    {
      "post_id": 530,
      "post_number": 6,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork Discussion Thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "Being tested \u2026 Hope it will be ok",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-03-05T19:42:14.189Z",
      "updated_at": "2025-03-05T19:42:14.189Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/6",
      "category_id": null
    },
    {
      "post_id": 504,
      "post_number": 1,
      "topic_id": 293,
      "topic_title": "Thoughts About Covenant And Async Message Standardization",
      "topic_slug": "thoughts-about-covenant-and-async-message-standardization",
      "content": "The canonical bridge post essentially suggested a way to authenticate that a UTXO belongs to some \u201ccovenant chain\u201d. A natural continuation of this would be to standardize the idea of a covenant and to allow authenticating it with some identity, in a way which is authorized by L1. One use-case for such standardization would be async message passing between covenants/rollups where L1 will certify the sender identity. It could also simplify the implementation of a delegation script such as the one described in the post. A reoccurring theme of covenant scripts is that they maintain some static part of the script (e.g., CANON, PROG) throughout time, while other parts mutate and represent the covenant dynamic state. I propose that a covenant script will contain a script-header specifying a mask which will allow to extract the static part of the script. This can be done by specifying masking ranges (e.g., ignore bytes 22-45) or by inserting special markers within the script itself (the latter is useful if the dynamic parts are also dynamic in size). Another complementing operation would be hashing the header/markers along with the static parts they signify, such that any script following the covenant will hash to the same \u201ccovenant identifier\u201d. I can see two immediate use-cases for such a mechanism. Async message passing: a covenant transaction (proof transaction in the rollup context), can contain a payload marked with a special SYSTEM prefix indicating that L1 authorized this payload. The payload will include a sender field with the sender id, and the message itself. L1 will verify (as a transaction acceptance rule), that the sender id corresponds to the standard covenant identifier of one of the spent input scripts. This means that other rollups interested in messages from this sender will only need to follow such messages (as long as they performed a one-time check/audit verifying that the underlying covenant is well-formed and cannot be forged) Delegation scripts: the standardization essentially captures the notion of \u201cwell-formed\u201d defined in the linked post. The DELEGATE script can thus be simplified to verify that the primary input has the desired covenant identifier.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-31T12:53:42.823Z",
      "updated_at": "2025-01-31T13:52:33.664Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/thoughts-about-covenant-and-async-message-standardization/293/1",
      "category_id": null
    },
    {
      "post_id": 505,
      "post_number": 1,
      "topic_id": 294,
      "topic_title": "Sync Monitor Mining Rule Engine",
      "topic_slug": "sync-monitor-mining-rule-engine",
      "content": "Introduction This forum post is a summary of some of the discussions with @michaelsutton @hashdag and @someone235 which can be found at Sync Monitor / Mining Rule Engine - Google Docs and Telegram: Contact @kasparnd Here we describe two pieces of related work: Creating a sync monitor/manager such that nodes can continue mining during a network halt under certain good conditions. This monitor answers the question \u201cShould the node mine?\u201d Creating a mining rule engine that allows mining behavior to adjust to conditions of the node or the network. This rule engine answers the question \u201cHow should this node mine?\u201d Sync Monitor This monitor determines whether a node should mine given its observable current state. The rusty kaspa node allows for mining if (1) the node has at least one peer and if (2) the sink is recent. These conditions are sufficient for when the network is operating normally - that is, the network is producing blocks. What happens if network mining halts for a significant period of time to the point all nodes fail the \u201cis sink recent?\u201d condition? To recover from this halted state, there will have to be some manual intervention for mining to continue. We know the network is operating normally if it is producing blocks. We also know what the expected number of blocks there are per second (1 block per second in mainnet, but soon to become 10 BPS). Using these two bits of information, we can determine just how fast a node\u2019s sync rate is. Specifically: sync_rate = (observed_blocks_per_second) / (expected_blocks_per_second) We can formally define that the \u201cnetwork is operating normally\u201d if the sync_rate is above some threshold. If it falls below this threshold, then regardless of the recency of the sink, the node should mine bring the sync_rate back up. So, the proposed updated rule a sync monitor is as follows: The node is connected to at least one peer AND (The sink is recent OR the sync rate is below threshold) With this rule, we cover the scenario where the network halts. If you can think of more scenarios that should be covered, please feel free to suggest it below. Mining Rule Engine This rule engine determines how a given node should mine. It will be a new addition to the node so that it can attempt to recover from some scenarios, such as performance degradation, that are encountered through the course of a node\u2019s operation. As this is new functionality, I will only describe some of the ideas for scenarios we may care about and some possible recovery methods. Recovery Methods Consider pointing only to blue blocks Useful when header processing takes longer than some threshold (like 0.5sec) and there are constantly red blocks in the mergeset which is a hint that these red blocks might be the cause of the perf issue. Consider mining empty blocks (no transactions) Useful for mining blocks if something about the transactions is what is causing the error such as with the BadMerkleRoot errors during this freeze: Kaspa 20 minute BlockDAG freeze post-mortem | by Ori Newman | Medium. This post-mortem also mentions this recovery method towards the end. Mining Scenarios Scenario Mining Behavior Node is functioning normally Node will mine with the standard behavior (as it works today in mainnet) Node is in IBD and the sink timestamp is recent enough Node will mine with the standard behavior (as it works today in mainnet) Node is in IBD and the sink timestamp is not recent enough No mining is allowed Node is receiving multiple blocks that fail validation with BadMerkleRoot If the node continuously receives ONLY blocks that have BadMerkleRoot error, switch to mining only empty blocks (no transactions). Network mining has halted Considering the updated sync monitor logic above to track sync rate, this will be a completely unexpected scenario and will have to have manual intervention. Too many red blocks being merged causing performance degradation Allow a maximum of only 1 red block to be merged (or do not merge any red blocks) until node performance settles down. I encourage the reader to propose more scenarios that may need to be handled or recovery methods to handle such scenarios.",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2025-02-01T06:12:36.334Z",
      "updated_at": "2025-02-01T07:26:15.024Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/sync-monitor-mining-rule-engine/294/1",
      "category_id": null
    },
    {
      "post_id": 508,
      "post_number": 1,
      "topic_id": 295,
      "topic_title": "A Proposal Towards Elastic Throughput",
      "topic_slug": "a-proposal-towards-elastic-throughput",
      "content": "In the following post I\u2019m discussing transient storage as one type of resource that is capped per block. The proposed method can be applied to other resources as well, e.g., persistent_storage_mass, compute_mass. Prepaid Transient Storage: A Proposal Fullnodes on commodity hardware require strict transient storage limits. Currently, a 200GB cap per pruning epoch (~42 hours) is enforced. The enforcement mechanism uniformly distributes this cap across blocks, allocating an equal fraction of storage per block. This approach limits flexibility and prevents occasional large transactions from utilizing excess available storage, even when the global transient storage cap is not exceeded. Problem with the current enforcement mechanism The current mechanism enforces a fixed per-block limit: transient_storage_mass_epoch_cap / total_blocks_per_epoch This method prevents miners from accommodating natural fluctuations in transaction size. If some blocks use less than their allocated storage, the excess cannot be transferred to other blocks. As a result, transactions requiring more storage than a single block\u2019s allocation are not feasible, even when total transient storage remains within the global cap. Example usecase 1: supporting high peak txn demand (aka elastic throughput) If a miner expects high transaction demand within an epoch, such as during peak hours, or needs to guarantee block space for users who have prepaid for transaction approval, they can mine underutilized blocks at the beginning of an epoch. This reserves transient storage for future blocks within the same epoch, ensuring that miners can handle anticipated transaction surges efficiently and allocate block space as needed. (Implicitly, I\u2019m assuming here the method is applied to compute mass as well.) The idea to support elastic throughput came to me from Gregory Maxwell and Meni Rosenfeld\u2019s elastic block cap ideas (around the big block debates), https://bitcointalk.org/index.php?topic=1078521.msg11517847#msg11517847. The gap between max capacity and peak capacity is of greater importance in Bitcoin vs Kaspa, since the former (supposedly) employs no pruning. Practically though, Kaspa\u2019s pruning epoch of ~42 hours corresponds to more than 4 months\u2019 worth of data growth in Bitcoin. In short, the peak vs avg gap is relevant to Kaspa in-pruning-epoch as well. Example usecase 2: supporting native STARK rollups A zk rollup entity may seek to implement native STARK verification using arithmetic field operations. Unlike SNARKs, STARKs do not require a trusted setup and offer quantum resistance, making them especially attractive for zk rollup infra. However, STARK proofs and the verifier size are significantly larger than SNARK proofs and verification scripts, potentially exceeding a few hundreds of KBs. Under the current enforcement mechanism, such proofs may not fit within a single block, making STARK-based rollups cumbersome or requiring them to go through a SNARK reduction, which is a legit construction, though slightly compromises the trustless property (it\u2019s not too bad, since PLONK SNARK setup is universal updatable). Proposed Solution: prepaid transient storage To enable occasional publication of large blocks while maintaining the global transient storage cap, miners should be able to accumulate transient storage credits by underutilizing previous blocks. The credits here are used metaphorically, as a conceptual concept within the consensus/fullnode. When a miner produces a block B, the transient storage consumed is recorded as transient_storage_mass(B). If transient_storage_mass(B) < transient_storage_mass_cap, the difference is stored as credit. In a future block X, the miner may prove via digital signature that it is the miner of block B, and utilize: transient_storage_mass_cap + (transient_storage_mass_cap - transient_storage_mass(B)) Generalizing this across multiple previously mined blocks B_1, ..., B_n, the total allowable transient storage in block X is: C = (n+1) * transient_storage_mass_cap - \u03a3 transient_storage_mass(B_i) The full node then deducts the usage proportionally from the previously mined blocks: transient_storage_mass(B_i) -= C/n This mechanism enables miners to accumulate storage credits and later use them for transactions requiring more storage in a single block, ensuring better resource allocation while adhering to the global cap. Elastic throughput and DAGKNIGHT (DK) Recall that larger blocks propagate slower, which widens the DAG. Fortunately, the DK protocol can handle dynamic DAG widths, by readjusting the parameter k in real time. Even with DK, some hard cap on individual blocks\u2019 sizes must be applied, e.g., each block shouldn\u2019t exceed 2 MB. Solo miners and the prepaid approach One not accustomed to Kaspa\u2019s high block creation rate \u2013 upcoming 10 per second \u2013 might find this whole approach of prepaid block space awkward. However, with 10 bps and north, it is likely that the mining market will change and adjust. In particular, some service providers \u2013 e.g., wallets, rollup/prover teams \u2013 may find it profitable to either mine only or primarily their own users\u2019 txns or to engage in some agreement with existing miners. This gives rise to the notion that mined blocks \u2013 the economics of mining txns \u2013 will sometimes reflect the needs of specific entities and sectors, alongside ordinary generic mining nodes. (All of the latter rambling describes offchain economics; no entity will receive privileged treatment from consensus\u2019 POV.) Notes Emphasizing again that this approach can be applied to other resource constraints as well, such as persistent storage and compute mass limits. Though, it seems particularly relevant for transient storage. One caveat of this approach is that, by providing proof of mining of previous blocks, miners link their mined blocks, thereby reducing their anonymity. However, most miners seem to not actively conceal their identity, so this is unlikely to be a significant issue. Post pruning all blocks\u2019 credits must be zeroed, b/c the global transient storage cap is relevant for and enforced per pruning epochs (thank you @coderofstuff for this comment and general proofreading)",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-02-05T23:08:47.503Z",
      "updated_at": "2025-02-06T16:32:04.139Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/1",
      "category_id": null
    },
    {
      "post_id": 583,
      "post_number": 2,
      "topic_id": 295,
      "topic_title": "A Proposal Towards Elastic Throughput",
      "topic_slug": "a-proposal-towards-elastic-throughput",
      "content": "hashdag: The proposed method can be applied to other resources as well, e.g., persistent_storage_mass, compute_mass. Applicability to transient storage is unique in that the epoch with which it is relevant is clearly defined (the pruning period). After such an epoch, the transient storage credits are reset and at any time the expected maximum bound for storage use within the pruning period is maintained even if the transient storage limits are made elastic. However, given your claim above I\u2019m curious what your thoughts are for how this credit system this would be applied to persistent storage mass (which KIP9 applies to; and relates to UTXO storage and is boundless) and to compute mass (related to CPU usage, bounded to 100% cpu capacity but is also technically a resource that can\u2019t be \u201cstored for use later\u201d easily)",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2025-06-02T20:11:11.781Z",
      "updated_at": "2025-06-02T20:11:11.781Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/2",
      "category_id": null
    },
    {
      "post_id": 584,
      "post_number": 3,
      "topic_id": 295,
      "topic_title": "A Proposal Towards Elastic Throughput",
      "topic_slug": "a-proposal-towards-elastic-throughput",
      "content": "Re persistent_storage_mass, it seems no different than transient_storage_mass, in that we don\u2019t care about the peak usage just about its regulated growth in time, be it abrupt or gradual (ofc apart from the CPU cost of the storing operation). Applying credit to persistent_storage_mass would imply the credit is never expiring. Re cpu_mass, I was alluding to a credit that plays on the margin between peak and avg within the scope of an epoch, but better leave that complex optimization aside and admit that in the CPU context, as you said, we are interested in peak and not only aggregate consumption over time. BTW I\u2019lll mention here another type of mechanism, which I am not necessarily advocating but which we might draw inspiration from: There\u2019s an EVM feature where operations on zero-led addresses enjoy discounted gas costs. https://blockwithanand.hashnode.dev/the-impact-of-leading-zeros-in-ethereum-addresses-on-transaction-costs. At the time Eth teams used this hack to reduce their gas costs, by investing POW in creating zero-led addresses, in order to save on future costs. If this address-POW mechanism simplifies implementation, we can consider using it in order to regulate transient storage by allowing txn issuers eg provers that use such addresses to publish large txns/proofs that exceed the usual block size; the address or POW would need to be renewed every pruning period. The benefit would be avoiding the need to couple identity of miner and prover/txn issuer, and no need to compromise miner anonymity through block linkage. If you view this path as superior to credit, we can discuss details eg whether this POW needs adjustment.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-06-03T17:01:44.773Z",
      "updated_at": "2025-06-03T17:01:44.773Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/3",
      "category_id": null
    },
    {
      "post_id": 519,
      "post_number": 1,
      "topic_id": 303,
      "topic_title": "Kip 15 Discussion Thread",
      "topic_slug": "kip-15-discussion-thread",
      "content": "This is the discussion thread for KIP 15",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2025-02-23T09:07:49.125Z",
      "updated_at": "2025-02-23T09:07:49.125Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-15-discussion-thread/303/1",
      "category_id": null
    },
    {
      "post_id": 543,
      "post_number": 1,
      "topic_id": 323,
      "topic_title": "A Basic Framework For Proofs Stitching",
      "topic_slug": "a-basic-framework-for-proofs-stitching",
      "content": "Introduction In an atomic synchronous composability model, a single transaction may invoke execution in several logic zones (conceptually, smart contracts or rollups). A priori, synchronous composability ambitions appear to be at odds with ZK execution scaling\u2014if provers and verifiers are forced to ZK prove and verify such involved transactions as a whole, the separation to distinct and independent logic zones will effectively collapse to a single monolithic design. This post outlines a proof of concept for how provers of various logic zones can cumulatively create proofs for multi logic zone transactions, while each is only required to provide proofs limited to executions of code in their respective logic zone. This consists an initial but essential step to potentially prevent the aforementioned collapse. The main idea is stitching in a coherent manner several heterogenous zk proofs. In that regard, the framework bears some resemblance to works like legosnarks, but unlike those, merely uses Snark in a black box manner. Subtransactions In our setting, multiple logic zones execute within a single transaction, sequentially calling on one another, where the initial logic zone and its input arguments are determined by the transaction\u2019s contents. Each zone maintains its own internal state, isolated from others, which can neither read from nor write to it. The execution trace derived from the above can be broken into continuous segments I refer to as subtransactions. Each subtransaction represents a continuous block of execution in one logic zone and is executed in a strict sequence, with a call stack managing the flow between them. To account for atomicity, the call stack is extended with a special element \\bot, that when pushed to the stack denotes that the transaction as a whole has failed. A subtransaction can be categorized by three elements: Inputs: The internal state of the logic zone executed, at the outset of the subtransaction. The input call stack, with the executed logic zone and the current local variables (possibly including a program counter) stored as a pair at the stack\u2019s top. This call stack allows managing proper context switching between the subtransactions. Outputs: The updated internal state of the executed logic zone, after execution of the subtransaction. The output call stack, capturing any changes made to the call stack as a result of a call to a new logic zone (update of the top\u2019s local variables, and a push of a new pair to the stack consisting of the called logic zone, and the local variables initialized by the call\u2019s arguments) or a return from one (A pop of the current element of the stack, and an extension of the new top\u2019s local variables with a new variable denoting the return value). Ordinal Number: A subtransaction is assigned an index representing its position in the overall sequence of execution. This explicit ordering will be useful for efficiently stitching subtransactions together. Zero-Knowledge Proof Messages A proof message is constructed as a tuple (index, inputs, outputs, \\pi), conceptually representing a subtransaction\u2019s execution. Namely: index: ordinal number i of the subtransaction. inputs: The starting inputs, including the executed logic zone\u2019s internal state and the input call stack. outputs: The resulting outputs, including the updated internal state and the output call stack. \\pi: a zero-knowledge proof demonstrating that, given these inputs, a subtransaction with index i correctly produces the specified outputs. The logic zone responsible for executing the subtransaction, as well as the next logic zone in line, are implicitly identified within the call stack data. Proofs can be internally valid (i.e. \\pi verifies the subtransaction as stated correctly) even if they do not represent a computation that occurs at the transaction\u2019s proper execution trace. Proofs hence need be treated as conditional, and not finalized until they are \u201cstitched\u201d (see below) together with others up to the transaction\u2019s conclusion. Proving Conditional Proofs Provers each specialize in a designated logic zone and produce proofs for its corresponding subtransactions. To generate a proof of a certain subtransaction\u2019s execution, a prover needs to have the inputs of that subtransaction. These intermediary values may depend on the results of subtransactions in other logic zones. Provers could acquire the results of these intermediary computations, in one of two ways: Communicate intermediary results of subtransactions with each other via an offchain provers\u2019 network. Execute (but not prove) subtransactions of other logic zones in order to derive the intermediary results they require by themselves. Proofs Stitching All proof messages, once submitted and verified, are stored in a public database. They are assumed to be submitted in an unordered manner. To finalize a transaction, a sequence of proofs need to be sequentially stitched with each other, in a manner such that the outputs and inputs are consistent, from the initial subtransaction and the global state on all associated logic zones at its inception, until either the stack is empty, or alternatively, a \\bot has been pushed to it. An implementation for stitching is given below. class TransactionStitcher: FAILURE_SIGN = '\u22a5' class StitchedChain: def __init__(self, initial_call_stack, initial_states): \"\"\" :param initial_call_stack: The initial call stack object. It must include a field 'currentLogicZone' that identifies the active logic zone. :param initial_states: A dict mapping each logic zone to its internal state at transaction inception. \"\"\" self.proof_list = [] # List of proofs in forward order. self.latest_writes = dict(initial_states) # zone -> internal state (latest updates) self.current_call_stack = initial_call_stack # The current call stack. def __init__(self, initial_stack, initial_states): \"\"\" :param initial_stack: The initial call stack (an object with a field 'currentLogicZone'). :param initial_states: A dict mapping each logic zone to its initial internal state. \"\"\" # proofs_by_index maps an index to a dict: # key: (internalState, callStack) from the proof's inputs, value: the proof. self.proofs_by_index = {} self.stitched_chain = self.StitchedChain(initial_stack, initial_states) self.initial_states = dict(initial_states) # For failure case. # --- Main Method --- def submit_proof_message(self, proof): \"\"\" Processes a new publicly verified proof by storing it by index and attempting to extend the active forward-growing chain. Outputs the result of the transaction if it can be fully stitched together :param proof: A dictionary representing a proof with keys: - 'index': int, the subtransaction's ordinal index. - 'inputs': dict with keys: 'callStack': an object with a field 'currentLogicZone', 'internalState': the state of the active logic zone at the start. - 'outputs': dict with keys: 'callStack': an object (same structure as inputs['callStack']), 'internalState': the updated state after execution. - '\u03c0': the zero-knowledge proof data (not used in stitching logic). - 'callstack': an object representing the call stack; it must include a field 'currentLogicZone'. \"\"\" self.store_proof(proof) while True: candidate = self.lookup_candidate() if candidate: self.extend_chain_with_candidate(candidate) if self.is_candidate_ending(candidate): if self.is_failure(candidate['outputs']['callStack']): return self.initial_states # Failure: return initial state. else: return self.stitched_chain.latest_writes # Success: return latest writes. else: break return None # --- Helper Functions --- def is_empty(self, call_stack): \"\"\" Determine if the call stack is empty. (Implementation left out \u2013 should return True if call_stack represents an empty stack.) \"\"\" pass def is_failure(self, call_stack): \"\"\" Determine if the call stack signals failure (i.e. contains FAILURE_SIGN as the logic zone at the top). (Implementation left out.) \"\"\" pass def is_candidate_ending(self, proof): \"\"\" Returns True if the proof's output call stack indicates termination: either an empty call stack (success) or a failure signal. \"\"\" cs = proof['outputs']['callStack'] return self.is_empty(cs) or self.is_failure(cs) def get_candidate_key(self, proof): \"\"\" Returns a tuple (internalState, callStack) from the proof's inputs for dictionary lookup. \"\"\" return (proof['inputs']['internalState'], proof['inputs']['callStack']) def store_proof(self, proof): \"\"\" Stores the proof in the proofs_by_index dictionary. \"\"\" index = proof['index'] key = self.get_candidate_key(proof) self.proofs_by_index.setdefault(index, {})[key] = proof def lookup_candidate(self): \"\"\" Looks up and returns a candidate proof for the next index using the expected inputs, or returns None if no candidate exists. \"\"\" active_zone = self.stitched_chain.current_call_stack.currentLogicZone expected_state = self.stitched_chain.latest_writes.get(active_zone) expected_stack = self.stitched_chain.current_call_stack next_index = len(self.stitched_chain.proof_list) + 1 candidate_key = (expected_state, expected_stack) return self.proofs_by_index.get(next_index, {}).get(candidate_key, None) def extend_chain_with_candidate(self, candidate): \"\"\" Extends the stitched chain with the given candidate proof and updates the chain's state. \"\"\" self.stitched_chain.proof_list.append(candidate) self.stitched_chain.current_call_stack = candidate['outputs']['callStack'] active_zone = self.stitched_chain.current_call_stack.currentLogicZone self.stitched_chain.latest_writes[active_zone] = candidate['outputs']['internalState'] Final Notes The precise identity of TransactionStitcher in the ecosystem remains to be finalized. It could be invoked within each logic zone separately, at a single stitching specialized logic zone, or even delegated to the L1. Regardless, it must be aware of the standard used to encode data (inputs and outputs) in proof messages. Whole transactions could be proven ahead of time and stitched together in a similar manner. Transactions notably form a dependency DAG between themselves according to their associated logic zones. Two branches of this DAG could potentially advance themselves independently of the other. Proof messages of the same logic zones will likely be submitted by provers in batches (likely even including messages from several transactions). Parsimonious encoding of these batches, and correspondingly decoding their contents, warrants more detailed discussion. A more general setting could consider parallelizable calls within the transaction. Such intra-transaction parallelism opens up various questions on data races prevention, determinism, and read and write permissions, which at the moment do not appear justified from a practical perspective. Challenge for the reader: does a proof message truly must commit to the index of the subtransaction? does it need the state of the stack in its entirety?",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-04-02T12:08:08.794Z",
      "updated_at": "2025-04-21T12:39:50.272Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/a-basic-framework-for-proofs-stitching/323/1",
      "category_id": null
    },
    {
      "post_id": 566,
      "post_number": 1,
      "topic_id": 346,
      "topic_title": "Kip Draft Threshold Address Format For Kip 10 Mutual Transactions",
      "topic_slug": "kip-draft-threshold-address-format-for-kip-10-mutual-transactions",
      "content": "Introduction This proposal introduces a new address type for Kaspa that leverages a compact floating-point representation for threshold values. The Float24 format enables efficient encoding of threshold values within P2SH scripts, borrowing functionality while maintaining compatibility with existing address infrastructure. Motivation Current address formats don\u2019t efficiently support threshold-based spending conditions as described in KIP-10. By encoding threshold values directly in addresses, we can: Support borrowing functionality where UTXOs can only be spent by increasing their value Provide a compact representation for a wide range of threshold values Maintain compatibility with existing P2SH infrastructure Specification Address Version We propose a new address version for threshold-based scripts: Version::ThresholdScript = 9 Address Structure A threshold address consists of: Network prefix (e.g., \u201ckaspa\u201d, \u201ckaspatest\u201d) Version byte (9 for ThresholdScript) Address payload (35 bytes for ECDSA, 35 bytes for Schnorr) Address Payload Format The address payload consists of: Public key X coordinate (32 bytes) Float24 threshold value (3 bytes) The Float24 format uses 3 bytes (24 bits) with the following structure: [1 bit: signature type] [1 bit: Y coordinate parity (ECDSA) or reserved (Schnorr)] [17 bits: mantissa] [5 bits: exponent] Where: Signature type bit: 0 for ECDSA, 1 for Schnorr Y coordinate parity bit: For ECDSA: 0 for even Y, 1 for odd Y For Schnorr: Always 0 (reserved for future use) Mantissa: 17-bit unsigned integer (0-131,071) Exponent: 5-bit unsigned integer (0-31) The threshold value is calculated as: threshold = mantissa * (2^exponent) Range of Representable Values With this format, we can represent: Minimum value: 0 * 2^0 = 0 sompi (allowing zero-threshold conditions) Maximum value: 131,071 * 2^31 \u2248 281.4 trillion sompi This range easily covers most practical threshold values, though it falls short of the maximum possible Kaspa supply of 29 billion KAS (2.9 * 10^18 sompi). Script Template The address automatically generates a P2SH script with the following pattern: For non-zero thresholds: OP_IF <pubkey> OP_CHECKSIG[_ECDSA] OP_ELSE OP_TXINPUTINDEX OP_TXINPUTSPK OP_TXINPUTINDEX OP_TXOUTPUTSPK OP_EQUALVERIFY OP_TXINPUTINDEX OP_TXOUTPUTAMOUNT <threshold_value> OP_SUB OP_TXINPUTINDEX OP_TXINPUTAMOUNT OP_GREATERTHANOREQUAL OP_ENDIF Examples Example 1: Zero Threshold with Schnorr Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes) Signature type: Schnorr (1) Y coordinate bit: 0 (reserved for Schnorr) Threshold: 0 sompi Float24: [1][0][00000000000000000][00000] (mantissa=0, exponent=0) Example 2: Small Threshold (1,024 sompi) with Schnorr Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes) Signature type: Schnorr (1) Y coordinate bit: 0 (reserved for Schnorr) Threshold: 1,024 sompi Float24: [1][0][00000000000000001][00010] (mantissa=1, exponent=10) Example 3: Medium Threshold (100,000 sompi) with ECDSA (Even Y) Public key X coordinate: ba01fc5f4e9d9879599c69a3dafdb835a7255e5f2e934e9322ecd3af190ab0f6 (32 bytes) Signature type: ECDSA (0) Y coordinate bit: 0 (even Y) Threshold: 100,000 sompi Float24: [0][0][00000000001100100][00000] (mantissa=100000, exponent=0) Example 4: Medium Threshold (5,242,880 sompi) with ECDSA (Odd Y) Public key X coordinate: ba01fc5f4e9d9879599c69a3dafdb835a7255e5f2e934e9322ecd3af190ab0f6 (32 bytes) Signature type: ECDSA (0) Y coordinate bit: 1 (odd Y) Threshold: 5,242,880 sompi Float24: [0][1][00000000000000101][00020] (mantissa=5, exponent=20) Example 5: Large Threshold (~1 million KAS) with Schnorr Public key X coordinate: Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes) Signature type: Schnorr (1) Y coordinate bit: 0 (reserved for Schnorr) Threshold: 99,999,547,392 sompi (~999,995 KAS) Float24: [1][0][10111010010000111][10100] (mantissa=95367, exponent=20) Alternative Approaches Alternative 1: Adjusting Bit Allocation for Full Range Coverage One alternative approach would be to adjust the bit allocation to provide coverage for the full range of possible Kaspa values. By moving 1 bit from the mantissa to the exponent, we could use: [1 bit: signature type] [1 bit: Y coordinate parity] [16 bits: mantissa] [6 bits: exponent] With this adjustment: Mantissa: 16-bit unsigned integer (0-65,535) Exponent: 6-bit unsigned integer (0-63) The maximum representable value would be: 65,535 * 2^63 \u2248 6.04 * 10^23 sompi This would easily cover the maximum Kaspa supply of 2.9 * 10^18 sompi with significant headroom. The tradeoff would be slightly less precision in the mantissa (16 bits instead of 17 bits), but 65,535 distinct mantissa values should still provide sufficient granularity for practical threshold purposes. Alternative 2: Non-Power-of-2 Exponent Base Another alternative would be to use a non-power-of-2 base for the exponent, such as 10: threshold = mantissa * (10^exponent) This would make the values more human-readable and potentially allow for more intuitive threshold settings. However, this approach has significant drawbacks: Performance Penalties: Computing powers of 10 is more computationally expensive than powers of 2, which can be implemented as simple bit shifts. Precision Issues: Powers of 10 cannot be represented exactly in binary, potentially leading to rounding errors. For these reasons, the power-of-2 approach is preferred for the Float24 format. Benefits Practical Range: Float24 can represent values from 0 sompi to well beyond most practical threshold needs Zero Threshold Support: Enables simple same-address spending without value constraints Signature Type Choice: Allows selection between ECDSA and Schnorr signatures Complete Public Key Information: Preserves Y coordinate parity for ECDSA keys Implementation Considerations Bech32 Encoding: The complete 35-byte payload would be encoded using bech32 Script Generation: Wallet software would need to generate the appropriate script based on the Float24 value and signature type Validation: Address validation would need to check the Float24 format is valid Compatibility: Existing software would need updates to recognize and handle the new address type Summary The Float24 Threshold Address format provides an efficient way to encode threshold values directly in Kaspa addresses, enabling powerful compounding and borrowing functionality without requiring consensus changes. This proposal builds on the transaction introspection capabilities introduced in KIP-10 while maintaining compatibility with existing P2SH infrastructure. By standardizing this address format, we can enable a new class of applications that leverage threshold-based spending conditions, particularly for mining pools and other services that benefit from auto-compounding UTXOs. The support for both ECDSA and Schnorr signatures, along with a wide range of threshold values, provides flexibility for various financial applications.",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2025-05-22T08:23:49.222Z",
      "updated_at": "2025-05-22T09:25:10.878Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-draft-threshold-address-format-for-kip-10-mutual-transactions/346/1",
      "category_id": null
    },
    {
      "post_id": 567,
      "post_number": 2,
      "topic_id": 346,
      "topic_title": "Kip Draft Threshold Address Format For Kip 10 Mutual Transactions",
      "topic_slug": "kip-draft-threshold-address-format-for-kip-10-mutual-transactions",
      "content": "It\u2019s important to note that the conversion from an address to a script public key (SPK) is a one-way process. Once an address is converted to an SPK, the original script cannot be recovered from the SPK hash alone unless all placeholder values are known.",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2025-05-22T08:39:04.245Z",
      "updated_at": "2025-05-22T08:39:04.245Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-draft-threshold-address-format-for-kip-10-mutual-transactions/346/2",
      "category_id": null
    },
    {
      "post_id": 568,
      "post_number": 1,
      "topic_id": 347,
      "topic_title": "On The Inherent Tension Between Multileader Consensus And Inclusion Time Proving",
      "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
      "content": "Parallelized L1s (e.g., Kaspa\u2019s 10bps block-DAG, advanced multileader designs) inherently offer sub-RTT block time and strong intra-round censorship resistance. A key byproduct of their parallelism is execution uncertainty at inclusion time: transactions are included prior to final global ordering and execution. This uncertainty is not a flaw but an enabler for features such as MEV-resistance strategies which operate by obscuring sequence predictability from block composers. At the same time, for universal synchronous composability across multiple based ZK rollups (often conceived as distinct logic zones, each managing independent state), inclusion-time proving represents a near-ideal: Achieving atomic cross-zone operations for composable txns necessitates complex off-chain coordination\u2014related to our framework for proof stitching\u2014before these operations culminate in L1 settlement. Inclusion-time proving would offer immediate, verifiable L1 commitment for the state transitions resulting from these coordinated efforts. The inherent counter-duality The conflict between inclusion-time proving and execution uncertainty is direct: Inclusion-time proving mandates a known, unambiguous pre-state for proof generation at the moment of L1 inclusion. Execution uncertainty (resulting from multileader, and conducive to MEV-resistance) implies an indefinite pre-state at L1 inclusion, dependent on eventual sequencing of concurrently processed, potentially contending transactions. This basic conflict presents a choice. We opt for multileader consensus, embracing its natural execution uncertainty. Consequently, true inclusion-time proving for L1-visible L2 effects (like state commitments) cannot be achieved. Proofs for such effects must therefore be deferred, appearing on L1 only after parallel processing converges and transaction order is sufficiently established to define a clear state. Proof availability requirement This inherent gap introduces a critical challenge: L1 has already accepted the transaction data (achieved DA for it) and the system is, in a sense, committed to its potential effects. What if the required proof never materializes? This immediately necessitates a robust proof DA mechanism\u2014the eventual availability of the proof itself must be guaranteed or its absence handled gracefully. Furthermore, in an ecosystem of autonomous Logic Zones (LGs)\u2014where each LG is responsible for generating proofs for its own operational segments but cannot compel others\u2014atomic cross-LG operations create interdependencies. The successful L1 settlement of such a composite transaction thus becomes reliant on the timely L1 submission and verification of valid proofs from all participating LGs. This critical interdependency, particularly given the autonomy of each LG, naturally leads to an operational model we term \u201cTimebound proof settlement\u201d. Timebound proof settlement Under this model, transaction data first achieves L1 DA. Ultimate L1 settlement of its cross-domain effects, however, is explicitly dependent on subsequent L1 verification of ZK proofs, which must be submitted within a defined time window, T, post-L1 sequencing. Confirmation of an L2 operation\u2019s L1 impact is thus its proof-verified settlement within T; failure by any party in a multi-segment operation to provide its proof within this bound means that segment (and potentially the entire atomic operation) fails to settle, with penalties ensuring accountability. A key implication of timebound proof settlement is the viability of fast, user-side optimistic confirmation well before L1 proof settlement. Unlike inclusion-time proving where prover censorship directly blocks L1 inclusion, here L1 DA of a transaction already binds it to a based rollup\u2019s L1-registered program. Any designated prover failing to subsequently prove such an L1-committed transaction compromises their rollup\u2019s liveness. This incentive structure extends to multi-rollup atomic operations: users running composite execution nodes can optimistically confirm transactions, relying on each participating rollup\u2019s self-interest in maintaining its own liveness by submitting its proof segment. While such \u201cfat node\u201d optimistic confirmation offers immediate feedback, the underlying L1 settlement latency itself\u2014determined by L1 sequencing plus the cumulative L2 proving times\u2014remains crucial. Importantly, as ZK proving technology continues its rapid advance towards near real-time performance (where real-time << 12 seconds\u2026), this L1 settlement latency under timebound proof settlement is poised to significantly decrease, enhancing the model\u2019s practicality. This timebound proof settlement approach contrasts with embedding full witness DA within L1 transaction payloads, which, while ensuring eventual provability, imposes substantial and constant DA overhead. The architectural path an L1 takes will profoundly shape its multileader/MEV characteristics and the efficiency of its rollup ecosystem\u2019s composability. Future L1 designs might explore tiered DA/execution models, offering distinct contexts for \u201cuncertain inclusion\u201d and \u201ccertain inclusion\u201d (perhaps with different fee structures or trust assumptions). Ultimately, while timebound proof settlement offers a pragmatic path, novel cryptographic approaches (e.g., proofs over partially indeterminate states) could eventually reshape these trade-offs.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-05-22T19:49:02.766Z",
      "updated_at": "2025-05-22T21:13:01.069Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/1",
      "category_id": null
    },
    {
      "post_id": 569,
      "post_number": 2,
      "topic_id": 347,
      "topic_title": "On The Inherent Tension Between Multileader Consensus And Inclusion Time Proving",
      "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
      "content": "Would it be possible to have a common default deadline (e.g. 5 min), but allow rollups to explicitly set a longer one when needed \u2013 for async interop or complex composition cases? In the interop layer, we\u2019d isolate state impact, so if a cross-rollup proof fails, only the affected zone is rejected, and the rest of the rollup proceeds unaffected.",
      "raw_content": "",
      "author": "Pavel_Emdin",
      "created_at": "2025-05-23T08:55:05.346Z",
      "updated_at": "2025-05-23T08:55:05.346Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/2",
      "category_id": null
    },
    {
      "post_id": 577,
      "post_number": 3,
      "topic_id": 347,
      "topic_title": "On The Inherent Tension Between Multileader Consensus And Inclusion Time Proving",
      "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
      "content": "Async interop should generally be discouraged, but the issue there is in my opinion similar only in cosmetics - it would ressemble more a locking of outboxes and such, and of the top of my head as long as all parties know what they are into, it has no effect on others in the system and they can set their rules as they see fit. The difficulty with sync interop is that until the cross tx is settled all other tx of these logic zones cannot settle. for complex composition cases - I\u2019d kind of say the opposite. The less the probability a tx is proven eventually, the less leeway I think it should get in terms of proving time. Allowing people to create complex transactions that could potentially fail to settle is an attack vector. Generally speaking, different logic zones can have different timeouts, or even different tx of the same logic zones can have different timeouts, but it\u2019s still imperative that these be globally known - everyone involved directly or indirectly must agree on whether a tx succeeded or failed. The world I personally dream of is a permissionless world where anyone can participate and open a new zkapp (possibly constrained to standard code infras to maintain security), but their ability to interact with other zkapps is automatically in correlation to their credibility in supplying proofs. i.e. if your zkapp was unable to provide a proof for its part in a cross tx, you would get less and less leeway (and possibly higher fees) every time it happens henceforth - and the opposite.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-05-27T11:08:29.570Z",
      "updated_at": "2025-05-27T11:10:19.251Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/3",
      "category_id": null
    }
  ],
  "metadata": {
    "forums_processed": 1,
    "total_posts_fetched": 65,
    "credential_status": "configured",
    "processing_mode": "full_history"
  }
}