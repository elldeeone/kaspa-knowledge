{
  "date": "2025-07-09",
  "generated_at": "2025-07-09T01:04:41.556553+00:00",
  "source": "discourse_forum",
  "status": "success",
  "forum_posts": [
    {
      "post_id": 508,
      "post_number": 1,
      "topic_id": 295,
      "topic_title": "A proposal towards elastic throughput",
      "topic_slug": "a-proposal-towards-elastic-throughput",
      "content": "In the following post I\u2019m discussing transient storage as one type of resource that is capped per block. The proposed method can be applied to other resources as well, e.g., persistent_storage_mass, compute_mass. Prepaid Transient Storage: A Proposal Fullnodes on commodity hardware require strict transient storage limits. Currently, a 200GB cap per pruning epoch (~42 hours) is enforced. The enforcement mechanism uniformly distributes this cap across blocks, allocating an equal fraction of storage per block. This approach limits flexibility and prevents occasional large transactions from utilizing excess available storage, even when the global transient storage cap is not exceeded. Problem with the current enforcement mechanism The current mechanism enforces a fixed per-block limit: transient_storage_mass_epoch_cap / total_blocks_per_epoch This method prevents miners from accommodating natural fluctuations in transaction size. If some blocks use less than their allocated storage, the excess cannot be transferred to other blocks. As a result, transactions requiring more storage than a single block\u2019s allocation are not feasible, even when total transient storage remains within the global cap. Example usecase 1: supporting high peak txn demand (aka elastic throughput) If a miner expects high transaction demand within an epoch, such as during peak hours, or needs to guarantee block space for users who have prepaid for transaction approval, they can mine underutilized blocks at the beginning of an epoch. This reserves transient storage for future blocks within the same epoch, ensuring that miners can handle anticipated transaction surges efficiently and allocate block space as needed. (Implicitly, I\u2019m assuming here the method is applied to compute mass as well.) The idea to support elastic throughput came to me from Gregory Maxwell and Meni Rosenfeld\u2019s elastic block cap ideas (around the big block debates), https://bitcointalk.org/index.php?topic=1078521.msg11517847#msg11517847. The gap between max capacity and peak capacity is of greater importance in Bitcoin vs Kaspa, since the former (supposedly) employs no pruning. Practically though, Kaspa\u2019s pruning epoch of ~42 hours corresponds to more than 4 months\u2019 worth of data growth in Bitcoin. In short, the peak vs avg gap is relevant to Kaspa in-pruning-epoch as well. Example usecase 2: supporting native STARK rollups A zk rollup entity may seek to implement native STARK verification using arithmetic field operations. Unlike SNARKs, STARKs do not require a trusted setup and offer quantum resistance, making them especially attractive for zk rollup infra. However, STARK proofs and the verifier size are significantly larger than SNARK proofs and verification scripts, potentially exceeding a few hundreds of KBs. Under the current enforcement mechanism, such proofs may not fit within a single block, making STARK-based rollups cumbersome or requiring them to go through a SNARK reduction, which is a legit construction, though slightly compromises the trustless property (it\u2019s not too bad, since PLONK SNARK setup is universal updatable). Proposed Solution: prepaid transient storage To enable occasional publication of large blocks while maintaining the global transient storage cap, miners should be able to accumulate transient storage credits by underutilizing previous blocks. The credits here are used metaphorically, as a conceptual concept within the consensus/fullnode. When a miner produces a block B, the transient storage consumed is recorded as transient_storage_mass(B). If transient_storage_mass(B) < transient_storage_mass_cap, the difference is stored as credit. In a future block X, the miner may prove via digital signature that it is the miner of block B, and utilize: transient_storage_mass_cap + (transient_storage_mass_cap - transient_storage_mass(B)) Generalizing this across multiple previously mined blocks B_1, ..., B_n, the total allowable transient storage in block X is: C = (n+1) * transient_storage_mass_cap - \u03a3 transient_storage_mass(B_i) The full node then deducts the usage proportionally from the previously mined blocks: transient_storage_mass(B_i) -= C/n This mechanism enables miners to accumulate storage credits and later use them for transactions requiring more storage in a single block, ensuring better resource allocation while adhering to the global cap. Elastic throughput and DAGKNIGHT (DK) Recall that larger blocks propagate slower, which widens the DAG. Fortunately, the DK protocol can handle dynamic DAG widths, by readjusting the parameter k in real time. Even with DK, some hard cap on individual blocks\u2019 sizes must be applied, e.g., each block shouldn\u2019t exceed 2 MB. Solo miners and the prepaid approach One not accustomed to Kaspa\u2019s high block creation rate \u2013 upcoming 10 per second \u2013 might find this whole approach of prepaid block space awkward. However, with 10 bps and north, it is likely that the mining market will change and adjust. In particular, some service providers \u2013 e.g., wallets, rollup/prover teams \u2013 may find it profitable to either mine only or primarily their own users\u2019 txns or to engage in some agreement with existing miners. This gives rise to the notion that mined blocks \u2013 the economics of mining txns \u2013 will sometimes reflect the needs of specific entities and sectors, alongside ordinary generic mining nodes. (All of the latter rambling describes offchain economics; no entity will receive privileged treatment from consensus\u2019 POV.) Notes Emphasizing again that this approach can be applied to other resource constraints as well, such as persistent storage and compute mass limits. Though, it seems particularly relevant for transient storage. One caveat of this approach is that, by providing proof of mining of previous blocks, miners link their mined blocks, thereby reducing their anonymity. However, most miners seem to not actively conceal their identity, so this is unlikely to be a significant issue. Post pruning all blocks\u2019 credits must be zeroed, b/c the global transient storage cap is relevant for and enforced per pruning epochs (thank you @coderofstuff for this comment and general proofreading)",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-02-05T23:08:47.503Z",
      "updated_at": "2025-02-06T16:32:04.139Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/1",
      "category_id": 1
    },
    {
      "post_id": 583,
      "post_number": 2,
      "topic_id": 295,
      "topic_title": "A proposal towards elastic throughput",
      "topic_slug": "a-proposal-towards-elastic-throughput",
      "content": "hashdag: The proposed method can be applied to other resources as well, e.g., persistent_storage_mass, compute_mass. Applicability to transient storage is unique in that the epoch with which it is relevant is clearly defined (the pruning period). After such an epoch, the transient storage credits are reset and at any time the expected maximum bound for storage use within the pruning period is maintained even if the transient storage limits are made elastic. However, given your claim above I\u2019m curious what your thoughts are for how this credit system this would be applied to persistent storage mass (which KIP9 applies to; and relates to UTXO storage and is boundless) and to compute mass (related to CPU usage, bounded to 100% cpu capacity but is also technically a resource that can\u2019t be \u201cstored for use later\u201d easily)",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2025-06-02T20:11:11.781Z",
      "updated_at": "2025-06-02T20:11:11.781Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/2",
      "category_id": 1
    },
    {
      "post_id": 584,
      "post_number": 3,
      "topic_id": 295,
      "topic_title": "A proposal towards elastic throughput",
      "topic_slug": "a-proposal-towards-elastic-throughput",
      "content": "Re persistent_storage_mass, it seems no different than transient_storage_mass, in that we don\u2019t care about the peak usage just about its regulated growth in time, be it abrupt or gradual (ofc apart from the CPU cost of the storing operation). Applying credit to persistent_storage_mass would imply the credit is never expiring. Re cpu_mass, I was alluding to a credit that plays on the margin between peak and avg within the scope of an epoch, but better leave that complex optimization aside and admit that in the CPU context, as you said, we are interested in peak and not only aggregate consumption over time. BTW I\u2019lll mention here another type of mechanism, which I am not necessarily advocating but which we might draw inspiration from: There\u2019s an EVM feature where operations on zero-led addresses enjoy discounted gas costs. https://blockwithanand.hashnode.dev/the-impact-of-leading-zeros-in-ethereum-addresses-on-transaction-costs. At the time Eth teams used this hack to reduce their gas costs, by investing POW in creating zero-led addresses, in order to save on future costs. If this address-POW mechanism simplifies implementation, we can consider using it in order to regulate transient storage by allowing txn issuers eg provers that use such addresses to publish large txns/proofs that exceed the usual block size; the address or POW would need to be renewed every pruning period. The benefit would be avoiding the need to couple identity of miner and prover/txn issuer, and no need to compromise miner anonymity through block linkage. If you view this path as superior to credit, we can discuss details eg whether this POW needs adjustment.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-06-03T17:01:44.773Z",
      "updated_at": "2025-06-03T17:01:44.773Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/a-proposal-towards-elastic-throughput/295/3",
      "category_id": 1
    },
    {
      "post_id": 568,
      "post_number": 1,
      "topic_id": 347,
      "topic_title": "On the inherent tension between multileader consensus and inclusion-time proving",
      "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
      "content": "Parallelized L1s (e.g., Kaspa\u2019s 10bps block-DAG, advanced multileader designs) inherently offer sub-RTT block time and strong intra-round censorship resistance. A key byproduct of their parallelism is execution uncertainty at inclusion time: transactions are included prior to final global ordering and execution. This uncertainty is not a flaw but an enabler for features such as MEV-resistance strategies which operate by obscuring sequence predictability from block composers. At the same time, for universal synchronous composability across multiple based ZK rollups (often conceived as distinct logic zones, each managing independent state), inclusion-time proving represents a near-ideal: Achieving atomic cross-zone operations for composable txns necessitates complex off-chain coordination\u2014related to our framework for proof stitching\u2014before these operations culminate in L1 settlement. Inclusion-time proving would offer immediate, verifiable L1 commitment for the state transitions resulting from these coordinated efforts. The inherent counter-duality The conflict between inclusion-time proving and execution uncertainty is direct: Inclusion-time proving mandates a known, unambiguous pre-state for proof generation at the moment of L1 inclusion. Execution uncertainty (resulting from multileader, and conducive to MEV-resistance) implies an indefinite pre-state at L1 inclusion, dependent on eventual sequencing of concurrently processed, potentially contending transactions. This basic conflict presents a choice. We opt for multileader consensus, embracing its natural execution uncertainty. Consequently, true inclusion-time proving for L1-visible L2 effects (like state commitments) cannot be achieved. Proofs for such effects must therefore be deferred, appearing on L1 only after parallel processing converges and transaction order is sufficiently established to define a clear state. Proof availability requirement This inherent gap introduces a critical challenge: L1 has already accepted the transaction data (achieved DA for it) and the system is, in a sense, committed to its potential effects. What if the required proof never materializes? This immediately necessitates a robust proof DA mechanism\u2014the eventual availability of the proof itself must be guaranteed or its absence handled gracefully. Furthermore, in an ecosystem of autonomous Logic Zones (LGs)\u2014where each LG is responsible for generating proofs for its own operational segments but cannot compel others\u2014atomic cross-LG operations create interdependencies. The successful L1 settlement of such a composite transaction thus becomes reliant on the timely L1 submission and verification of valid proofs from all participating LGs. This critical interdependency, particularly given the autonomy of each LG, naturally leads to an operational model we term \u201cTimebound proof settlement\u201d. Timebound proof settlement Under this model, transaction data first achieves L1 DA. Ultimate L1 settlement of its cross-domain effects, however, is explicitly dependent on subsequent L1 verification of ZK proofs, which must be submitted within a defined time window, T, post-L1 sequencing. Confirmation of an L2 operation\u2019s L1 impact is thus its proof-verified settlement within T; failure by any party in a multi-segment operation to provide its proof within this bound means that segment (and potentially the entire atomic operation) fails to settle, with penalties ensuring accountability. A key implication of timebound proof settlement is the viability of fast, user-side optimistic confirmation well before L1 proof settlement. Unlike inclusion-time proving where prover censorship directly blocks L1 inclusion, here L1 DA of a transaction already binds it to a based rollup\u2019s L1-registered program. Any designated prover failing to subsequently prove such an L1-committed transaction compromises their rollup\u2019s liveness. This incentive structure extends to multi-rollup atomic operations: users running composite execution nodes can optimistically confirm transactions, relying on each participating rollup\u2019s self-interest in maintaining its own liveness by submitting its proof segment. While such \u201cfat node\u201d optimistic confirmation offers immediate feedback, the underlying L1 settlement latency itself\u2014determined by L1 sequencing plus the cumulative L2 proving times\u2014remains crucial. Importantly, as ZK proving technology continues its rapid advance towards near real-time performance (where real-time << 12 seconds\u2026), this L1 settlement latency under timebound proof settlement is poised to significantly decrease, enhancing the model\u2019s practicality. This timebound proof settlement approach contrasts with embedding full witness DA within L1 transaction payloads, which, while ensuring eventual provability, imposes substantial and constant DA overhead. The architectural path an L1 takes will profoundly shape its multileader/MEV characteristics and the efficiency of its rollup ecosystem\u2019s composability. Future L1 designs might explore tiered DA/execution models, offering distinct contexts for \u201cuncertain inclusion\u201d and \u201ccertain inclusion\u201d (perhaps with different fee structures or trust assumptions). Ultimately, while timebound proof settlement offers a pragmatic path, novel cryptographic approaches (e.g., proofs over partially indeterminate states) could eventually reshape these trade-offs.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-05-22T19:49:02.766Z",
      "updated_at": "2025-05-22T21:13:01.069Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/1",
      "category_id": 11
    },
    {
      "post_id": 569,
      "post_number": 2,
      "topic_id": 347,
      "topic_title": "On the inherent tension between multileader consensus and inclusion-time proving",
      "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
      "content": "Would it be possible to have a common default deadline (e.g. 5 min), but allow rollups to explicitly set a longer one when needed \u2013 for async interop or complex composition cases? In the interop layer, we\u2019d isolate state impact, so if a cross-rollup proof fails, only the affected zone is rejected, and the rest of the rollup proceeds unaffected.",
      "raw_content": "",
      "author": "Pavel_Emdin",
      "created_at": "2025-05-23T08:55:05.346Z",
      "updated_at": "2025-05-23T08:55:05.346Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/2",
      "category_id": 11
    },
    {
      "post_id": 577,
      "post_number": 3,
      "topic_id": 347,
      "topic_title": "On the inherent tension between multileader consensus and inclusion-time proving",
      "topic_slug": "on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving",
      "content": "Async interop should generally be discouraged, but the issue there is in my opinion similar only in cosmetics - it would ressemble more a locking of outboxes and such, and of the top of my head as long as all parties know what they are into, it has no effect on others in the system and they can set their rules as they see fit. The difficulty with sync interop is that until the cross tx is settled all other tx of these logic zones cannot settle. for complex composition cases - I\u2019d kind of say the opposite. The less the probability a tx is proven eventually, the less leeway I think it should get in terms of proving time. Allowing people to create complex transactions that could potentially fail to settle is an attack vector. Generally speaking, different logic zones can have different timeouts, or even different tx of the same logic zones can have different timeouts, but it\u2019s still imperative that these be globally known - everyone involved directly or indirectly must agree on whether a tx succeeded or failed. The world I personally dream of is a permissionless world where anyone can participate and open a new zkapp (possibly constrained to standard code infras to maintain security), but their ability to interact with other zkapps is automatically in correlation to their credibility in supplying proofs. i.e. if your zkapp was unable to provide a proof for its part in a cross tx, you would get less and less leeway (and possibly higher fees) every time it happens henceforth - and the opposite.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-05-27T11:08:29.570Z",
      "updated_at": "2025-05-27T11:10:19.251Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-inherent-tension-between-multileader-consensus-and-inclusion-time-proving/347/3",
      "category_id": 11
    },
    {
      "post_id": 566,
      "post_number": 1,
      "topic_id": 346,
      "topic_title": "KIP Draft: Threshold Address Format for KIP-10 Mutual Transactions",
      "topic_slug": "kip-draft-threshold-address-format-for-kip-10-mutual-transactions",
      "content": "Introduction This proposal introduces a new address type for Kaspa that leverages a compact floating-point representation for threshold values. The Float24 format enables efficient encoding of threshold values within P2SH scripts, borrowing functionality while maintaining compatibility with existing address infrastructure. Motivation Current address formats don\u2019t efficiently support threshold-based spending conditions as described in KIP-10. By encoding threshold values directly in addresses, we can: Support borrowing functionality where UTXOs can only be spent by increasing their value Provide a compact representation for a wide range of threshold values Maintain compatibility with existing P2SH infrastructure Specification Address Version We propose a new address version for threshold-based scripts: Version::ThresholdScript = 9 Address Structure A threshold address consists of: Network prefix (e.g., \u201ckaspa\u201d, \u201ckaspatest\u201d) Version byte (9 for ThresholdScript) Address payload (35 bytes for ECDSA, 35 bytes for Schnorr) Address Payload Format The address payload consists of: Public key X coordinate (32 bytes) Float24 threshold value (3 bytes) The Float24 format uses 3 bytes (24 bits) with the following structure: [1 bit: signature type] [1 bit: Y coordinate parity (ECDSA) or reserved (Schnorr)] [17 bits: mantissa] [5 bits: exponent] Where: Signature type bit: 0 for ECDSA, 1 for Schnorr Y coordinate parity bit: For ECDSA: 0 for even Y, 1 for odd Y For Schnorr: Always 0 (reserved for future use) Mantissa: 17-bit unsigned integer (0-131,071) Exponent: 5-bit unsigned integer (0-31) The threshold value is calculated as: threshold = mantissa * (2^exponent) Range of Representable Values With this format, we can represent: Minimum value: 0 * 2^0 = 0 sompi (allowing zero-threshold conditions) Maximum value: 131,071 * 2^31 \u2248 281.4 trillion sompi This range easily covers most practical threshold values, though it falls short of the maximum possible Kaspa supply of 29 billion KAS (2.9 * 10^18 sompi). Script Template The address automatically generates a P2SH script with the following pattern: For non-zero thresholds: OP_IF <pubkey> OP_CHECKSIG[_ECDSA] OP_ELSE OP_TXINPUTINDEX OP_TXINPUTSPK OP_TXINPUTINDEX OP_TXOUTPUTSPK OP_EQUALVERIFY OP_TXINPUTINDEX OP_TXOUTPUTAMOUNT <threshold_value> OP_SUB OP_TXINPUTINDEX OP_TXINPUTAMOUNT OP_GREATERTHANOREQUAL OP_ENDIF Examples Example 1: Zero Threshold with Schnorr Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes) Signature type: Schnorr (1) Y coordinate bit: 0 (reserved for Schnorr) Threshold: 0 sompi Float24: [1][0][00000000000000000][00000] (mantissa=0, exponent=0) Example 2: Small Threshold (1,024 sompi) with Schnorr Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes) Signature type: Schnorr (1) Y coordinate bit: 0 (reserved for Schnorr) Threshold: 1,024 sompi Float24: [1][0][00000000000000001][00010] (mantissa=1, exponent=10) Example 3: Medium Threshold (100,000 sompi) with ECDSA (Even Y) Public key X coordinate: ba01fc5f4e9d9879599c69a3dafdb835a7255e5f2e934e9322ecd3af190ab0f6 (32 bytes) Signature type: ECDSA (0) Y coordinate bit: 0 (even Y) Threshold: 100,000 sompi Float24: [0][0][00000000001100100][00000] (mantissa=100000, exponent=0) Example 4: Medium Threshold (5,242,880 sompi) with ECDSA (Odd Y) Public key X coordinate: ba01fc5f4e9d9879599c69a3dafdb835a7255e5f2e934e9322ecd3af190ab0f6 (32 bytes) Signature type: ECDSA (0) Y coordinate bit: 1 (odd Y) Threshold: 5,242,880 sompi Float24: [0][1][00000000000000101][00020] (mantissa=5, exponent=20) Example 5: Large Threshold (~1 million KAS) with Schnorr Public key X coordinate: Public key X coordinate: 5fff3c4da18f45adcdd499e44611e9fff148ba69db3c4ea2ddd955fc46a59522 (32 bytes) Signature type: Schnorr (1) Y coordinate bit: 0 (reserved for Schnorr) Threshold: 99,999,547,392 sompi (~999,995 KAS) Float24: [1][0][10111010010000111][10100] (mantissa=95367, exponent=20) Alternative Approaches Alternative 1: Adjusting Bit Allocation for Full Range Coverage One alternative approach would be to adjust the bit allocation to provide coverage for the full range of possible Kaspa values. By moving 1 bit from the mantissa to the exponent, we could use: [1 bit: signature type] [1 bit: Y coordinate parity] [16 bits: mantissa] [6 bits: exponent] With this adjustment: Mantissa: 16-bit unsigned integer (0-65,535) Exponent: 6-bit unsigned integer (0-63) The maximum representable value would be: 65,535 * 2^63 \u2248 6.04 * 10^23 sompi This would easily cover the maximum Kaspa supply of 2.9 * 10^18 sompi with significant headroom. The tradeoff would be slightly less precision in the mantissa (16 bits instead of 17 bits), but 65,535 distinct mantissa values should still provide sufficient granularity for practical threshold purposes. Alternative 2: Non-Power-of-2 Exponent Base Another alternative would be to use a non-power-of-2 base for the exponent, such as 10: threshold = mantissa * (10^exponent) This would make the values more human-readable and potentially allow for more intuitive threshold settings. However, this approach has significant drawbacks: Performance Penalties: Computing powers of 10 is more computationally expensive than powers of 2, which can be implemented as simple bit shifts. Precision Issues: Powers of 10 cannot be represented exactly in binary, potentially leading to rounding errors. For these reasons, the power-of-2 approach is preferred for the Float24 format. Benefits Practical Range: Float24 can represent values from 0 sompi to well beyond most practical threshold needs Zero Threshold Support: Enables simple same-address spending without value constraints Signature Type Choice: Allows selection between ECDSA and Schnorr signatures Complete Public Key Information: Preserves Y coordinate parity for ECDSA keys Implementation Considerations Bech32 Encoding: The complete 35-byte payload would be encoded using bech32 Script Generation: Wallet software would need to generate the appropriate script based on the Float24 value and signature type Validation: Address validation would need to check the Float24 format is valid Compatibility: Existing software would need updates to recognize and handle the new address type Summary The Float24 Threshold Address format provides an efficient way to encode threshold values directly in Kaspa addresses, enabling powerful compounding and borrowing functionality without requiring consensus changes. This proposal builds on the transaction introspection capabilities introduced in KIP-10 while maintaining compatibility with existing P2SH infrastructure. By standardizing this address format, we can enable a new class of applications that leverage threshold-based spending conditions, particularly for mining pools and other services that benefit from auto-compounding UTXOs. The support for both ECDSA and Schnorr signatures, along with a wide range of threshold values, provides flexibility for various financial applications.",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2025-05-22T08:23:49.222Z",
      "updated_at": "2025-05-22T09:25:10.878Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-draft-threshold-address-format-for-kip-10-mutual-transactions/346/1",
      "category_id": 1
    },
    {
      "post_id": 567,
      "post_number": 2,
      "topic_id": 346,
      "topic_title": "KIP Draft: Threshold Address Format for KIP-10 Mutual Transactions",
      "topic_slug": "kip-draft-threshold-address-format-for-kip-10-mutual-transactions",
      "content": "It\u2019s important to note that the conversion from an address to a script public key (SPK) is a one-way process. Once an address is converted to an SPK, the original script cannot be recovered from the SPK hash alone unless all placeholder values are known.",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2025-05-22T08:39:04.245Z",
      "updated_at": "2025-05-22T08:39:04.245Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-draft-threshold-address-format-for-kip-10-mutual-transactions/346/2",
      "category_id": 1
    },
    {
      "post_id": 543,
      "post_number": 1,
      "topic_id": 323,
      "topic_title": "A Basic Framework For Proofs Stitching",
      "topic_slug": "a-basic-framework-for-proofs-stitching",
      "content": "Introduction In an atomic synchronous composability model, a single transaction may invoke execution in several logic zones (conceptually, smart contracts or rollups). A priori, synchronous composability ambitions appear to be at odds with ZK execution scaling\u2014if provers and verifiers are forced to ZK prove and verify such involved transactions as a whole, the separation to distinct and independent logic zones will effectively collapse to a single monolithic design. This post outlines a proof of concept for how provers of various logic zones can cumulatively create proofs for multi logic zone transactions, while each is only required to provide proofs limited to executions of code in their respective logic zone. This consists an initial but essential step to potentially prevent the aforementioned collapse. The main idea is stitching in a coherent manner several heterogenous zk proofs. In that regard, the framework bears some resemblance to works like legosnarks, but unlike those, merely uses Snark in a black box manner. Subtransactions In our setting, multiple logic zones execute within a single transaction, sequentially calling on one another, where the initial logic zone and its input arguments are determined by the transaction\u2019s contents. Each zone maintains its own internal state, isolated from others, which can neither read from nor write to it. The execution trace derived from the above can be broken into continuous segments I refer to as subtransactions. Each subtransaction represents a continuous block of execution in one logic zone and is executed in a strict sequence, with a call stack managing the flow between them. To account for atomicity, the call stack is extended with a special element \\bot, that when pushed to the stack denotes that the transaction as a whole has failed. A subtransaction can be categorized by three elements: Inputs: The internal state of the logic zone executed, at the outset of the subtransaction. The input call stack, with the executed logic zone and the current local variables (possibly including a program counter) stored as a pair at the stack\u2019s top. This call stack allows managing proper context switching between the subtransactions. Outputs: The updated internal state of the executed logic zone, after execution of the subtransaction. The output call stack, capturing any changes made to the call stack as a result of a call to a new logic zone (update of the top\u2019s local variables, and a push of a new pair to the stack consisting of the called logic zone, and the local variables initialized by the call\u2019s arguments) or a return from one (A pop of the current element of the stack, and an extension of the new top\u2019s local variables with a new variable denoting the return value). Ordinal Number: A subtransaction is assigned an index representing its position in the overall sequence of execution. This explicit ordering will be useful for efficiently stitching subtransactions together. Zero-Knowledge Proof Messages A proof message is constructed as a tuple (index, inputs, outputs, \\pi), conceptually representing a subtransaction\u2019s execution. Namely: index: ordinal number i of the subtransaction. inputs: The starting inputs, including the executed logic zone\u2019s internal state and the input call stack. outputs: The resulting outputs, including the updated internal state and the output call stack. \\pi: a zero-knowledge proof demonstrating that, given these inputs, a subtransaction with index i correctly produces the specified outputs. The logic zone responsible for executing the subtransaction, as well as the next logic zone in line, are implicitly identified within the call stack data. Proofs can be internally valid (i.e. \\pi verifies the subtransaction as stated correctly) even if they do not represent a computation that occurs at the transaction\u2019s proper execution trace. Proofs hence need be treated as conditional, and not finalized until they are \u201cstitched\u201d (see below) together with others up to the transaction\u2019s conclusion. Proving Conditional Proofs Provers each specialize in a designated logic zone and produce proofs for its corresponding subtransactions. To generate a proof of a certain subtransaction\u2019s execution, a prover needs to have the inputs of that subtransaction. These intermediary values may depend on the results of subtransactions in other logic zones. Provers could acquire the results of these intermediary computations, in one of two ways: Communicate intermediary results of subtransactions with each other via an offchain provers\u2019 network. Execute (but not prove) subtransactions of other logic zones in order to derive the intermediary results they require by themselves. Proofs Stitching All proof messages, once submitted and verified, are stored in a public database. They are assumed to be submitted in an unordered manner. To finalize a transaction, a sequence of proofs need to be sequentially stitched with each other, in a manner such that the outputs and inputs are consistent, from the initial subtransaction and the global state on all associated logic zones at its inception, until either the stack is empty, or alternatively, a \\bot has been pushed to it. An implementation for stitching is given below. class TransactionStitcher: FAILURE_SIGN = '\u22a5' class StitchedChain: def __init__(self, initial_call_stack, initial_states): \"\"\" :param initial_call_stack: The initial call stack object. It must include a field 'currentLogicZone' that identifies the active logic zone. :param initial_states: A dict mapping each logic zone to its internal state at transaction inception. \"\"\" self.proof_list = [] # List of proofs in forward order. self.latest_writes = dict(initial_states) # zone -> internal state (latest updates) self.current_call_stack = initial_call_stack # The current call stack. def __init__(self, initial_stack, initial_states): \"\"\" :param initial_stack: The initial call stack (an object with a field 'currentLogicZone'). :param initial_states: A dict mapping each logic zone to its initial internal state. \"\"\" # proofs_by_index maps an index to a dict: # key: (internalState, callStack) from the proof's inputs, value: the proof. self.proofs_by_index = {} self.stitched_chain = self.StitchedChain(initial_stack, initial_states) self.initial_states = dict(initial_states) # For failure case. # --- Main Method --- def submit_proof_message(self, proof): \"\"\" Processes a new publicly verified proof by storing it by index and attempting to extend the active forward-growing chain. Outputs the result of the transaction if it can be fully stitched together :param proof: A dictionary representing a proof with keys: - 'index': int, the subtransaction's ordinal index. - 'inputs': dict with keys: 'callStack': an object with a field 'currentLogicZone', 'internalState': the state of the active logic zone at the start. - 'outputs': dict with keys: 'callStack': an object (same structure as inputs['callStack']), 'internalState': the updated state after execution. - '\u03c0': the zero-knowledge proof data (not used in stitching logic). - 'callstack': an object representing the call stack; it must include a field 'currentLogicZone'. \"\"\" self.store_proof(proof) while True: candidate = self.lookup_candidate() if candidate: self.extend_chain_with_candidate(candidate) if self.is_candidate_ending(candidate): if self.is_failure(candidate['outputs']['callStack']): return self.initial_states # Failure: return initial state. else: return self.stitched_chain.latest_writes # Success: return latest writes. else: break return None # --- Helper Functions --- def is_empty(self, call_stack): \"\"\" Determine if the call stack is empty. (Implementation left out \u2013 should return True if call_stack represents an empty stack.) \"\"\" pass def is_failure(self, call_stack): \"\"\" Determine if the call stack signals failure (i.e. contains FAILURE_SIGN as the logic zone at the top). (Implementation left out.) \"\"\" pass def is_candidate_ending(self, proof): \"\"\" Returns True if the proof's output call stack indicates termination: either an empty call stack (success) or a failure signal. \"\"\" cs = proof['outputs']['callStack'] return self.is_empty(cs) or self.is_failure(cs) def get_candidate_key(self, proof): \"\"\" Returns a tuple (internalState, callStack) from the proof's inputs for dictionary lookup. \"\"\" return (proof['inputs']['internalState'], proof['inputs']['callStack']) def store_proof(self, proof): \"\"\" Stores the proof in the proofs_by_index dictionary. \"\"\" index = proof['index'] key = self.get_candidate_key(proof) self.proofs_by_index.setdefault(index, {})[key] = proof def lookup_candidate(self): \"\"\" Looks up and returns a candidate proof for the next index using the expected inputs, or returns None if no candidate exists. \"\"\" active_zone = self.stitched_chain.current_call_stack.currentLogicZone expected_state = self.stitched_chain.latest_writes.get(active_zone) expected_stack = self.stitched_chain.current_call_stack next_index = len(self.stitched_chain.proof_list) + 1 candidate_key = (expected_state, expected_stack) return self.proofs_by_index.get(next_index, {}).get(candidate_key, None) def extend_chain_with_candidate(self, candidate): \"\"\" Extends the stitched chain with the given candidate proof and updates the chain's state. \"\"\" self.stitched_chain.proof_list.append(candidate) self.stitched_chain.current_call_stack = candidate['outputs']['callStack'] active_zone = self.stitched_chain.current_call_stack.currentLogicZone self.stitched_chain.latest_writes[active_zone] = candidate['outputs']['internalState'] Final Notes The precise identity of TransactionStitcher in the ecosystem remains to be finalized. It could be invoked within each logic zone separately, at a single stitching specialized logic zone, or even delegated to the L1. Regardless, it must be aware of the standard used to encode data (inputs and outputs) in proof messages. Whole transactions could be proven ahead of time and stitched together in a similar manner. Transactions notably form a dependency DAG between themselves according to their associated logic zones. Two branches of this DAG could potentially advance themselves independently of the other. Proof messages of the same logic zones will likely be submitted by provers in batches (likely even including messages from several transactions). Parsimonious encoding of these batches, and correspondingly decoding their contents, warrants more detailed discussion. A more general setting could consider parallelizable calls within the transaction. Such intra-transaction parallelism opens up various questions on data races prevention, determinism, and read and write permissions, which at the moment do not appear justified from a practical perspective. Challenge for the reader: does a proof message truly must commit to the index of the subtransaction? does it need the state of the stack in its entirety?",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-04-02T12:08:08.794Z",
      "updated_at": "2025-04-21T12:39:50.272Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/a-basic-framework-for-proofs-stitching/323/1",
      "category_id": 11
    },
    {
      "post_id": 473,
      "post_number": 1,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork discussion thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "This is a discussion thread for KIP-14 \u2014 The 10-BPS Crescendo Hardfork: kips/kip-0014.md at master \u00b7 kaspanet/kips \u00b7 GitHub",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-21T21:11:53.611Z",
      "updated_at": "2025-01-22T19:04:34.518Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/1",
      "category_id": 9
    },
    {
      "post_id": 506,
      "post_number": 2,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork discussion thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "RE The pruning depth formula provides a lower bound, yet the actual pruning period can be set longer. Plugging in the scaled parameters, the lower bound is calculated to be 627,258 blocks, representing approximately ~17.4238 hours. We suggest rounding this up to 30 hours for simplicity and practical application. A 30-hour period is closer to the current mainnet pruning period (~51 hours) and aligns closely with the value used and benchmarked throughout TN11 (~31 hours). I think setting it as some (integer) multiplication of finality depth (something like 3\u03d5) can simplify pruning point calculation, since the difference between pruning points is at least the finality depth.",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2025-02-03T16:41:51.158Z",
      "updated_at": "2025-02-03T16:41:51.158Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/2",
      "category_id": 9
    },
    {
      "post_id": 507,
      "post_number": 3,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork discussion thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "36 hours will slightly increase the storage upper bound (from 190GB for transient storage to ~230GB), but it does seem reasonable.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-02-03T17:31:29.374Z",
      "updated_at": "2025-02-03T17:31:29.374Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/3",
      "category_id": 9
    },
    {
      "post_id": 520,
      "post_number": 4,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork discussion thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "Dear community - I propose to consider including KIP-15 to the Crescendo HF. The change is tiny (literally few lines of code) but still IMHO very powerful.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-02-23T12:54:08.596Z",
      "updated_at": "2025-02-23T12:54:08.596Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/4",
      "category_id": 9
    },
    {
      "post_id": 525,
      "post_number": 5,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork discussion thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "Any decision about KIP-15?",
      "raw_content": "",
      "author": "Jacek_Kozicki",
      "created_at": "2025-02-27T22:12:19.277Z",
      "updated_at": "2025-02-27T22:12:19.277Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/5",
      "category_id": 9
    },
    {
      "post_id": 530,
      "post_number": 6,
      "topic_id": 279,
      "topic_title": "Crescendo Hardfork discussion thread",
      "topic_slug": "crescendo-hardfork-discussion-thread",
      "content": "Being tested \u2026 Hope it will be ok",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-03-05T19:42:14.189Z",
      "updated_at": "2025-03-05T19:42:14.189Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/crescendo-hardfork-discussion-thread/279/6",
      "category_id": 9
    },
    {
      "post_id": 339,
      "post_number": 1,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "This post aims to provide a possible picture of how based zk (zero-knowledge) rollups can be designed to operate over Kaspa\u2019s UTXO-based L1 (see @hashdag\u2019s post for broader context). This is by no means a final design, but rather the accumulation of several months of discussions with @hashdag, @reshmem and Ilia@starkware, which is presented here for aligning the core R&D discussion in the Kaspa community around common ground before finalizing (the L1 part of) the design. I\u2019m making a deliberate effort in this post to use the already established jargon and language of the broader SC community. To this end I suggest that unfamiliar readers review documents such as eth-research-based, eth-docs-zk and nested links therein before proceeding with this post. Brief description of based zk rollups A zk rollup is an L2 scaling solution where L2 operators use succinct zk validity proofs to prove correct smart contract execution without requiring L1 validators to run the full computations themselves. A based zk rollup is a type of rollup design where the L2 is committed to operations submitted to L1 as data blobs (payloads in Kaspa\u2019s jargon), and cannot censor or manipulate the order dictated by L1. Based zk rollups as an L1 \u2194 L2 protocol Since based zk rollups represent an interaction between the base layer (L1) and the rollup layer (L2), the L1 must expose certain functionalities to support rollup operations. Conceptually, this defines a protocol between L1 validators and L2 provers. Logical functionalities expected from L1: Aggregate rollup transactions: L1 aggregates user-submitted data blobs in the order received, providing a reliable anchoring for L2 validity proofs. Verify proof submissions: L2 operators submit zk proofs that confirm correct processing of these transactions, ensuring L2 execution follows the agreed protocol and updating the L2 state commitments stored on L1. Entry/Exit of L1 funds: The protocol must enable deposits and withdrawals of native L1 funds to and from L2, ensuring consistent state and authorized spending. Key point: L1 acting as the zk proof verifier (point 2) is crucial for enabling the native currency (KAS) to serve as collateral for L2 financial activity, underscoring the importance of point 3. Additional benefits of having L1 verify the proofs include establishing a clear, single point of verification (enforced by L1 consensus rather than requiring each interested party to perform it individually) and providing proof of state commitment for new L2 validators. VM-based vs. UTXO-based L1 When the L1 is a fully-fledged smart contract VM, the establishment of this protocol is straightforward: the rollup designer (i) publishes a core contract on L1 with a set of rules for L1 validators to follow when interacting with the rollup; and (ii) specifies a program hash (PROG) that the L2 provers are required to prove the execution of (via a zk proof, ZKP). This two-sided interplay establishes a well-defined commitment of the rollup to its users. For UTXO/scripting-based L1s like Kaspa, a more embedded approach is required in order to support various rollup designs in the most generic and flexible way. Technical design of the L1 side of the protocol in UTXO systems To begin with, I present a simplified design which assumes a single rollup managed by a single state commitment on L1. Following this minimum-viable design I discuss the implications of alleviating these assumptions. Kaspa DAG preliminaries A block B \\in G has a selected parent \\in parents(B) as chosen by GHOSTDAG. The mergeset of B is defined as past(B) \\setminus past(\\text{selected parent of } B). The block inherits the ordering from its selected parent and appends its mergeset in some consensus-agreed topological order. Block C is considered a chain block from B 's pov if there\u2019s a path of selected parent links from B to C (which means the DAG ordering of B is an extension of C \u2019s DAG ordering). Detailed protocol design Recursive DAG ordering commitments: A new header field, called ordered_history_merkle_root, will be introduced to commit to the full, ordered transaction history. The purpose of this field is to maintain a recursive commitment to the complete sequence of (rollup) transactions. The leftmost leaf of the underlying Merkle tree will contain the selected parent\u2019s ordered_history_merkle_root, thereby recursively referencing the entire ordered history. The remaining tree leaves correspond to the ordered sequence of mergeset transactions, as induced by the mergeset block order. ZK-related opcodes: OpZkVerify: An opcode that accepts public proof arguments and verifies the correctness of the zk proof (our final design might decompose this opcode into more basic cryptographic primitives; however, this is out of scope for this post and will be continued by @reshmem). OpChainBlockHistoryRoot: An opcode that provides access to the ordered_history_merkle_root field of a previous chain block. This opcode expects a block hash as an argument and fails if the block has been pruned (i.e., its depth has passed a threshold) or is not a chain-block from the perspective of the merging block executing the script. It will be used to supply valid anchor points to which the ZKP can prove execution. State-commitment UTXO: The UTXO responsible for managing the rollup state will appear as an ordinary UTXO from L1\u2019s perspective, with no special handling or differentiation at the base layer. The UTXO spk (script_public_key) will be of type p2sh (pay to script hash), which will represent the hash of a more complex structure. Specifically, it will be the hash of the following pre-image: PROG (the hash of the permanent program L2 is obligated to execute) state_commitment (the L2 state commitment) history_merkle_root (the ordered_history_merkle_root from L1\u2019s header, representing the point in the DAG ordering up to which L1 transactions have been processed to produce the corresponding L2 state commitment) Additional auxiliary data required to verify a ZKP (e.g., a well-known verification key) The remaining execution script (will be specified below) Proof transaction: In its minimal form, a proof transaction consists of an incoming state-commitment UTXO, an outgoing updated state-commitment UTXO and a signature revealing the pre-images and the ZKP. Assuming such in, out UTXOs and a signature script sig, the following pseudo code outlines the script execution required to verify the validity of this signature (and logically verify the L2 state transition): // // All data is extracted from sig // Some operations below might use new Kip10 introspection opcodes // // Skipping the part where the script itself is proven to be in the preimage // (which is standard p2sh processing) // // Prove preimages show (prog_in , commit_in , hr_in , ...) is the preimage of in.spk show (prog_out, commit_out, hr_out, ...) is the preimage of out.spk verify prog_in == prog_out // verify prog is preserved // Verify L1 history-root anchoring extract block_hash from sig // the chain block we are claiming execution to hr_ref <- OpChainBlockHistoryRoot( block_hash ) // fails if anchoring is invalid verify hr_ref == hr_out // Verify the proof extract zkp from sig OpZkVerify( proof: zkp, proof_pub_inputs: [commit_in, commit_out, hr_in, hr_out]) // ^ omitting prog and other auxiliary verification data L2 program semantics: In order to provide the desired based rollup guarantees, the execution specified by the (publicly known) L2 PROG must strictly adhere to the following rules: Reveal (through private program inputs) the full tree diff claimed to be processed: T(hr_out) \\setminus T(hr_in) Execute the identified transactions in order, without any additions or removals Observe that the first rule, combined with the OpChainBlockHistoryRoot call within the script, ensures that the state commitment always advances to a valid state: The script verifies that hr_out is a valid chain block history commitment. The PROG verifies that hr_out recursively references hr_in and, as a result, must be an extension of it. This verification by PROG is enforced by L1 through OpZkVerify. Operational flow: Tx_1, Tx_2, ..., Tx_n with data payloads are submitted to the DAG, included in blocks, and accepted by chain blocks C_1, C_2, ..., C_n, respectively. The transaction hashes are embedded into the ordered_history_merkle_root fields of the corresponding headers, enforced as part of L1 consensus validation. An L2 prover chooses to prove execution up to block C_i A proof transaction referencing the initial state-commitment UTXO and producing a new state-commitment UTXO (encoding the new history root ordered_history_merkle_root(C_i)) is created. The proof is validated by L1, and the new state-commitment UTXO replaces the previous one in the UTXO set, recording the L2 state transition on L1. Soundness and flexibility of the proposed design The design presented supports fully based zk rollups by embedding ordered history Merkle roots into block headers and introducing new zk-related opcodes into Kaspa\u2019s script engine. The history roots provide modular and complete evidence of DAG ordering, enabling provers to select granularity over any period of consecutive chain blocks (while still requiring processing in mergeset bulks). The combination of L1 script opcodes and L2 PROG programmability provides substantial flexibility for rollup design. Points for subsequent discussion Despite its focus on the most baseline design, this post is already becoming rather long, so I\u2019ll wrap up by briefly outlining some key \u201czoom-in\u201d points for further discussion and refinement. Uniqueness of the state-commitment UTXO 1.1 Challenge: Proving that a specific UTXO represents \u201cthe\u201d well-known authorized state commitment for a given rollup. Although state transitions do not require such proof, it is important, for instance, for proving L2 state to a newly syncing L2 node. 1.2 Solution Direction: L2 source code can define a \u201cgenesis\u201d state (encoded in the initial UTXO), and zk validity proofs can recursively attest that the current state originated from that genesis. Entry/Exit of L1 funds 2.1 Allow deposits to static addresses representing the L2. 2.2 Allow withdrawals back to L1 (as additional proof transaction outcomes). 2.3 The N-to-const problem: Address the challenges arising from local limits on transaction size and the potentially many outcomes resulting from a batched proof operation. Extension to many rollups \u00b9 3.1 Requirement/Desire: Each L2 rollup prover should only need to execute O(\\text{rollup activity}) within their PROG proof execution. 3.2 Solution Direction: Manage the L1 history Merkle tree by grouping by rollup and further dividing into activity/inactivity branches. Note: Applying the grouping recursively might result in long-term storage requirements per rollup, which has broader implications. Multiple state commitments per rollup 4.1 Challenge: Allowing L1 to manage multiple state commitments for a single rollup in order to balance scalability and validity (allowing different provers to partially advance independent segments/logic zones of L2 state). 4.2 Solution Direction: Implement partitioned state commitments on L1, representing dynamic cuts of the L2 state tree. If taken to the extreme, this solution could allow a user to solely control their own L2 account via a dedicated state commitment on L1. Another major aspect not discussed in this post is the zero-knowledge technology stack to be supported and its implications for L1 components (e.g., the hash function used to construct the Merkle history trees). Laving this part to @reshmem, @aspect and others for full follow-up dives. [\u00b9] The introduction of many rollups (or subnets in Kaspa\u2019s jargon) touches on conceptual topics such as L2 state fragmentation and atomic composability, which are beyond the scope of this post and were preliminarily discussed in @hashdag\u2019s post. Here, I\u2019m referring merely to the technical definitions and consequences.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-09T13:09:26.728Z",
      "updated_at": "2025-01-09T12:48:11.413Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/1",
      "category_id": 11
    },
    {
      "post_id": 347,
      "post_number": 2,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "I know that for zkEVM, a single account holds the balances of all users in the rollup. It looks like this design makes that account a UTXO which contains a commitment in the form of a merkle root of a merkle tree that commits to all the current balances of existing accounts in the rollup. Re 1: Highly cogent and reasonbale. Re 2: Will the entire tree be posted on-chain or just the root of the tree? If it is just the root, how do users get their branch in the tree in order to be capable of exiting without permission when they want to? Re 3: Not sure how you gonna do that based on your description. It sounds like some clustering. Personal thought is not sure if this is a high priority issue (do you need that many rollups on Kaspa or having one that works is more important?) Re 4: Not sure if understood correctly, but if the concern is about scalability and validity, I think fully distribututed ZKPs may be something interesting to think of. The scheme distributes proof generation across multiple machines and require minimal communication among them; it allows us to distribute ZKP generation in zkRollups and zkEVM among multiple participants as mining pools. Participants may share the reward for proof generation, akin to miners in PoW chains like Kaspa. This also is related to 3 since then groupin may be unecessary. One more thing: do we have a role like an aggregator (such as one in CKB)?",
      "raw_content": "",
      "author": "YesComrade",
      "created_at": "2024-12-10T21:26:43.666Z",
      "updated_at": "2024-12-10T21:26:43.666Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/2",
      "category_id": 11
    },
    {
      "post_id": 351,
      "post_number": 3,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "In the basic design - yes, the UTXO will contain a single state root representing all existing rollup accounts. Re 2. Only the root. Any user operation on his L2 account will mutate the root, and the prover reporting this root mutation (via a proof tx) will be obligated to apply the outcome in L1 in the form of an additional tx output going to the user\u2019s L1 address. This will be enforced as part of the PROG. Re 3. As mentioned, the above description applies to the basic design. In a more advanced/dynamic design, we can have the L2 state committed to L1 in multiple, fragmented state commitments\u2014where at the extreme, a single account can have its own L1 state-commitment UTXO. My mental picture of this is keeping the image of L2 state as a single tree, but drawing a tree cut representing the subtree roots which are reported to L1 (see image) image1072\u00d7352 35.8 KB Notes: Such L2 split strategies can be static (as in scenario 2), or dynamic (scenario 3) in which case they will be supported via an L2 \u201cdetach\u201d command which locally breaks a commitment to its tree-child commitments (on L1 this will look like a proof tx with a single incoming UTXO representing the previous subtree root and multiple output UTXO entries representing its children) In scenario 3, accounts 13, 14 can be solely controlled by their sole owner, perhaps even requiring that each operation must be provided with an immediate inline ZKP. Think krc20 token holders performing a send\u2014the transaction can consume the state-commitments UTXOs of both the sender and recipient and can output the updated UTXOs with the updated state commitments, where the signature is an inline ZKP (credit: @reshmem, @aspect). Of course there are numerous subtleties and complexities to this dynamic design which I\u2019m neglecting here, one major one being \u201chow to prove a subtree state commitment can be advanced w/o needing to execute unrelated rollup transactions (for showing non of them touched this part of the state)?\u201d. This requires some form of \u201cexclusion proof\u201d or an explicit way to state read/write dependencies (cc: @aspect). Re 4. I totally agree, I think proof generation should be a distributed effort. Imho it can be a coordinated collective effort, and even centralized to some degree, since decentralization and censorship-resistance are enforced by L1 due to the based design guarantees. Re the final remark. Afaiu the aggregator role in CKB is exactly the non-based part where an L2 operator is required to collect off-chain batches, hence it\u2019s irrelevant to this based design. That being said, there\u2019s no way to enforce such a thing from L1 (unless you define a single METAPROG which all rollups must extend)\u2014the point is to allow based rollups and to make them the default way to go, not to forbid non-based approaches.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-11T11:01:55.226Z",
      "updated_at": "2024-12-11T11:01:55.226Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/3",
      "category_id": 11
    },
    {
      "post_id": 354,
      "post_number": 4,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "This has been an interesting read so far. I\u2019m still trying to catch up to be able to properly parse and understand the content fully. For now, I do have some preliminary questions based on my first reading here: Entry/exit funds - will these use p2sh exclusively? Or p2pk for entry and exit but p2sh for state transitions? What does a state_commitment look like? Regarding the use of p2sh here referencing a single state commitment - does this mean that each state transition will necessarily use a different p2sh address each time? I\u2019m wondering about the impact of this on services like the explorer who will maintain several address entries that will keep getting used just the one time. With the statement: \u201cThe proof is validated by L1, and the new state-commitment UTXO replaces the previous one in the UTXO set, recording the L2 state transition on L1.\u201d - what happens during a re-org in L1? This will regularly happen around the tips. How is the L2 tolerant to such re-orgs? The post focuses on state transitions, but I\u2019m curious about how much funds would actually move between each transaction from p2sh to new p2sh? Each tx will incur a fee also. What are your thoughts on these amounts moved/paid on the base layer as state transitions in the L2?",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2024-12-12T06:56:20.319Z",
      "updated_at": "2024-12-12T06:56:20.319Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/4",
      "category_id": 11
    },
    {
      "post_id": 355,
      "post_number": 5,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "I\u2019ll try to address the questions to some degree, though I think some of the answers will be fully clarified only after in-depth follow-up posts. Note: for simplicity, my points below still assume the baseline design with a single state-commitment UTXO per rollup instance, but can easily be extended to the multiple-commitment scheme described in my previous comment. Re 1. First - this area is definitely still not finalized and you (or any other reader) should join the brainstorming. My current thinking is as follows: Think of L2 (a rollup instance) as a \u201cvirtual\u201d wallet which might own many UTXOs on L1. All spks (addresses) for this wallet will be p2sh; however, some will be dynamic and some static. Like you mention in point 3, the state-commitment UTXO is an everchanging dynamic p2sh address. In addition to that there will be a set of static p2sh addresses which can be driven from PROG \u00b9. The static addresses will be used by users for deposit/entry. Spending these UTXOs must be done through a proof transaction and the spending sig must reveal the preimage script which will delegate verification to the primary input sig of the proof tx (the one containing the ZKP) \u00b2. Re 2. The L2 state_commitment would usually be a Merkle root of its state (e.g., the root of a Patricia tree). But our L1 design should not rely on specific assumptions about it. Re 3: As mentioned, yes, it means a different p2sh address each time. Explorers will learn how to track these addresses and treat them as a continuous entity by following the specific L2 encoding. Re 4. Great question. This is precisely why OpChainBlockHistoryRoot \u00b3 is set to fail if the block hash isn\u2019t a chain block from pov of the executing merging block. If a reorg invalidates a previously used anchoring chain block, then the proof tx using that anchor will be invalidated as well (when executed through the new chain), thus effectively \u201cunspending\u201d the spent state-commitment UTXO. This means that following a reorg, L2 provers will need to resubmit proofs proving execution according to the order dictated by the new chain segment. Your mental picture here should be a proof chain following the DAG selected chain. Note that if done correctly, L2 provers can reuse hierarchic zk proofs used to compose the previous proof-chain. I.e., a reorg does not necessarily mean full re-computation of all reversed proofs. Re 5. It can be the full amount deposited to L2 all concentrated in the \u201cdynamic\u201d state-commitment UTXO. I don\u2019s see this as an issue. [\u00b9] These addresses can incorporate KIP10-style additive schemes for improved management of the L2 UTXO subset and for compliance with KIP9 [\u00b2] The advanced reader might notice that this scheme requires the \u201cUniqueness of the state-commitment UTXO\u201d property I mentioned at the end of the post. [\u00b3] Unrelated note on OpChainBlockHistoryRoot. The chain depth we allow access to here will affect syncing of new L1 nodes. We will need to sync a chain segment of that length below the pruning point in order for the syncee to be able to process all transactions above the pruning point deterministically.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-12T11:10:23.989Z",
      "updated_at": "2024-12-22T15:17:24.401Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/5",
      "category_id": 11
    },
    {
      "post_id": 394,
      "post_number": 6,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "It took me a while to find this ancient post in Bitcoin Forum: Storing UTXOs in a Balanced Merkle Tree (zero-trust nodes with O(1)-storage). I think it answers the basic questions of storing UTXOs plus that its #7 reply (starting with \u201cNode-deletion is not the inverse of node-insertion\u201d) is related to your Re 1. (addressing the entry/exit question) above.",
      "raw_content": "",
      "author": "LostandFound",
      "created_at": "2024-12-18T13:52:22.577Z",
      "updated_at": "2024-12-18T13:52:22.577Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/6",
      "category_id": 11
    },
    {
      "post_id": 398,
      "post_number": 7,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "@michaelsutton then I think the biggest thing what this post wasn\u2019t addressing is the \u201chow to be based\u201d question? I can think of execution of a zkSNARK verification program pre-configured with a verification key (like a SNARK VM) . It can be run and implemented in a really simple way with only two challenge scenarios and just 3 instructions ADD, SUB and MULKAS, running any zk-snark generator lib targeting groth16, for example. Certainly bridge that allows users to transfer assets between L1 and L2 would be needed. That should also easily solve the problem of deposit and withdrawal. There may be two types of proofs that will be needed: one for transaction inclusion and another one for state transition of bridge sc on the L2 side. For both a zkSNARK groth16 proof can be a good fit and proof recursion should be supported to allow Plony2 and STARKs proofs if needed. But if a role of operator is introduced then I guess this is agian non-based, right? As based-approach may be challenging and taking some time and saying \u201cnot to forbid non-based approaches\u201d, will Kaspa welcome third party teams to make a non-based rollup plan on Kaspa? [Question 1] Also what happens if the target block with the target TX is invalidated by reorg? The OpChainBlockHistoryRoot looks like a revert. Practically, for an unconfirmed transaction does the transaction remain in the mempool and is reprocessed? Would this end up with a new TX hash? How might we be notified of this new TX? For a confirmed (multiple confirmations) transaction (though may not be likely) will it be marked as reverted? I guess the answer is Yes as you mentioned about to resubmit. Could this potentially lead to a situation in which a TX1 is confirmed on L1 while another TX2 is later confirmed on L1 on the canonical, which means TX2 is final but TX1 is reverted after TX2 is submitted? [Question 2] An additional thought on reorg: is it possible to figure out a way maybe just do experiements? I have seen people do it for Opstack by Setting up an L2 replica \u2013 Bridge OAS from L1 to L2 using three different accounts (A, B, C) \u2013 Fork L1, and then roll up L2 to the miner\u2019s side \u2013 Transfer \u2013 Merge L1\u2019s minor chain with the majority \u2013 Set up another replica. Then just see what happens to the accounts. Would it be possible to figure out ansuwers to such questions just empirically maybe? So different L2s will at least know what will happen at least. [Question 3]",
      "raw_content": "",
      "author": "YesComrade",
      "created_at": "2024-12-21T20:50:12.322Z",
      "updated_at": "2024-12-21T20:50:53.006Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/7",
      "category_id": 11
    },
    {
      "post_id": 400,
      "post_number": 8,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "AFAIK Andrew\u2019s post deals with UTXO commitments (more accurately: accumulator), not sure what connection to this post you are suggesting here (?)",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-22T09:31:12.200Z",
      "updated_at": "2024-12-22T09:31:12.200Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/8",
      "category_id": 11
    },
    {
      "post_id": 521,
      "post_number": 9,
      "topic_id": 208,
      "topic_title": "On the design of based ZK rollups over Kaspa's UTXO-based DAG consensus",
      "topic_slug": "on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus",
      "content": "michaelsutton: OpChainBlockHistoryRoot: An opcode that provides access to the ordered_history_merkle_root field of a previous chain block. This opcode expects a block hash as an argument and fails if the block has been pruned (i.e., its depth has passed a threshold) or is not a chain-block from the perspective of the merging block executing the script. It will be used to supply valid anchor points to which the ZKP can prove execution. I want to emphasize the fact that this opcode is stateful, which breaks the assumption that a script validity is dependent only on the transaction itself and its previous UTXOs. This means that a transaction that spends a UTXO with such opcode might be invalidated without an intentional double spend, which might harm the UX of the recipient. This can be dealt by introducing a concept similar to coinbase maturity for such transactions, or developing an off-chain mechanism to help users identify such transactions (and dependent transactions) so they can set a higher confirmation time.",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2025-02-23T15:17:32.886Z",
      "updated_at": "2025-02-23T15:17:32.886Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/on-the-design-of-based-zk-rollups-over-kaspas-utxo-based-dag-consensus/208/9",
      "category_id": 11
    },
    {
      "post_id": 519,
      "post_number": 1,
      "topic_id": 303,
      "topic_title": "KIP 15 discussion thread",
      "topic_slug": "kip-15-discussion-thread",
      "content": "This is the discussion thread for KIP 15",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2025-02-23T09:07:49.125Z",
      "updated_at": "2025-02-23T09:07:49.125Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-15-discussion-thread/303/1",
      "category_id": 1
    },
    {
      "post_id": 356,
      "post_number": 1,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "This post was deeply inspired by ideas shared with me by @hashdag, @michaelsutton, and @proof. It assumes a strong foundational understanding of ZK concepts and the workings of ZK-protocols. This blog post is far from being a detailed design proposal, but the core concept is accurately outlined. A previous post by @michaelsutton outlined the basic L1<\u2013>L2 interaction. Here I dive in further into the technical bits: About Proving System Choice for L1 zk verification L1 Enhancements / Additions a. Low level elliptic curve opcodes b. L1 zk-verify function pseudo code as it should be implemented in Kaspa script c. ZK-friendly hash function choice (partly finished, require separate blogpost) d. UTXO ( input / output ) Anchoring via KIP10 (so-called canonical bridge) Possible L2 design examples a. Single proving system b. zkVM base solution c. Pseudocode for zk-SNARK Verification for Groth16 Additionally, the general guideline for implementing L2 solutions on top of Kaspa L1 should prioritize minimizing L1 modifications, ensuring support for a wide range of L2 solutions, and maintaining a strong guarantee of bounded performance impact for zk-verification executed by L1 nodes. This post serves as an attempt and example of how zk-L2s can be supported with minimal changes to the BTC-like Kaspa script language and L1 architecture, while still offering flexibility for diverse implementation options. Terminology and concepts adopted from @michaelsutton post: Recursive DAG ordering commitments Merkle root State UTXO / State commitment UTXO L1 based rollups (zk) ZK-related opcodes (OpZkVerify) 1. About Proving System Choice for L1 zk verification Groth16 / Plonk proving systems are proposed to be utilized due to their small proof size and constant-time verification complexity, which are critical for maintaining L1 performance. Groth16/ Plonk proving systems are highly stable, with thoroughly debugged technology stacks and rigorously audited, well-established mathematics. Despite not being recent developments, they remain state-of-the-art in terms of proof size, verifier efficiency, and overall stability and security. Moreover, these proving systems are the only ones natively supported by major blockchain networks such as Ethereum. Proving System zk-stacks & stability: Codebases for zk-proving systems like Groth16 and Plonk are mature, extensively tested, and battle-hardened across various blockchain ecosystems. This reduces the risk of bugs or implementation errors, ensuring a stable and secure integration. Elliptic Curve Choice: Groth16 / Plonk zk-proving systems are based on the BN254 elliptic curve (a Barreto-Naehrig curve with a 254-bit prime field). While these systems can operate with other pairing-friendly elliptic curves, BN254 offers a security level of approximately 100 bits, which is considered sufficient for many practical applications. Additionally, it is already supported by Ethereum-like chains, making it a strong candidate for adoption. 2. L1 Enhancements / Additions: To enable zk-proof verification and interaction between L1 and L2, the following enhancements are proposed: 2.a. Low level EC operations: Additions to BTC like Kaspa-script language: ADD: Modular addition for elliptic curve field arithmetic. MUL: Modular multiplication for elliptic curve field arithmetic and scalar multiplications. PAIRING: Executes elliptic curve pairing operations, required for verifying Groth16 / Plonk proofs. Source of Opcode Implementations: Implementations for `ADD`, `MUL`, and `PAIRING` opcodes will be derived from the stable and widely adopted blockchains, such as Ethereum. Most stable implementations are zCash, Geth (and maybe reth) Ethereum\u2019s opcode implementations for BN254 are well-tested and optimized for performance, providing a reliable foundation. 2.b. Details about Groth16 / Plonk ZK-Verify-Function (executed by L1 nodes): The verify function will be implemented directly in terms of the BTC-like Kaspa scripting language, along with the verification key and state hash. The exact definition of public parameters can vary based on the application, but this design ensures flexibility and trustless execution across use cases. The verification process requires only two public inputs *** enabling support for practically any desired logic. The verify function can be defined with four arguments: Verification Key: Less than 1kB. Public Parameter 1: 256-bit. Public Parameter 2: 256-bit. zk-Proof: Less than 1kB. The verification itself involves dozens of elliptic curve (EC) operations (ADD, MUL, and PAIR) and can be efficiently implemented within a Bitcoin-like Kaspa script if these operations are supported as was proposed earlier in this post. *** In general, more than two public inputs can be supported; however, the number should always be bounded, as the verification complexity increases linearly with the size of the public inputs. 2.c. ZK-friendly hash function choice (partly finished, require separate blogpost) L1 must provide additional support for zk-based L2 systems to prove transaction ordering as observed by L1. This ensures consistency and enables verifiable state updates. zk-L2s will leverage Merkle Inclusion Proofs, using Merkle Roots computed by L1 to verify transaction inclusion and ordering without requiring direct access to the complete transaction history during L1 verification. L1 will construct a Merkle Tree, where each leaf corresponds to an individual transaction. The Merkle Root will represent the set of all transactions in a block, and will be computed by L1 with a zk-friendly hash function (e.g., Poseidon2, Blake2s, or Blake3) ensuring compatibility with zk systems. This structure is referred to as Recursive DAG ordering commitments in a previous post by @michaelsutton Hash Function Considerations Efficiency: L1 requires a highly efficient hash function optimized for standard CPUs, as it must be executed by all Kaspa nodes. zk-Friendliness: L2 requires zk-friendly hash functions to compute inclusion proofs for each L2 transaction during proof generation. Security: The selected hash function must ensure robust security across both L1 and L2. For example, while the Blake2s/Blake3 family of hashes is field-agnostic and suitable for L1 (cpu friendly), it is significantly less performant than zk-friendly options like Poseidon2, which offers better compatibility with zk-proofs but is finate field dependent. Blake2s / Blake3 zk performance currently about at least 20 times slower than this of Poseidon2, moreover major zkVMs should be extended with Blake hash as its precompile circuit that is not a big problem but still should be done accurately and properly audited ( which can delay mainnet L2s ) Most major zkVMs (such as SP1, CairoVM, Risc0, Jolt, and Nexus) would / should be compatible with the proposed choice of hash function. \u201cCompatible\u201d means prove generation performance or number of circuit constraints per hash per byte of hashed data etc \u2026 An alternative approach could involve selecting a hash function optimized for Groth16 / Plonk, but this comes with at least two significant drawbacks: Fixed Tree Size: Groth16 and Plonk require a fixed tree size. While this limitation is manageable, it imposes a strict proving interval, meaning the number of transactions in the tree must remain constant for single zk-proof. Performance Impact: A more critical issue arises because transaction order and inclusion would not be proven by the zkVM itself. Instead, these transactions would become public inputs for the second-stage proving system (i.e., the zkVM verifier implemented using Groth16/Plonk). This approach results in increased zk-proof generation times, negatively impacting performance. The relationship between hash function choice, cpu vs zk performance and the dependency on finite field will be explored in a future blog post. Sub-Nets Support To enhance scalability and parallel processing, transactions can be categorized into sub-nets. The Merkle Tree\u2019s leftmost leaf will store the Merkle Root of the previous block and its merge-set (all transactions), ensuring consistent chain linkage. 2.d. UTXO ( input / output ) Anchoring via KIP10 (so-called canonical bridge) By utilizing KIP10 introspection opcodes, UTXO-input and UTXO-output anchoring can be incorporated into the spending script. This mechanism enables any party to provide a valid zk-proof to spend a State UTXO, ensuring that the newly created UTXO complies fully with the rules defined by the L2 state transition logic. It guarantees verifiability and security of L2 state transitions in a fully trustless manner, as KIP10 enforces the creation of specific UTXO-outputs when spending UTXO-inputs. This topic will be explored in greater detail in a separate blog post. For now, it suffices to say that KIP10 facilitates the linking of UTXO-inputs and UTXO-outputs within the same transaction through commitments. Summary for L1 additions: Math OpCodes: ADD, MUL, PAIR - allow implementation of zk-verifier (aka OpZkVerify) Merkle-Tree structure with zk-friendly hash function - allow TX inclusion proofs and ordering during state transaction (open question: hash function choice) Open Question: A dedicated opcode for constructing a Merkle root from public inputs may be required\u2014this will be addressed separately. In any case, the aim is to minimize additions to L1. The current Bitcoin-like Kaspa script capabilities are sufficient to support basic hash composition of public inputs. However, for more advanced schemes, L1 may need to provide additional support in the future. 3. Possible L2 Implementation examples How to Store L2 State in L1 UTXO The ScriptPubKey will store a hash representing a specific verification function, including its verification key and the L2 state hash. Zk-proof public parameters act as commitments to the previous and next states, dag recursive commitments and output UTXO (and maybe input UTXO), ensuring that the spending process is fully trustless. The ScriptPubKey field in L1 UTXOs will store the following information related to L2 state: Recursive DAG ordering commitments Merkle root State-Hash: Represents the Merkle root of the L2 state Program-Hash: Specifies the zk-circuit or state transition program. Verification-Key: Enables zk-proof verification on L1 ZK-Verification code 3.a. Example of a Single zk-proof system: State transition programs can be implemented directly using Groth16 or Plonk proving systems. This approach is well-suited for smaller, predefined, or existing solutions, for example zCash ZK core is implemented in terms of this proving system and at least theoretically can be deployed as is. In such cases, a single zk-proof is generated and verified on-chain. 3.b Example of a zkVM based solution: Another potential implementation for L2 is a zkVM-generated proof of state transitions. A zkVM enables the efficient execution of arbitrary smart contracts or virtual machine code (e.g., EVM, SVM) with state transitions verified through zk-proofs. This approach ensures program correctness while maintaining efficiency in proof generation and verification. Leveraging zkVM technology offers significant development flexibility and numerous essential features. Currently, zkVMs represent the dominant approach in the industry. They are continually improving, becoming more user- and developer-friendly, as well as increasingly efficient. Furthermore, two-stage proving can be seamlessly extended to N-stage proving, as zkVMs are capable of supporting additional zk-schema implementations on top of their existing framework. zkVM Codebase Compatibility: This approach enables integration with major zkVM codebases, including Risc0, SP1, and CairoVM and others, ensuring broad interoperability and ecosystem growth. Example of using 2 zk-proof systems (zkVM\u2192Groth16/Plonk): zkVM (e.g., Risc0, CairoVM, SP1): This system will prove the execution of a state transition program (e.g., EVM) alongside transaction ordering and inclusion proofs. Groth16 / Plonk: This proving system will implement a zk-verifier for the zkVM. zkVM proof will be feeded inside Groth16 / Plonk circuit that implement zkVM verifier. This will generate another zk-proof that going to be verified on-chain by L1. It will generate a zk-proof that is sent on-chain and verified by all Kaspa L1 nodes. 3.c. Pseudocode for zk-SNARK Verification for Groth16. Constants - PRIME_Q (256 bits, 32 bytes): Prime field size for alt_bn128 (aka BN254) elliptic curve. Data Structures 1. G1Point (512 bits, 64 bytes) - Represents a point on the curve with: - x (256 bits, 32 bytes). - y (256 bits, 32 bytes). 2. G2Point (1024 bits, 128 bytes): - Represents a point on the extension field with: - x[0], x[1] (each 256 bits, 32 bytes). - y[0], y[1] (each 256 bits, 32 bytes). 3. VerifyingKey (640 bytes): - alfa1 (G1Point, 64 bytes). - beta2 (G2Point, 128 bytes). - gamma2 (G2Point, 128 bytes). - delta2 (G2Point, 128 bytes). - ic (array of G1Points, size depends on the number of public inputs + 1; each G1Point = 64 bytes). 4. SnarkProof (192 bytes): - a (G1Point, 64 bytes). - b (G2Point, 128 bytes). - c (G1Point, 64 bytes). Elliptic Curve Math 1. P1 and P2: - P1 (512 bits, 64 bytes): Generator point for G1. - P2 (1024 bits, 128 bytes): Generator point for G2. 2. Negate Point: - Input: G1Point (64 bytes). - Output: Negated G1Point (64 bytes). 3. Addition: - Inputs: Two G1Point values (each 64 bytes). - Output: Resultant G1Point (64 bytes). 4. Scalar Multiplication: - Inputs: G1Point (64 bytes), scalar s (256 bits, 32 bytes). - Output: Scaled G1Point (64 bytes). 5. Pairing: - Inputs: Arrays of G1Point and G2Point (each G1Point = 64 bytes, each G2Point = 128 bytes). - Output: Boolean (1 byte). 6. Pairing Check: - Input: Four pairs of G1Point and G2Point (4 x 64 bytes + 4 x 128 bytes = 768 bytes). - Output: Boolean (1 byte). Verification Logic (logic that will be supported directly by L1) 1. Verify Function: - Inputs: - vk (Verification Key, 640 bytes). - input (Array of public inputs, each 256 bits, 32 bytes; total size = number of inputs \u00d7 32 bytes). - proof (SNARK proof, 192 bytes). - Steps: 1. Input Validation: - Ensure input.length + 1 == vk.ic.length. - Ensure all input[i] values are less than FIELD_SIZE (256 bits, 32 bytes). 2. Compute vk_x: - Start with vk_x as G1Point (0, 0) (64 bytes). - For each public input: - Scale vk.ic[i + 1] (64 bytes) by input[i] (32 bytes) using scalar_mul. - Add the result to vk_x (64 bytes) using addition. - Add vk.ic[0] (64 bytes) to finalize vk_x. 3. Pairing Check: - Use pairingProd4 (extended pairing function for 4 arguments) to verify the pairing: - Inputs: Negated proof a (64 bytes), proof b (128 bytes), vk.alfa1 (64 bytes), vk.beta2 (128 bytes), vk_x (64 bytes), vk.gamma2 (128 bytes), proof c (64 bytes), and vk.delta2 (128 bytes). - Output: Boolean (1 byte). - Output: - Returns true (1 byte) if the proof is valid, false (1 byte) otherwise. Summary of Sizes 1. Key Sizes: - Verification Key: 640 bytes. (example for 2 public inputs) - Public Inputs: Each 256 bits, 32 bytes. - SNARK Proof: 192 bytes. 2. Operations: - Addition (G1Point): Input = 64 bytes \u00d7 2, Output = 64 bytes. - Scalar Multiplication (G1Point): Input = 64 bytes + 32 bytes, Output = 64 bytes. - Pairing Check: Input = 768 bytes, Output = 1 byte.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2024-12-12T14:44:57.329Z",
      "updated_at": "2024-12-12T20:46:09.126Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/1",
      "category_id": 11
    },
    {
      "post_id": 359,
      "post_number": 2,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Are any of the zk-friendly hash functions considered sufficiently safe by cryptanalysts? And is there a reasonable chance that some new zkp system variant would require choosing a different hash function?",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-13T04:54:10.344Z",
      "updated_at": "2024-12-13T04:54:10.344Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/2",
      "category_id": 11
    },
    {
      "post_id": 390,
      "post_number": 3,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Re sufficiently safe: Each of zk-friendly hashes should be addressed separately. For example Poseidons family functions, are widely used by zk community in production projects and by zk-community it considered sufficiently safe, but by wider cryptographers / research / cryptanalysts community, the question reminds open. I think for proper answer to this question we need help of @proof & Elihai. And if my understanding is correct, re Blake family hash functions are considered to be safe ! Re zkp system variant would require choosing a different hash function: The short answer, most probably yes. Slightly more unfolded answer: for example for hash function like Poseidon, which is finite field dependent, it is clear that each zk system will prefer that chosen Poseidon hash to be based on its (zk-system) native finite field. This way the performance will be better since it will be highly aligned with specific proving system. This is one of the reasons we need to choose it carefully. I will try to address these things in separate post. Blake family on the other hand can be presumably \u201cgood\u201d for all / most of existing zkp systems, but not the best. Other perspective is the CPU friendliness, which is good for Kaspa-L1, that should be considered as well. To summarize: the final choice is multi dimensional and part of the requirements contradict with each other.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2024-12-16T11:10:30.362Z",
      "updated_at": "2024-12-16T11:12:27.002Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/3",
      "category_id": 11
    },
    {
      "post_id": 395,
      "post_number": 4,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Regarding hashes (after consulting with smart folks): Preliminary remark: \u201czk-friendliness\u201d of a hash function is not a purely intrinsic property - it depends on design/efficiency of the underlying AIRs (at least in the context of STARKs). AIR efficiency can vary by orders of magnitude, and of course AIR design depends on the particular proving protocol and implementation (it\u2019s a bit of an art). To start with some names, Blake, Pedersen, and Poseidon are all used in production rollups (e.g Starknet). Blake (like keccak and sha256) consists of many bitwise operations so it\u2019s very execution friendly. From the zk perspective, bitwise operations are only native to binary fields, which are not commonly used at the moment (neither for EC nor for STARKs). Hence Blake isn\u2019t zk-native. As remarked above, zk-friendliness depends on the ambient proving protocol and implementation. There\u2019s a very efficient AIR for the Stwo prover for STARKs over Mersenne 31. Pedersen (roughly) takes two felts, concatenates them, and returns the associated multiple of some EC generator. It\u2019s relatively zk-friendly if your proving protocol works over the same field. Pedersen lacks entropy in small fields such as Mersenne 31 and consequently isn\u2019t secure in the naive approach. On the other hand, proving is much more efficient over small fields. There\u2019s a middle ground if you use ECs over an extension field, but then you\u2019re losing proving efficiency due to the overhead of representing EC ops using base felts. Note EC ops are also relatively heavy to execute. Poseidon takes a vector of felts and returns another via sequences of matrix multiplications and pointwise powers and additions. Hence it\u2019s very execution-friendly and also quite zk-friendly assuming efficient AIRs. It also has plenty of entropy so it can be used over smaller fields. If you want a secure zk-friendly hash compatible with small fields, I think this is the best option. I\u2019d choose between Blake and small-field Poseidon depending on whether you want to optimize for execution time or proving time.",
      "raw_content": "",
      "author": "proof",
      "created_at": "2024-12-19T13:54:23.515Z",
      "updated_at": "2024-12-19T13:57:10.296Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/4",
      "category_id": 11
    },
    {
      "post_id": 399,
      "post_number": 5,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Thanks for this info @proof! Optimizing for execution of L1 ops should be primary over prover efficiency, so let\u2019s aim for Poseidon, unless someone provides new arguments. Poseidon over what field do you recommend we use? (Elichai, any chance to get your input on Poseidon safety?)",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-22T08:37:28.784Z",
      "updated_at": "2024-12-22T08:37:28.784Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/5",
      "category_id": 11
    },
    {
      "post_id": 401,
      "post_number": 6,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "@hashdag, @proof indeed suggested that Poseidon takes \u2026 hence it\u2019s very execution-friendly (where by \u201cexecution-friendly\u201d I\u2019m assuming he means L1 friendly), however afaik from @reshmem it is still an ~order of magnitude more expensive to compute than Blake-class hash functions. So it seems like we need exact benchmark numbers before making any conclusion here?",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-12-22T13:44:02.976Z",
      "updated_at": "2024-12-22T13:44:02.976Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/6",
      "category_id": 11
    },
    {
      "post_id": 402,
      "post_number": 7,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Sorry, my post was unclear. Blake will probably be faster to run on x86 than Poseidon (even on a small field). Perhaps the execution time will be comparable if the field is sufficiently small, in which case Poseidon will be the best of both worlds (as it\u2019s much more efficient to prove with Circle STARKs over a small field). In my opinion such a core level decision warrants benchmarks.",
      "raw_content": "",
      "author": "proof",
      "created_at": "2024-12-22T15:52:19.405Z",
      "updated_at": "2024-12-22T15:52:19.405Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/7",
      "category_id": 11
    },
    {
      "post_id": 405,
      "post_number": 8,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Here are benchmarks (CPU not ZK) for Blake3 & Poseidon2. Apple MacBook Pro M4, single threaded. chatgpt only helped to organise the results in table telegram-cloud-photo-size-4-5999079653719000034-y1071\u00d7833 93.2 KB Few comments: Blake3 perf stays the same for 32 bytes and 64 bytes input since pre-image length of Blake3 is 64 bytes Small input as 64bytes is still important for Kaspa L1 since during Merkle Tree construction ~1/2 hashes will be with 64bytes input For Poseidon2 bench I used Plonky3 lib. Blake3 is blake3 v1.5.3 (official rust implementation) These are preliminary results, that maybe good enough for discussion but still far from being extremely accurate. Question: @proof - do I understand it correctly that number of AIR constraints for Blake3 for n-rounds = 7, is 2976 + 900 ~= 4k ? *taken from the link provided by you.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2024-12-23T20:47:25.975Z",
      "updated_at": "2024-12-24T08:27:34.606Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/8",
      "category_id": 11
    },
    {
      "post_id": 406,
      "post_number": 9,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Am I correct in my understanding that groth and plonk will both require a trusted set-up phase for the L2s? I consider it desirable to allow L2 to avoid trusted set ups if they so choose, even if it comes at the price of something else. Do the suggested op codes allow for Starks and (best) transparent snargs to be implemented?",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2024-12-24T13:43:39.350Z",
      "updated_at": "2024-12-24T13:43:39.350Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/9",
      "category_id": 11
    },
    {
      "post_id": 413,
      "post_number": 10,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "@proof In continuation to our conversation, could you describe here the different methods you see in which Starks can be integrated?",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-04T21:46:52.348Z",
      "updated_at": "2025-01-04T21:46:52.348Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/10",
      "category_id": 11
    },
    {
      "post_id": 440,
      "post_number": 11,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Zero-knowledge proof is a rapidly developing technology. Even for Plonky, there are multiple versions like Plonky2, Plonky3, and FFLONK. In Ethereum, this can be implemented through smart contract updates. How does Kaspa ensure that it can quickly support more advanced proof systems when they become available on the market? Or must it undergo another hardfork?",
      "raw_content": "",
      "author": "Dash",
      "created_at": "2025-01-19T14:40:02.682Z",
      "updated_at": "2025-01-19T14:48:19.647Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/11",
      "category_id": 11
    },
    {
      "post_id": 465,
      "post_number": 12,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Indeed both Plonk & Groth16 require trusted-setup ceremony, but not for L2s. For each version of zkVMs in case of chain of 2 proof systems. If L2 decides to implement its logic directly in terms of Plonk/Groth16 then yes.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T12:09:43.679Z",
      "updated_at": "2025-01-20T20:35:27.112Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/12",
      "category_id": 11
    },
    {
      "post_id": 466,
      "post_number": 13,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "We\u2019re implementing the EC curve ops which will allow sufficient flexibility without direct need for hard-forks.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T12:12:37.520Z",
      "updated_at": "2025-01-20T12:12:37.520Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/13",
      "category_id": 11
    },
    {
      "post_id": 481,
      "post_number": 15,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "reshmem: We\u2019re implementing the EC curve ops which will allow sufficient flexibility without direct need for hard-forks. this is an assumption of a EC-based zk system? hash-based zk system would still be a hard fork?",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-23T20:55:26.788Z",
      "updated_at": "2025-01-23T20:55:26.788Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/15",
      "category_id": 11
    },
    {
      "post_id": 482,
      "post_number": 16,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Not in current construction. Any zk-verify function (hash-bases, EC) can be implemented in terms of EC-based Math-Opcodes ( Groth16 / Plonk).",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-24T11:57:11.071Z",
      "updated_at": "2025-01-24T11:57:11.071Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/16",
      "category_id": 11
    },
    {
      "post_id": 484,
      "post_number": 17,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "reshmem: can be implemented in terms of EC-based Math-Opcodes ( Groth16 / Plonk). is there secondary proving going on? \u201cWhile traditional zk-SNARKs rely on cutting-edge cryptographic hard problems and assumptions, the only cryptographic ingredient in a STARK proof system is a collision-resistant hash function.\u201d Anatomy of a STARK Anatomy of a STARK, Part 0: Introduction STARK tutorial with supporting source code in python.",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-24T14:42:11.717Z",
      "updated_at": "2025-01-24T14:42:11.717Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/17",
      "category_id": 11
    },
    {
      "post_id": 487,
      "post_number": 18,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Dear @superMainnet - can I kindly ask you to re-read the whole blogpost ? Key Points to Note A chain of zk proofs (STARK/any other new proof system \u2192 SNARK) leverages the best of both worlds: \u2022 STARK for efficient, transparent proof generation of large-scale computations. \u2022 SNARK for cost-effective on-chain verification (small proof size, constant-time checks). This architecture is increasingly discussed in modern zero-knowledge ecosystems, as it alleviates the on-chain verification bottleneck of large STARK proofs while still enjoying the transparency and scalability that STARKs offer off-chain. Proof Size STARK proofs grow (roughly) polylogarithmically/log with circuit size, which is still quite efficient for large n (especially with continual optimizations). Groth16 has some of the smallest proof sizes (~128\u2013192 bytes) and constant verification time, but each circuit needs its own trusted setup. PLONK typically produces a constant-size proof \u2014 often a few hundred bytes. This is larger than Groth16\u2019s but still considered \u201csmall\u201d in absolute terms. Verification Complexity STARK verification is generally O(log n) or a small polynomial in log n. Groth16 verification is constant (independent of n)\u2014only a handful of pairings (e.g., 3 pairings total). PLONK also has a constant number of pairings/polynomial checks and is therefore constant time in circuit size. Trusted Setup STARK systems avoid a trusted setup entirely (fully transparent). Groth16 requires a circuit-specific setup (each new circuit needs a new setup). PLONK uses a universal (or updatable) setup that can be reused across many different circuits. Security Assumptions STARK: Relies on collision-resistant hash functions and no heavy number-theoretic assumptions\u2014often touted as plausibly or partially post-quantum secure, though current standardization is still evolving. Groth16 and PLONK: Rely on elliptic-curve pairings and discrete-log assumptions, which are not believed to be secure against large-scale quantum computers. In summary, STARKs offer transparency and good asymptotic scalability but come with larger proof sizes. Groth16 remains popular for minimal proof sizes and fastest verification, while PLONK offers a middle ground of small (but slightly larger) proofs, constant verification, and a single universal trusted setup that simplifies deployment across many circuits. Observations Tens to Hundreds of Kilobytes \u2022 Real-world STARK proofs often land in the tens/handreds of kilobytes range for moderate-scale applications (batching thousands of trades, or verifying mid-sized programs). \u2022 Even not extremely large circuits (millions of constraints) can push proofs into the **hundreds of kilobytes / megabytes **. Ongoing Optimizations \u2022 Projects like StarkWare continue to optimize the FRI protocol, polynomial commitment schemes, and Cairo\u2019s AIR (algebraic intermediate representation). These improvements may reduce proof sizes further or keep them manageable as circuit complexity grows. Comparison to Groth16 / PLONK \u2022 STARK proofs are significantly larger than the constant-sized proofs typical of Groth16 or PLONK (a few hundred bytes to sub-kilobyte). \u2022 In exchange, STARKs offer transparency (no trusted setup) and have good scaling properties for very large circuits. Here is a recent comparison for zkVMs ( SP1, risc0 are stark-based ). image887\u00d71001 56.5 KB In addition, stark-based verifiers are not easily implementable with BTC-like kaspa scripting language, hence: Each version of such verifier per specific version of zk-VM requires a hard-fork Kaspa L1 should commit upfront to specific zkVMs which is not the case with basic EC math op-codes. Math op-codes are not in any sense are equal to this. zkVM codebases are extremely complex and new, with ton of bugs inside and hundreds of thousands lines of code. They are safe in a way that people put real money in these L2s / zk-rollups but solid chains like Ethereum still are far from accepting it inside ( requirement is at least formal verification of code vs math ) And many more other \u2026 \u201cbuts\u201d \u2026",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-26T09:45:38.766Z",
      "updated_at": "2025-01-26T09:49:52.461Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/18",
      "category_id": 11
    },
    {
      "post_id": 488,
      "post_number": 19,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "Just to put here for the reference EIPs related to these math-ops: EIP-196: Precompiled contracts for addition and scalar multiplication on the elliptic curve alt_bn128 EIP-197: Precompiled contracts for optimal ate pairing check on the elliptic curve alt_bn128 EIP-1108: Reduce alt_bn128 precompile gas costs Here is in review status of EC ops related to BLS12-381: EIP-2537: Precompile for BLS12-381 curve operations Here is a ton of information about ECs accepted in Ethereum, why and how and why others were rejected \u2026 https://ethresear.ch https://ethereum-magicians.org",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-26T13:21:57.998Z",
      "updated_at": "2025-01-26T13:34:47.364Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/19",
      "category_id": 11
    },
    {
      "post_id": 492,
      "post_number": 21,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "reshmem: A chain of zk proofs (STARK/any other new proof system \u2192 SNARK) leverages the best of both worlds: the wall of ai generated background info is appreciated, but in the interest of time a simple \u201cyes\u201d would have saved me 20 minutes superMainnet: is there secondary proving going on?",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-28T16:22:54.145Z",
      "updated_at": "2025-01-28T16:23:14.616Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/21",
      "category_id": 11
    },
    {
      "post_id": 493,
      "post_number": 22,
      "topic_id": 219,
      "topic_title": "Additional practical considerations re hash function and zk opcodes",
      "topic_slug": "additional-practical-considerations-re-hash-function-and-zk-opcodes",
      "content": "fyi groth16 seems to be falling out of favor in the Ethereum community x.com vitalik.eth (@VitalikButerin) on X @VitalikButerin One of the many things that we need to acceeeeeeelerate is abolishing groth16 Per-application trusted setups are just not ok in the 2020s. Universal setup at the minimum, ideally no setup at all This requires big improvements on infra and standardization for newer SNARK algos x.com Mauro Toscano \ud83d\udfe9 (@MauroAligned) on X @MauroAligned It's incredible that Groth16 is still widely used today. It's not the fastest, requires a trusted setup, it's not even universal, and most of the new tech have to do a lot of work to convert from their proving system to it. But not a surprise, since it's the cheapest proving",
      "raw_content": "",
      "author": "superMainnet",
      "created_at": "2025-01-28T16:26:15.555Z",
      "updated_at": "2025-01-28T16:26:15.555Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/additional-practical-considerations-re-hash-function-and-zk-opcodes/219/22",
      "category_id": 11
    },
    {
      "post_id": 505,
      "post_number": 1,
      "topic_id": 294,
      "topic_title": "Sync Monitor / Mining Rule Engine",
      "topic_slug": "sync-monitor-mining-rule-engine",
      "content": "Introduction This forum post is a summary of some of the discussions with @michaelsutton @hashdag and @someone235 which can be found at Sync Monitor / Mining Rule Engine - Google Docs and Telegram: Contact @kasparnd Here we describe two pieces of related work: Creating a sync monitor/manager such that nodes can continue mining during a network halt under certain good conditions. This monitor answers the question \u201cShould the node mine?\u201d Creating a mining rule engine that allows mining behavior to adjust to conditions of the node or the network. This rule engine answers the question \u201cHow should this node mine?\u201d Sync Monitor This monitor determines whether a node should mine given its observable current state. The rusty kaspa node allows for mining if (1) the node has at least one peer and if (2) the sink is recent. These conditions are sufficient for when the network is operating normally - that is, the network is producing blocks. What happens if network mining halts for a significant period of time to the point all nodes fail the \u201cis sink recent?\u201d condition? To recover from this halted state, there will have to be some manual intervention for mining to continue. We know the network is operating normally if it is producing blocks. We also know what the expected number of blocks there are per second (1 block per second in mainnet, but soon to become 10 BPS). Using these two bits of information, we can determine just how fast a node\u2019s sync rate is. Specifically: sync_rate = (observed_blocks_per_second) / (expected_blocks_per_second) We can formally define that the \u201cnetwork is operating normally\u201d if the sync_rate is above some threshold. If it falls below this threshold, then regardless of the recency of the sink, the node should mine bring the sync_rate back up. So, the proposed updated rule a sync monitor is as follows: The node is connected to at least one peer AND (The sink is recent OR the sync rate is below threshold) With this rule, we cover the scenario where the network halts. If you can think of more scenarios that should be covered, please feel free to suggest it below. Mining Rule Engine This rule engine determines how a given node should mine. It will be a new addition to the node so that it can attempt to recover from some scenarios, such as performance degradation, that are encountered through the course of a node\u2019s operation. As this is new functionality, I will only describe some of the ideas for scenarios we may care about and some possible recovery methods. Recovery Methods Consider pointing only to blue blocks Useful when header processing takes longer than some threshold (like 0.5sec) and there are constantly red blocks in the mergeset which is a hint that these red blocks might be the cause of the perf issue. Consider mining empty blocks (no transactions) Useful for mining blocks if something about the transactions is what is causing the error such as with the BadMerkleRoot errors during this freeze: Kaspa 20 minute BlockDAG freeze post-mortem | by Ori Newman | Medium. This post-mortem also mentions this recovery method towards the end. Mining Scenarios Scenario Mining Behavior Node is functioning normally Node will mine with the standard behavior (as it works today in mainnet) Node is in IBD and the sink timestamp is recent enough Node will mine with the standard behavior (as it works today in mainnet) Node is in IBD and the sink timestamp is not recent enough No mining is allowed Node is receiving multiple blocks that fail validation with BadMerkleRoot If the node continuously receives ONLY blocks that have BadMerkleRoot error, switch to mining only empty blocks (no transactions). Network mining has halted Considering the updated sync monitor logic above to track sync rate, this will be a completely unexpected scenario and will have to have manual intervention. Too many red blocks being merged causing performance degradation Allow a maximum of only 1 red block to be merged (or do not merge any red blocks) until node performance settles down. I encourage the reader to propose more scenarios that may need to be handled or recovery methods to handle such scenarios.",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2025-02-01T06:12:36.334Z",
      "updated_at": "2025-02-01T07:26:15.024Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/sync-monitor-mining-rule-engine/294/1",
      "category_id": 7
    },
    {
      "post_id": 504,
      "post_number": 1,
      "topic_id": 293,
      "topic_title": "Thoughts about covenant and async message standardization",
      "topic_slug": "thoughts-about-covenant-and-async-message-standardization",
      "content": "The canonical bridge post essentially suggested a way to authenticate that a UTXO belongs to some \u201ccovenant chain\u201d. A natural continuation of this would be to standardize the idea of a covenant and to allow authenticating it with some identity, in a way which is authorized by L1. One use-case for such standardization would be async message passing between covenants/rollups where L1 will certify the sender identity. It could also simplify the implementation of a delegation script such as the one described in the post. A reoccurring theme of covenant scripts is that they maintain some static part of the script (e.g., CANON, PROG) throughout time, while other parts mutate and represent the covenant dynamic state. I propose that a covenant script will contain a script-header specifying a mask which will allow to extract the static part of the script. This can be done by specifying masking ranges (e.g., ignore bytes 22-45) or by inserting special markers within the script itself (the latter is useful if the dynamic parts are also dynamic in size). Another complementing operation would be hashing the header/markers along with the static parts they signify, such that any script following the covenant will hash to the same \u201ccovenant identifier\u201d. I can see two immediate use-cases for such a mechanism. Async message passing: a covenant transaction (proof transaction in the rollup context), can contain a payload marked with a special SYSTEM prefix indicating that L1 authorized this payload. The payload will include a sender field with the sender id, and the message itself. L1 will verify (as a transaction acceptance rule), that the sender id corresponds to the standard covenant identifier of one of the spent input scripts. This means that other rollups interested in messages from this sender will only need to follow such messages (as long as they performed a one-time check/audit verifying that the underlying covenant is well-formed and cannot be forged) Delegation scripts: the standardization essentially captures the notion of \u201cwell-formed\u201d defined in the linked post. The DELEGATE script can thus be simplified to verify that the primary input has the desired covenant identifier.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-31T12:53:42.823Z",
      "updated_at": "2025-01-31T13:52:33.664Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/thoughts-about-covenant-and-async-message-standardization/293/1",
      "category_id": 11
    },
    {
      "post_id": 463,
      "post_number": 1,
      "topic_id": 274,
      "topic_title": "Subnets sequencing commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "Context/prerequisite: Based rollup design Notation glossary: H - a hash function, e.g. SHA256, or blake, (see discussion here re zk-friendly hashes) MT(x_1,x_2...,x_k) - the Merkle tree derived from x_1,...,x_k, with some implicit hash function H MR(x_1,x_2...,x_k) - the root of the above Merkle Tree SQC(B) - the \u201csequencing commitment\u201d field of block B B.sp - the selected parent of the block B Introduction In Kaspa\u2019s based rollup design we suggested introducing a new field, which serves as a sequencing commitment (previously referred to as the history root). The new field is recursively defined as follows: let B be a block, and let t_1...t_n be all of B's accepted transactions (more accurately, the tx-hash or tx-id of those, to be determined at a later stage), in the order they were accepted in. Ignoring initialization of the first block in the smart contracts hardfork, the new field SQC(B) is: SQC(B)=H(SQC(B.sp),t_1, t_2...t_n),^1 as illustrated below: image1209\u00d7679 149 KB To prove the progression of this field between a selected chain block Src and a selected chain block Dst matches the progression of the rollup, the prover requires all txs that were accepted between both of them as private inputs - inputs that the prover of a statement requires to produce a proof, but the verifier of that proof does not. More formally, these are all txs that were accepted on a selected chain block in the future of Src and in the past of Dst, including Dst itself. This scheme is perfectly functional, but it has the following inefficiency: a prover of a certain rollup, is really only interested in those txs that are directed to their rollup. All other txs, and any block that accepted no tx associated with this rollup, are \u201cnoise\u201d from their point of view. Yet in order to prove they are not associated with the rollup, provers are forced to take those as private inputs, increasing their runtime complexity. In particular, if a rollup is only active once every 5 minutes, it would still have to go through the entire selected chain and all transactions accepted on it. I explore below a method to represent a particular rollup history in a more concise manner. Separate Subnet Histories A rollup is represented by a field in the transaction called subnetId, and we refer to each rollup as a \u201csubnet\u201d. Our suggestion (courtesy of @michaelsutton) is that SQC have an inner structure representing the different subnets\u2019 histories. For a block B, and for each subnet i, let t_{i_1}...t_{i_k} be those transactions within B's accepted txs t_1, t_2...t_n that belong to subnet i. We say a subnet i is active at block B, if there ever in history was an accepted transaction belonging to subnet i prior or at block B (this permanent definition of activeness will be relaxed below). We can recursively define an abstract field: if block B accepted no txs of subnet i this field takes the same value as its selected parent, i.e. SQC_i(B)=SQC_i(B.sp). Otherwise, SQC_i(B)=H(SQC_i(B.sp),t_{i_1},...t_{i_k}), or simply SQC_i(B)=H(t_{i_1},...t_{i_k}), if SQC_i(B.sp) was previously undefined. Lastly, SQC(B)=MR(SQC_i(B)|\\textit{i is active as of block } B), meaning the Merkle root of the sequencing commitments of all active subnets. The prover of subnet i now only requires the accepted txs between Src and Dst that belong to subnet i, as well as witnesses to prove the inclusion of SQC_i(Src), SQC_i(Dst) in SQC(Src) and SQC(Dst). These two witnesses are of logarithmic size in the number of subnets active in the network. This prover now has O(A_i+\\log R) private inputs, where A_i is the number of txs in subnet i at the span in between Src and Dst, and R is the number of active subnets. For some concreteness, with current parameters, \\log R is at most 160. The solution as is requires each L1 node to keep track of and compute SQC_i for all R subnets that are active, and subnets never become deactivated. Effectively this creates a \u201cregistry\u201d of subnets. Thought must be devoted to prevent unlimited growth of this registry or spamming attacks. I propose implementing pruning of the subnets: nodes will also store the last (selected chain) DAA score at which subnet i was updated, and subnets will only be considered active if the current DAA score is less than finality-depth greater than the last update on the subnet. If subnets become indisputably inactive, their stored data can be thrown away completely during pruning. This ensures a bound on the number of active subnets at any given time. Naive use of Merkle trees may result in recalculation of the entire tree at every block. To prevent this constant recalculation, I advocate using sparse Merkle trees. For ease of syncing and general data tracking, SQC_i(B) should be updated to explicitly contain the last DAA score subnet i was updated at, e.g., SQC_i(B)=H(DAA(B),H(SQC_i(B.sp),t_{i_1},...t_{i_k})) whenever an update occurs. A sketch of the structure of subnets history is provided below: image1196\u00d7698 172 KB The pruning means that a subnet should make at least a single tx every finality-depth blocks if it wishes not to be pruned. Finality depth is planned to be about half a day after the 10bps hard fork. It appears to me like a reasonable requirement of a subnet to demonstrate its activity by issuing a tx every 12 hours. Nevertheless, there is value in allowing a fallback mechanism in case that for whatever reason a subnet has been pruned prematurely. Hence I suggest we add another branch to SQC(B)'s sparse Merkle tree, called SQC_{TOT} (B)=H(SQC(B.sp),t_1,...,t_n) which stores the total history as originally proposed, without division to subnets and without pruning. That way pruned subnets could still supply proofs - at the cost of waiving all optimizations.^2 Another advantage of maintaining this divisionless branch, is that it maintains the original ordering of the accepted txs in their entirety, which may be required for some applications that choose to track all txs. [1] Previously @michaelsutton suggested H(MR(SQC(B.sp),t_1, t_2...t_n)). The current reasoning to use a Merkle tree instead of a cumulative hash mostly reduces to overloading the new field with the responsibility previously kept by the Accepted Transactions Merkle Tree field, or possibly, the responsibilities of the Pchmr field proposed in KIP6 - if we later choose to implement a \u201clogarithmic jumps\u201d lookout into history instead of the linear one described here. This kind of decisions are to be made at a later stage and I see fit to decouple them from the subject at hand. [2] It is worth emphasizing that while a proof end point must refer to a relatively recent block for it to be valid, its starting point can be arbitrarily old.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-20T11:22:14.519Z",
      "updated_at": "2025-01-20T11:32:28.541Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/1",
      "category_id": 11
    },
    {
      "post_id": 464,
      "post_number": 2,
      "topic_id": 274,
      "topic_title": "Subnets sequencing commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "I think that some L2s will be interested to know the order between 2 transactions from 2 different subnets. SQC-TOT will allow to extract this information as well, but maybe it worth considering to extend definition of the leaves to be subnet transaction and global sequence number ( sequence number of the transaction in the global acceptance order in this block ).",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T12:04:33.393Z",
      "updated_at": "2025-01-20T12:04:33.393Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/2",
      "category_id": 11
    },
    {
      "post_id": 470,
      "post_number": 3,
      "topic_id": 274,
      "topic_title": "Subnets sequencing commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "You\u2019re right, that\u2019s probably the better way to go. For the fallback mechanism we only really need SQC(B.sp) rather than anything else, and hashing the global index alongside the tx will prevent the need to hash all txs twice.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-20T15:08:10.221Z",
      "updated_at": "2025-01-20T15:08:10.221Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/3",
      "category_id": 11
    },
    {
      "post_id": 471,
      "post_number": 4,
      "topic_id": 274,
      "topic_title": "Subnets sequencing commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "FreshAir08: Naive use of Merkle trees may result in recalculation of the entire tree at every block. To prevent this constant recalculation, I advocate using sparse Merkle trees. @FreshAir08 - can you please unfold this a bit \u2026 I can see how SMT helps for proof of non-inclusion ( very useful in some zk cases \u2026), but where / how SMT will decrease CPU side computation done by Kaspa L1 ?",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-20T21:09:24.676Z",
      "updated_at": "2025-01-20T21:09:24.676Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/4",
      "category_id": 11
    },
    {
      "post_id": 500,
      "post_number": 5,
      "topic_id": 274,
      "topic_title": "Subnets sequencing commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "Following the conversation, I\u2019d like to make a final proposal/recap combining all: hierarchic subnets; subnet expiry; global mergeset sequence number and elements of KIP-6. I\u2019ll use \\psi, \\psi_i as short for SQC, SQC_i. Notation: mergeset(B) := [t_1, ..., t_n] denotes the sequence of n txs accepted by block B in order for each subnet i, let t_{i_1}...t_{i_k} be those transactions within mergeset(B) that belong to subnet i. Note that i_j is the original index within mergeset(B) of the j'th subnet tx. we (abuse notation and) say i \\in mergeset(B) iff there exists a tx in mergeset(B) which belongs to subnet i B.sp_i is the i'th selected parent starting from B. I.e., B.sp = B.sp_1, (B.sp).sp = B.sp_2 and so on. Denote expiry(i, B) := bluescore(B) - bluescore(B.sp_m); where B.sp_m is the most recent chain ancestor of B such that i \\in mergeset(B.sp_m); or genesis if no such block. Definition 1: \\psi_i(B) := \\psi_i(B.sp); iff i \\notin mergeset(B) H(\\psi_i(B.sp), (t_{i_1}, i_1), ..., (t_{i_k}, i_k)); otherwise Definition 2: \\psi(B) := MR(\\psi(B.sp_1), \\psi(B.sp_2), ..., \\psi(B.sp_{2^m}), \\psi_i(B) \\text{ }| \\text{ for all } i \\text{ s.t. } expiry(i, B) \\le F) where F is some expiry constant, and m is set in a way detailed later. The above definition of \\psi(B) accomplishes the following: Subnets can prove their execution in O(\\text{subnet activity}) time as long as there\u2019s at least a single subnet tx every F epoch (nit: each proof requires additional O(log(\\text{#non-expired-subnets}) + log(\\text{mergeset-tx-limit}))) The size of the maintained subnet tree (supporting the construction of \\psi(B)) is bounded by the number of non-expired subnets which in turn is bounded by F\\cdot \\text{mergeset-tx-limit} (assuming a worst-case scenario where each tx in the epoch belongs to a unique subnet). By setting F\\approx \\text{pruning-period-len} we get a balance where tree storage requirements are only a fraction of the storage required for keeping header and transaction data in the pruning period on the one hand, while maintaining efficient subnet proving requires only a single tx per F epoch on the other hand (which can be thought of as minimal tax paid by any reasonable non-spam subnet). The addition of the global indices i_1, ..., i_k hashed within def. 1 allows proving cross-subnet DAG order relations. The series \\psi(B.sp_1), \\psi(B.sp_2), ..., \\psi(B.sp_{2^m}) embedded within \\psi(B) is set such that B.sp_{2^{m+1}} would be below the previous pruning/posterity point (see KIP-6 by @Deshe2). This construction can be used to prove arbitrary transaction acceptance in the L1 DAG throughout history with the same \u03b8(log(N)loglog(N)) complexity suggested in the KIP (technically this might require switching H to MR in def. 1). Note that we do this without going through the header-chain, thus not providing PoChM explicitly, nor the possibility to prove transaction inclusion. The mention of using SMR\u2019s @reshmem is not in relation to non-inclusion proofs but rather for efficiently managing a mutating key-value tree with log depth and with in-consensus semantics. The key here is the subnet id and \\psi_i(B) is the mutating value. Keeping a dense tree for all active (non-expired) subnets, would require mutations and tree rebalancing strategies which are hard to form in-consensus. Reinvestigating this, I guess we can also use a Patricia Tree (with another hashing layer over keys) which might provide better density tradeoffs.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-30T15:18:12.735Z",
      "updated_at": "2025-01-31T11:46:30.681Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/5",
      "category_id": 11
    },
    {
      "post_id": 503,
      "post_number": 6,
      "topic_id": 274,
      "topic_title": "Subnets sequencing commitments",
      "topic_slug": "subnets-sequencing-commitments",
      "content": "michaelsutton: Subnets can prove their execution in O(\\text{subnet activity}) time as long as there\u2019s at least a single subnet tx every F epoch I noticed that expired subnets can prove inactivity by providing a non-inclusion proof every F epoch. I.e., by showing subnet i is not included in \\psi(B), you essentially prove there was no activity for the F epoch prior to B. So a subnet inactive for T time will only require O(T/F) proof steps.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-31T11:58:41.571Z",
      "updated_at": "2025-01-31T11:58:41.571Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/subnets-sequencing-commitments/274/6",
      "category_id": 11
    },
    {
      "post_id": 429,
      "post_number": 1,
      "topic_id": 258,
      "topic_title": "L1<>L2 canonical bridge (entry/exit mechanism)",
      "topic_slug": "l1-l2-canonical-bridge-entry-exit-mechanism",
      "content": "Context/prerequisite: part 1 \u2013 based rollups design. A primary objective of the based rollup design is to support the entry and exit of L1 KAS funds to and from the L2\u2014a mechanism often referred to as a canonical bridge. The end result should be a bridged KAS token (henceforth referred to as BKAS), which can be used on L2 and has a 1:1 relation with native L1 KAS. From the perspective of L1, this means there will be a \u201cpool\u201d of native KAS allocated across several UTXOs, which is locked and owned by the L2. The internal distribution of this pool is managed by the L2 state. A user should be able to initiate an \u201centry\u201d by sending KAS to their L2 account via an L1 operation\u2014sending KAS to a designated, well-known L2 address while specifying (via the payload) their destination address within L2. Complementarily, they should be able to initiate an \u201cexit\u201d request, demanding that the L2 send their BKAS back from L2 to an L1 address. Distinguishing semantics of Entry and Exit operations From the perspective of L1, entry and exit operations serve fundamentally different purposes and feature distinct behaviors. An entry operation is an L1 transaction that is validated inline by L1 itself, making it effective immediately for both the transfer of funds on L1 and the corresponding state update on L2. In contrast, an exit operation requires internal L2 authorization, which cannot be directly validated by L1. Instead, it depends on the submission of a zero-knowledge proof (ZKP) to confirm its validity and take effect on L1. This semantic distinction highlights a key operational difference: entry funds can be used immediately on L2, even before the entry operation has been formally proven via ZKP. For example, an entry operation could provide the necessary collateral for a subsequent proof and exit request by another user, even if its own proof is still pending. This immediate usability of entry funds reflects the simpler integration of entry operations, contrasting with the more complex requirements of exit operations. Below, we dive into the technical details and complexities of designing a bridge to support entry and exit operations. Specifically, canonical bridges have been successfully designed in the industry and are relatively easy to come up with when the L1 is an SC layer as well. Our goal here is constructing such a bridge above the UTXO/scripting model. We focus on two key aspects: Using Kaspa\u2019s scripting capabilities to form user-friendly entry addresses that can be easily used by users. This set of addresses and their corresponding UTXOs will effectively define an \u201cL2 virtual wallet\u201d which can be used by provers for managing L2-owned KAS funds on L1. Addressing the issue of many exit operations resulting from a single proof transaction, which might surpass local L1 transaction size limits. Note: Exit operations are one type of outcome that must be transmitted from L2 to L1. More generally, this primitive is also required for asynchronous messages sent over L1 between rollup instances. These outcomes, verified by L1 as part of the ZKP check, attest to state reads within L2 and can be shared in plain text with other L2s. This process can be viewed as part of an abstract \u201coutbox\u201d that the prover is obligated to deliver to L1. Static entry addresses and the primary state commitment UTXO As detailed in part 1, the state commitment UTXO of a rollup instance contains an ever-changing dynamic p2sh address. Conceptually, this address cannot be used as the receiving address for entry operations, both because it is short-lived (expiring with the next proof) and because spending it inherently requires a full ZKP. Instead, we define static addresses that delegate their spending authorization to an existing ZKP provided by primary proof scripts. This motivates the following introduction of static \u201cdelegation\u201d p2sh addresses, which directly resolve both issues: the address is no longer dynamic, and its spending signature no longer requires full ZKP verification. Specification of static delegation scripts To focus on solving the foundational problem, I assume a single state commitment UTXO representing the full rollup state. Extending the discussion below to multiple state commitment UTXOs per rollup introduces additional complexities that require careful consideration and is therefore out of scope for this post. Preliminaries To establish a foundation for the discussion, let us revisit the definition of key components related to the state commitment process: Denote SCU_i as the i'th State Commitment UTXO of a given rollup instance since inception. In other words, [SCU_0, \\dots, SCU_i, \\dots, SCU_n] forms a chain of UTXOs where the i'th proof transaction spends SCU_{i-1} and creates SCU_i. Denote key(SCU_i) as the key identifier of this UTXO on L1. As per standard UTXO management, this key is composed of the following tuple (often referred to as the outpoint): (\\text{prev tx id}, \\text{output index}). In other words, the UTXO entry is uniquely identified by the transaction that outputs it and the index of that specific output within the transaction. Similarly, denote script(SCU_i) as the p2sh script specified within SCU_i. Define the rollup\u2019s canonical L1 script as CANON (see pseudo code below). Recall that PROG is the hash of the permanent program L2 is obligated to execute. Canonical state commitment script To provide a more rigorous explanation of the canonical script\u2019s role in verifying state commitments, its process is detailed below in pseudo-code (reiterating** parts of the code described in part 1 under \u2018Proof transaction\u2019). This expanded explanation also integrates the p2sh preimage verification step, ensuring a complete picture: ** Note that I\u2019m adopting @hashdag\u2019s terminology (here) and referring to the previously known history_merkle_root as the sequencing_commitment (seq_in in the code below). Inputs: script_hash, sig_script ------------------------------- [<sig_data>, <redeem_script>] = sig_script // Standard p2sh preimage verify verify OpBlake2b(input: redeem_script) == script_hash // Decompose redeem_script into its canonical script (only opcodes) and // its data arguments. // The L2 PROG will appear as a push data opcode in the canonical // script, creating a strong binding between them. Additionally, the // script encodes the incoming L2 state commitment (state_in), and // the incoming L1 sequencing commitment (seq_in) [<CANON>, [<PROG>, <state_in>, <seq_in>]] = redeem_script // Execute the verified preimage script (converted into pseudo code for // brevity). // I.e., CANON is the actual pseudo code listed below in its stack-based // script language format (this transfer to execution of the inner script // is part of the standard p2sh processing) Execute CANON(PROG, state_in, seq_in, sig_data): // Extract arguments from signature data [<out_script>, <chainblock_hash>, <zkp>] = sig_data // Verify the output script preimage. OpTxOutputSpk is a new Kip10 // introspection opcode and is a crucial component in defining // this \"covenant\" between the spending and the output scripts verify OpBlake2b(input: out_script) == OpTxOutputSpk(index: 0) // Decompose the out script, which is expected to be in the same format // as the input script and deviate only in the dynamic elements (state // and seq). Note that this decomposition might require new data-masking // opcodes. [<out_canon>, [<out_prog>, <state_out>, <seq_out>]] = out_script // Verify PROG is preserved verify out_prog == PROG // Verify the canonical script itself is preserved (excluding // the extracted variables state_out and seq_out). This can be // thought of as the \"covenant\" by which the output script must // follow the input script verify out_canon == CANON // Verify L1 sequencing root anchoring verify OpChainBlockHistoryRoot(hash: chainblock_hash) == seq_out // Verify the state transition ZK proof OpZkVerify(prog: PROG, proof: zkp, proof_pub_inputs: [state_in, state_out, seq_in, seq_out]) // ^ omitting other auxiliary verification data The canonical script enforces the correctness of state transitions, ensuring both the input and output states, as well as sequencing commitments, are consistent with the L2 program (PROG ) and the ZKP. Static delegation script To support entry operations without directly relying on dynamic state commitments, the following delegation script is proposed. This static script delegates certain responsibilities to the canonical script while remaining independent of the current state or history. Inputs: script_hash, sig_script ------------------------------- [<sig_data>, <redeem_script>] = sig_script // Standard p2sh preimage verify verify OpBlake2b(input: redeem_script) == script_hash // Decompose redeem_script into its script part and data arguments [<DELEGATE>, [<PROG>, <CANON>]] = redeem_script // Execute the delegation script Execute DELEGATE(PROG, CANON, sig_data): [<primary_script>] = sig_data // Verify primary script preimage verify OpBlake2b(input: primary_script) == OpTxInputSpk(index: 0) // Decompose primary script [<canon_primary>, [<prog_primary>, ...]] = primary_script // Verify PROG is preserved verify prog_primary == PROG // Verify the delegation targets a correct canonical script verify canon_primary == CANON The delegation scheme builds on the inherent property that a transaction is only accepted when all its inputs are validated as correctly spent. Through this mechanism, the delegator ensures that the primary input script meets specific requirements, authorizing its own spending if those conditions are satisfied. The delegation script described above achieves a static structure by avoiding reliance on specific sequencing or dynamic state commitments, as shown by the redeem_script preimage in the code snippet. However, without showcasing further properties, the scheme remains vulnerable to the following attack vector. Attack vector. Alice sends funds from a standard L1 Schnorr address to a script structured correctly with CANON and PROG but using fabricated state and seq commitments. She then constructs a proof transaction, using the resulting UTXO as the primary input and adding additional inputs from static delegation addresses, redirecting the KAS funds to the primary state-commitment output. The result is that the funds are locked in a malformed state-commitment UTXO distinct from the authentic proof chain. Even if she uses a valid (state, seq) pair, the problem persists as she has effectively created a separate proof chain. The challenge. Addressing this attack requires incorporating information beyond the script itself. This is because L1 cannot enforce restrictions on the \u201centrance\u201d to a script covenant\u2014for instance, any user can send funds to an opaque p2sh address. Consequently, authentic and forged state-commitment UTXOs are indistinguishable at the script level. Solutions based on static registration of rollup-associated information outside the UTXO set are currently ruled out, as they would introduce significant complexity and compromise the cleanliness and soundness of L1 state management. Key-based authentication. An alternative solution involves leveraging UTXO keys to differentiate between authentic and non-authentic state-commitment UTXOs. A UTXO key is uniquely derived from the transaction that creates it, allowing the authenticity of a UTXO to be tied directly to its creating transaction. One possible approach relies on defining a genesis UTXO in the L2 source code and proving that the current UTXO input belongs to a transaction chain originating from this genesis. While requiring the entire proof chain as a witness in each new proof transition is impractical, a feasible alternative involves using a constant-length suffix of the proof chain, as outlined in the following section (in collaboration with @FreshAir08; based on preliminary discussions with @reshmem & @hashdag as well). Key-based state-commitment UTXO authentication We propose the following scheme as in illustrative example of a construction providing state-commitment authenticity. A GENESIS UTXO is hardcoded in L2 PROG via its source code The CANON part of the canonical script is hardcoded in L2 PROG as well. Note that this is a pure script without any dependency on PROG (thus no hash cycles) The ZKP receives as public input the key of the currently spent state-commitment UTXO, i.e., key(SCU_{i-1}) The corresponding L2 prover program (producing this ZKP) receives as private witness the full i-2, i-1 proof transactions and the preimage of script(SCU_{i-2}) The following logic is executed as part of the program: // L2 program running for each proof transition // // Arguments: input_key - the UTXO key of the currently // spent state-commitment UTXO // source_tx - the source transaction of input_key // (i.e., proof transaction i-1) // source_script - the preimage of the script hash used as // input for source_tx // script_affirming_tx - proof transaction i-2; provided for // affirming source_script (since the // input of i-1 does not specify it) PROG(...,pub_inputs: [..., input_key], private_inputs: [..., source_tx, source_script, script_affirming_tx]): ... // First, verify that input_key is indeed an output of source_tx verify (blake2b(source_tx), 0) == input_key if source_tx.inputs[0].outpoint == GENESIS: // Allow entering the covenant only via the hardcoded genesis UTXO pass else: // Otherwise, we must verify that input_key was produced as part of the // covenant. To do that we acquire the incoming script through the i-2 // transaction, decompose it, and verify that it follows the expected // canonical script // Verify the linkage between the two transactions (i-2 -> i-1) verify (blake2b(script_affirming_tx), 0) == source_tx.inputs[0].outpoint // Verify that source_script is the preimage verify blake2b(source_script) == script_affirming_tx.outputs[0].script // Decompose the script [<canon>, [<_prog>, <_state>, <_seq>]] = source_script // Verify it follows the expected canonical script verify canon == CANON ... Definition 1: A state commitment UTXO is well-formed if its script preimage can be decomposed into [<CANON>, [<PROG>, <*>, <*>]]. Definition 2: A state commitment UTXO is forged if it hasn\u2019t originated from the GENESIS UTXO, i.e., GENESIS was never part of a transaction chain leading to it. Claim 1: A well-formed forged state commitment UTXO is unspendable. Proof: Assume for contradiction that such a UTXO exists. Then there must be a maximal transaction X on the chain creating it, where the first input script S_1 is not well-formed (or at the very least missing altogether, since root transactions are always coinbase). By the maximality of X, it follows that the output of X, used as the first input for the following transaction, is well-formed. Denote this output as S_2. There are two cases to consider: Case 1: S_1 is partially malformed Here, S_1 can be decomposed into [<CANON>, [<PROG'>, <*>, <*>]], but it uses an incorrect PROG'. In this case, X would fail L1 verification because S_2 will decompose into [<CANON>, [<PROG>, <*>, <*>]], and the L1 CANON execution would attempt to verify that PROG' is preserved, resulting in failure. Case 2: S_1 is completely malformed Here, S_1 cannot even be decomposed into [<CANON>, [<*>, <*>, <*>]]. However, S_2 is well-formed. From our contradiction assumption, S_2 must be spendable. The spender must provide a valid ZKP based on the real PROG. However, the prover must include X as a witness, and during PROG execution, both branches will fail: (i) X's first input key cannot be GENESIS, as it is forged; and (ii) S_1 cannot be decomposed into CANON, so it will fail the final line of execution. Corollary: Delegation scripts cannot be redirected to a forged proof chain. Proof: The last line of DELEGATE verifies that the primary input script is well-formed, thus by Claim 1 it can only be spent if it is not forged. L2 initialization procedure To initialize an L2 system with this scheme, the process begins by sending KAS on L1 to an ordinary address (e.g., Schnorr) controlled by L2 initiators. The resulting UTXO key from this transaction is then added to the L2 source code and designated as GENESIS. This key serves as the starting point for the rollup\u2019s transaction chain. The next step is to create the canonical script CANON and add it as a constant to the L2 source code. While CANON is not directly part of the L2 system, it is essential for enabling chain link verification between transactions. Once CANON is defined, the L2 program is compiled to produce PROG, which serves as the main logic for generating proofs. Using CANON, PROG, and the initial state and seq, the initial state-commitment script is composed. This script represents the starting point for the rollup\u2019s state and sequencing commitments. An L1 transaction is then performed to transfer funds from the GENESIS UTXO to the newly created state-commitment script, marking the rollup\u2019s formal initiation. After the transaction is confirmed, a proof of its acceptance on L1 should be saved as part of the L2\u2019s integrity data. This proof can also be shared with new L2 nodes to establish trust in the L2 initialization process. The combination of L1-approved proof transactions and L2-verified chain links ensures that only authenticated state-commitment UTXOs pass ZKP verification on L1, effectively mitigating the attack vector of redirecting delegated address funds. Syncing new L2 nodes This scheme provides a trustless mechanism for fully syncing new L2 nodes from the recent state. By verifying the correct state-commitment UTXO and the suffix of the transaction chain leading to it (e.g., the last two transactions), new nodes can use the authenticated L2 state commitment on L1 to confirm the newly synced L2 state is consistent with this commitment. Exit operations as proof outputs (Thx @FreshAir08 for writing the majority of this section.) After a user issues a \u201cwithdrawal\u201d transaction to L2, the associated funds are no longer available in L2 but remain in the L2-owned addresses. When a proof is submitted, it must ensure and enforce the transfer of these funds from the addresses to the requested L1 address. Conceptually, these pending withdrawal transactions form an outbox of exit operations, which can be inferred from the executed L1 transactions within the proved period. To support this mechanism, we propose the following additions: L1 CANON script change: modify CANON to compute the cumulative hash of all outputs in the current transaction except the primary state commitment output (similar to the Schnorr sighash process). This hash will be passed to OpZkVerify as an additional public proof input. L2 PROG change: update PROG to compile a list of expected proof outputs from the outbox and compute their cumulative hash using the same method employed by CANON. This hash is then verified against the public input provided in the proof. N to Const problem So far, we have assumed that the prover can include all pending exit outputs in a single proof transaction. However, due to mass limitations (e.g., KIP9), these added outputs could potentially take up a significant amount of block space. While higher fees for the prover or slower L2 progress are concerns, the main issue is that the transaction might exceed the block size limit, causing a deadlock in the L2. In extreme cases, even a single mergeset could congest an entire block. We explore two possible solutions to address this issue: Instead of directly including all L1 addresses in outputs, public keys (or more general scripts) can be used. Provers would transfer funds to special-purpose p2sh addresses that represent all public keys associated with the withdrawals. These addresses would allow each public key to spend only its share of the funds, similar to a KIP10-like mechanism. The public keys could be stored in a Merkle tree within the redeem script, and an additional opcode might be added to verify Merkle witnesses efficiently. If funds are not immediately repatriated to L1, they can remain in a designated L2 outbox, which acts as an extension to the L2 state. The program would enforce that the active L2 state cannot advance until the outbox has been cleared. If the outbox becomes too congested, the L1 sequencing commitment would remain unchanged, and only the outbox would be updated. Adapting the hash commitments discussed earlier to this structure would be straightforward. A more lenient variation of this idea could allow state advancement with partial outbox clearing. For example, provers might be required to clear the outbox at twice the rate they add to it. This restriction could activate only once the outbox exceeds a predefined congestion limit.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-13T12:49:19.647Z",
      "updated_at": "2025-01-13T20:28:30.653Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258/1",
      "category_id": 11
    },
    {
      "post_id": 490,
      "post_number": 2,
      "topic_id": 258,
      "topic_title": "L1<>L2 canonical bridge (entry/exit mechanism)",
      "topic_slug": "l1-l2-canonical-bridge-entry-exit-mechanism",
      "content": "michaelsutton: Key-based state-commitment UTXO authentication Worth mentioning that after writing this post I bumped into a similar authenticity proposal by starkware: To address this, we perform what is called a \u201cgenesis check.\u201d Essentially, we make the aggregation covenant check its previous transaction and the transaction preceding that one \u2014 that is, its ancestor transactions. The covenant verifies that these transactions contain the same covenant script and perform the same checks. In this way, we achieve an inductive transaction history check. Because both of the previous transactions performed the same checks as this covenant does, we know that the ancestors of that transaction did the same, all the way back to the leaf (i.e., the genesis transaction).",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2025-01-28T10:40:09.751Z",
      "updated_at": "2025-01-28T10:40:09.751Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/l1-l2-canonical-bridge-entry-exit-mechanism/258/2",
      "category_id": 11
    },
    {
      "post_id": 422,
      "post_number": 1,
      "topic_id": 251,
      "topic_title": "Conflicting Proofs Policy",
      "topic_slug": "conflicting-proofs-policy",
      "content": "In Kaspa based rollups design, entities called provers will regularly submit proofs of an ordered aggregation of txs that occurred since the last proof was submitted \u2013 a point in time which I will refer to as the latest settlement point. More precisely, txs are proven in batches according to the chain blocks that accepted them. These proofs are treated as regular txs from the point of view of L1. Currently there is no set bound planned on how late or how early can these proofs arrive. Provers may cooperate completely on the proving effort \u2013 there is no clear danger in them cooperating as the results of their computation are deterministically decided by the contents of the L1. Nevertheless for reasons of lack of coordination, or for plain avarice, two provers may end up submitting two distinct proofs, starting from the same L2 latest settlement point. The two proofs will naturally agree on a prefix of the proven txs, but one will usually contain a suffix missing from the second. I refer to such events as conflicting proofs. If the longer of the two is the first included in the L1 ledger, then it is clear that the second proof is to be ignored. But if the shorter is included first, it is less obvious what to do. Naively, the longer proof is treated as a double spend attempt/ invalid tx and ignored. However there are several issues with it, first, in terms of quality of service, the longer proof contains txs that the L1 has yet to see the proof of. Ideally, these proofs can be included immediately, but the naive design foregoes that. Second, the prover has worked tirelessly to create their longer proof, but now will have to start from anew, leading to losses, potentially making proving financially unsustainable if this occurs routinely. Third, this paves the way for a liveness attack on the L2: a malicious party can regularly submit proofs proving 1 block of txs to scoop all other provers (as supplying long proofs takes more time for the benevolent provers), resulting in the settlement to L1 progressing very slowly. Two possible solutions come to mind: the first, perform major logical changes to allow miners to include a more inclusive proof even if its settlement point has already been deprecated. I believe this is possible to engineer in some manner, but will likely demand a further level of utxos abstraction, more opcodes, and might end up introducing other complications. The second solution is the empty solution where we will accept this phenomenon as a fact of life, yet explain away why the problems are not as fatal as they superficially seem: a property of Snark/Stark proofs as I understand them is that they are componentized. i.e., proofs are not necessarily created as one massive indivisible black box block, rather the different stages of the computation could be broken apart and proven separately, then combined together. If this supposition is true, then even if scooped, a prover only loses the work put to prove the subset of txs proven by its competitor, and some constant metadata. Thus they can relatively quickly rebuild a new proof from the surplus txs they previously proved but their competitor did not. It is self apparent that if this supposition is true it will mitigate the effects of all 3 of the issues discussed above, without the need to take drastic measure. However the \u201cif\u201d here is crucial: I address the zk audience (@reshmem and @proof are explicitly called, but please others join in) with a question: is this supposition about the way snark/stark proofs behave true? As a sidenote, it is worth mentioning that \u201cscooping\u201d is not all bad: if the fear of being scooped rules the ring, one might expect provers to compete within themselves on proving as fast as possible, vastly improving quality of service. However the exact mechanics required to ensure such competition without collapsing to cannibalism and monopoly require more thought, and seem out of scope for this post.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-08T18:34:30.099Z",
      "updated_at": "2025-01-09T10:07:02.213Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/conflicting-proofs-policy/251/1",
      "category_id": 11
    },
    {
      "post_id": 483,
      "post_number": 2,
      "topic_id": 251,
      "topic_title": "Conflicting Proofs Policy",
      "topic_slug": "conflicting-proofs-policy",
      "content": "Please don\u2019t treat my answer as a complete one because it will be too long to answer and in any case I think that even with a very long detailed answer there is not possible to cover everything since every particular choice of ZK system should be considered separately. ZK schemes / systems in general support technics as recursion / folding and other smart technics on parallel proof generation but, this topic is very depends on particular system, privacy requirements and many other factors and choices made for this particular zk-system. It looks like it is up to the L2s to choose what is good for them. For example STARK based proving systems with recursion and / or folding are able to support it. Packing stage ( G16 / Plonk ) does not support recursion / folding natively, but MSMs can be paralleled, for example via Pippenger\u2019s algo and FFTs parallel nature is obvious. Especially given the fact that witness in this case has no visible private requirements. Privacy protocols that have this type of requirements will use known technics how to accomplish what the need. Even without such support ( no deep dive to the details here ), it looks possible to control desired behaviour via L2s logic. For example L2s can choose the provers\u2019 queue and anchor it inside spending script via ZK itself ( or via BTC-like kaspa script + Merkle-Root )\u2026 Bottom line, I think L1 should not care about it at all ( at least for the next 1-2 years . I highly support your second solution ) and let L2s to decide what is good for them and how to do it in a best possible / available way ( hence every week we see new game-changing zk scheme comes out. But, I still think that this problematic should be reflected in our overall design suggestions in order to provide (at least) some guidelines to these L2s that are new to ZK concepts. Auditors (ZK) on the other hand will catch these points very easily and it is a responsibility of each L2 to conduct such audit.",
      "raw_content": "",
      "author": "reshmem",
      "created_at": "2025-01-24T12:30:59.310Z",
      "updated_at": "2025-01-24T12:37:19.472Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/conflicting-proofs-policy/251/2",
      "category_id": 11
    },
    {
      "post_id": 438,
      "post_number": 1,
      "topic_id": 267,
      "topic_title": "Optimal Transaction Selection - A Multidimensional Knapsack Problem",
      "topic_slug": "optimal-transaction-selection-a-multidimensional-knapsack-problem",
      "content": "KIP9 (Persistent Storage Mass) and KIP13 (Transient Storage Mass) introduced new types of mass which represent some resource consumption within a node. Originally there was only compute mass (usage of compute resource) but now there is also persistent storage mass (usage of storage space that persist between pruning periods) and transient storage mass (usage of storage space only within a pruning period). The KIPs above also propose to independently track each mass when checking for block mass limit on the consensus level. This allows fo optimal block space allocation for transactions that consume different resources. The block mass limit is 500,000 grams. So such independent tracking allows for including two transactions that have masses (compute:490,000g; persistent_storage_mass: 10,000g) and (compute:10,000g; persistent_storage_mass: 490,000g) in the same block. Now, an interesting question arises: On the mempool level, when constructing the block template, how can the transaction selection logic be updated to optimize for block space consumption?",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2025-01-18T21:42:56.325Z",
      "updated_at": "2025-01-18T21:42:56.325Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/optimal-transaction-selection-a-multidimensional-knapsack-problem/267/1",
      "category_id": 7
    },
    {
      "post_id": 417,
      "post_number": 1,
      "topic_id": 247,
      "topic_title": "Fees and throughput regulation dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "Context: based rollups. Since L2\u2019s are permissionless, they too need a mechanism to cap their throughput. As per the custom, let\u2019s refer to the computational load on L2 nodes in gas units. This load applies to any L2 operator \u2013 entities parsing the state, eg L2 explorers, wallets, etc. \u2013 but more importantly to provers. The latter\u2019s load is 2-3 order of magnitude heavier than entities merely executing the state. What entities should regulate L2 throughput? One could imagine a setup where an L2 instance caps its own throughput by encoding a gas limit rule in its program, but this could lead to an undesirable outcome where users\u2019 transactions are included in L1 blocks but ignored by the corresponding L2. If we want to avoid such horrible UX, then L1 sequencers are the only relevant entities to take on the responsibility of regulating L2 throughput. Let\u2019s start by defining the requirements for a regulation mechanism: L2 throughput must be capped. L1 shouldn\u2019t typically include transactions that exceed the throughput cap. L2 quality of service (QoS) should be preserved: Users with higher urgency should be able to prioritize their txns. (This crucial property is often overlooked, ignoring this requirement can lead to practical DoS for urgent transactions, similarly to the havoc on KAS, June '24, due to the lack of RBF to deal with txn congestion.) The mechanism shouldn\u2019t rely on users\u2019 honesty or miners\u2019 goodwill. The mechanism shouldn\u2019t require L1 actors executing L2 logic. L2 provers should be compensated for sustainability. One simple approach we propose (@FreshAir08 +the usual suspects) is: i. Transaction issuers declare the maximum gas their transactions consume. ii. Miners ensure no more than X gas units are confirmed per block per L2 instance (enforced in L1 consensus). iii. Transactions that exceed their declared maximum gas are reverted but still pay their fees, similar to Ethereum L1 rules. Note: Wallets can typically provide reasonable gas estimates. Even for complex transactions, users can often give tight bounds, so they won\u2019t usually suffer from gross misestimates. iv. Transactions that don\u2019t exhaust their declared gas still pay the full gas fee to L2 from the perspective of L1. Whether this is refunded to the user within L2 logic is outside this discussion. [A riddle for the reader: If users are refunded the gas change, can they overestimate gas just to be lazy? Why not?] It is easy to verify that this design satisfies all aforementioned requirements. Observe an important implication of this design: When L2 demand is high, revenue and fees go to L1 sequencers, not to L2 provers. I argue this is not a problem, and the preservation of L1\u2019s security budget outweighs L2 layer biz model concerns. An alternative proposal will be be described at the end of the post. Another observation is that introducing gas units and limits per block (point ii above) adds complexity for sequencers (miners) optimizing their blocks. It effectively creates a multi-knapsack problem for sequencers, which is NP-hard. However, this computation isn\u2019t done on-chain or in consensus, and simple heuristics can achieve a sufficient approximation of the optimal solution. Importantly, this challenge isn\u2019t unique to gas units; similar considerations apply to storage-mass (KIP9) units. See KIP13 by @coderofstuff. Importantly, each L2 instance can define its own interpretation and scale for gas\u2014there\u2019s no implication in L1 consensus about what gas specifically means. If one instance computes gas differently \u2014 whether in a simple or complex manner \u2014 that\u2019s fine. L2 instances can also dynamically adjust gas scales based on load (@proof note this). However, this dynamic adjustment should be carefully designed to avoid situations where users\u2019 transactions are included by miners but ignored by L2 programs, causing fee loss and poor UX. Finally, since we proposed that all transaction fees \u2014 regardless of rising demand \u2014 flow to sequencers, one might ask how L2 provers are paid and what guarantees their compensation and sustainability. One alternative advocated by @someone235 is that a fraction of L1 fees associated with a certain L2 instance flow to its P2SH address and be distributed to provers via its PROG. Hopefully others can evaluate this direction or provide alternative L2 sustainability schemes",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-01-07T17:09:52.136Z",
      "updated_at": "2025-01-08T21:10:55.183Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/1",
      "category_id": 11
    },
    {
      "post_id": 420,
      "post_number": 2,
      "topic_id": 247,
      "topic_title": "Fees and throughput regulation dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "A couple of notes and thoughts. Many of these might protrude a bit into the realm of the L2 design itself which may be out of scope but I want to mention them for the reader. First, there is no real problem in the L2 requiring another inner L2 fee in proportion to the L1 miner fee (if the inner fee is not sufficient, txs are included but considered invalid from the L2 perspective) is there? I think of the inner fee moving to a SC with its locked money only released by a proof tx. That\u2019s a cleaner way to reward L2 provers for high activity in their subnet imo than forcing L1 to deal with the distribution. Upon reread that might be what was suggested in the post but I wouldn\u2019t use the term \u201cL1 fees\u201d for that. Second, without stepping too much into the L2 shoes, I believe L2 rewards should encourage fast proving in some manner, i.e. rewarding higher fees for proofs that come \u201cfast enough\u201d. Could be handled in a similar manner to what I suggested above, with the excess money going back to the original payer if proofs didn\u2019t arrive fast enough. Of course this does not apply to all L2: some may just be service providers and will provide fast proofs to their clients as part of their service. Lastly one might ponder what happens if the monetary policy of L2 ends up being unsustainable due to price drop, or unjust due to price increase. L2s can and arguably should have some update mechanism for the gas scale (and potentially other things). Obviously this update mechanism and the extents of what is allowed to be updated, presents many questions of L2 governance, which L2 creators will need to address according to their specific needs and ethea.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2025-01-08T18:29:57.808Z",
      "updated_at": "2025-01-08T20:20:21.364Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/2",
      "category_id": 11
    },
    {
      "post_id": 444,
      "post_number": 3,
      "topic_id": 247,
      "topic_title": "Fees and throughput regulation dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "Do I understand correctly why \u201crevenue and fees go to L1 sequencers, not to L2 provers\u201d \u2013 is it precisely because it \u201cadds complexity for sequencers (miners) optimizing their blocks\u201d? Before, miners were incentivized to perform only the optimization to select transactions based on storage-mass, now, in addition to that, they will also perform transaction selection based on gas amount. While economics and profitability of miner business very much depends on the demand and the competition, we can think of ZK provers as almost having constant costs to operate - therefore, they are not in danger if they are paid just enough? Or maybe you say a more general thing, since you seem to downplay the actual amount of additional work they will do in practice, that it is only miners that are incentivised in the system for increased demand for block space, no matter where the demand is coming from?",
      "raw_content": "",
      "author": "oudeis",
      "created_at": "2025-01-19T22:00:02.770Z",
      "updated_at": "2025-01-19T22:00:02.770Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/3",
      "category_id": 11
    },
    {
      "post_id": 474,
      "post_number": 4,
      "topic_id": 247,
      "topic_title": "Fees and throughput regulation dynamics",
      "topic_slug": "fees-and-throughput-regulation-dynamics",
      "content": "Even if L2 txns incur no special costs to miners, no added complexity etc., still the miners are inevitably the entity to receive users\u2019 txn fees and to enjoy surge in demand for L2 txns, b/c they are the (only) gatekeepers to the entire system, so the user must incentivize them \u2013 bribe, if you may \u2013 in order to be sequenced sooner rather than later. The more pressure and demand for L2 txns \u2013 the more impatient users are willing to pay to enter the sequencing in a timely manner \u2013 and if this willingness to pay does not transfer to the miners themselves in the form of greater revenue from these impatient txns then the miner would ignore these high priority txns, which destroys the QoS and practically leads to a DoS for L2. Whether you need another L2-dictated form of payment or mechanism to satisfy the economics or sustainability of provers \u2013 I did not form a strong opinion on that, there are several options that come to mind. One could argue, as you mentioned, that provers\u2019 load is const and therefore they should require no extra compensation, others can argue that a healthy macro dynamic is one where service providers receive higher compensation in case of higher interest/demand in their services. L2\u2019s can decide their own mechanism here, as long as they comprehend that fees for prioritization under demand must inherently flow to the miners.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2025-01-22T22:04:59.535Z",
      "updated_at": "2025-01-22T22:04:59.535Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/fees-and-throughput-regulation-dynamics/247/4",
      "category_id": 11
    },
    {
      "post_id": 404,
      "post_number": 1,
      "topic_id": 237,
      "topic_title": "Updateable list of L1<>L2 topics to flesh out before finalizing design",
      "topic_slug": "updateable-list-of-l1-l2-topics-to-flesh-out-before-finalizing-design",
      "content": "For the sake of visibility and coordination of research core, dumping here an updateable list of research topics that must be fleshed out before we present an initial holistic design for L1<>L2. All topics and terms will be defined and explained clearly in separate independent posts. Published high level op_codes; L1<>L2 conceptual protocol: see @michaelsutton\u2019s post. cryptographic primitives, low-level op_codes: see @reshmem\u2019s post. Needs to be written/published: Benchmarks towards final decisions on zk-friendly hash; see comments to @reshmem\u2019s post State commitments: uniqueness of the state commitment utxo. Done proving race rules; liveness attacks (proof construction to prevent ramifications). Done multiple state commitments per rollup (related to intra subnet parallelism) Sequencing commitments; hierarchical data structure (linear; consider usecase for log). Done Entry/exit mechanisms; virtual wallet management. Done Throughput regulation; fee market; gas vs fees; global vs per subnet gas limit; (related: multidimensional knapsack problem). Done L2\u2019s interop: messaging protocol; design for sync atomic composability",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-23T13:49:35.720Z",
      "updated_at": "2025-01-20T11:31:42.541Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/updateable-list-of-l1-l2-topics-to-flesh-out-before-finalizing-design/237/1",
      "category_id": 11
    },
    {
      "post_id": 90,
      "post_number": 1,
      "topic_id": 62,
      "topic_title": "Including transactions of red blocks",
      "topic_slug": "including-transactions-of-red-blocks",
      "content": "Red blocks are \u201cthe orphans of DAG\u201d, they are blocks that arrived too late and therefore their POW is not counted as contributing to the weight of their respective branches. This is formally defined by the PHANTOM/GHOSTDAG procedure. I propose to include their txns in the UTXO set as well. The motivation is that it allows for very fast 1 confirmation: In many use cases there is no real threat of a double spend, and the user only wants to know that her txn entered the ledger and is accepted for sure (assuming no double spend). This gives protection, e.g., against temporary 51% attacks, where an attacker gets hold of a majority of the hashrate for some limited time, and is able to gain full control over the set of blue blocks and to reject all txns included by the honest miners and users, which are now inside red blocks. (A counterargument is that valid txns that were rejected by this temp attack will be reintroduced to the mempool and mined again, post the attack. I still argue that this proposal greatly simplifies the UX for users/integration points, and that counting on the mempool process adds risks and uncertainty - mempool may bloat due to the attack, fees can temporarily skyrocket, etc.) This can be achieved by simply iterating over txns in the set of blocks B.past \\setminus B.selectedParent.past according to some deterministic topological ordering, and for each txn adding it to the UTXO set if it is consistent with the current UTXO. From a DAST point of view, there is no need to keep in memory the information on red blocks, it can be simply part of the UTXO handling around the accepting (blue) block. Note that txn fees should not be paid to the miner of red blocks. Note further that even red blocks must respect the mass and gas caps, and can\u2019t introduce more txns than allowed.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2020-04-07T16:03:47.235Z",
      "updated_at": "2020-05-11T16:24:44.945Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/1",
      "category_id": 9
    },
    {
      "post_id": 91,
      "post_number": 2,
      "topic_id": 62,
      "topic_title": "Including transactions of red blocks",
      "topic_slug": "including-transactions-of-red-blocks",
      "content": "I really like the idea of including red blocks, but it seems that we\u2019ll have some difficulties with mixing it with the idea of not validating double spends of red blocks. If we don\u2019t keep the past UTXO of red blocks it means it won\u2019t be trivial to count the number of sigops in a transaction, so it means we won\u2019t know if a red block has more mass than allowed (and by proxy also the blue block that points it).",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2020-04-16T13:28:35.261Z",
      "updated_at": "2020-04-16T13:28:35.261Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/2",
      "category_id": 9
    },
    {
      "post_id": 344,
      "post_number": 3,
      "topic_id": 62,
      "topic_title": "Including transactions of red blocks",
      "topic_slug": "including-transactions-of-red-blocks",
      "content": "Sorry for resurrecting this thread. I have a few questions along these lines and didn\u2019t want to create a new thread when there was already one with minimal discussion. I just want to say first that argument about having protection against temporary 51% attacks is a compelling one for including txns within red blocks. It\u2019s not mentioned above, but to include txns within red blocks necessarily means including red blocks in a block\u2019s mergeset (mentioning it here in case it\u2019s unclear to anyone reading). I\u2019m wondering about a few things: Are red blocks ordered the same way as blue blocks would be (except they go after all the blue blocks in the same mergeset)? My intuition tells me it must be since they\u2019d still have blue work info even if they\u2019re red so they can be ordered that way. And since ordering is consensus sensitive then so ordering them must also be done in a consistent, stable manner. How deep of a red block (and txs within) are we allowing for inclusion here? I\u2019m thinking about a few boundaries like the DAA window, the merge depth bound under which we (maybe?) no longer accept red blocks.",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2024-12-10T16:49:11.887Z",
      "updated_at": "2024-12-10T16:49:11.887Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/3",
      "category_id": 9
    },
    {
      "post_id": 349,
      "post_number": 4,
      "topic_id": 62,
      "topic_title": "Including transactions of red blocks",
      "topic_slug": "including-transactions-of-red-blocks",
      "content": "IIRC red blocks receive no special treatment within the merge set\u2013the rules of inclusion and the ordering do not explicitly take into account the colour of a block (re inclusion, it so happens that merge_depth violating blocks will be red, since merge_depth>>k). From the \u201clocal\u201d merge-set pov, the only different treatment they receive is that their rewards are sent to the merging block, in order to impose a clear cost on double-spending attempts (similarly to Bitcoin orphans not receiving rewards; also related: the uncle mining attack on Ethereum 1.0). The real impact of blue-red discrimination is the contribution to a block\u2019s score (aka blue score), which impacts the chain selection rule, and in turn the entire ordering. Since only blue blocks contribute to a block\u2019s ability to compete for chain membership, red blocks do not contribute to the security of the DAG\u2019s ordering. BTW, I\u2019ll try to explain in a later comment the answer to another question (which i believe you once raised): if red blocks do not contribute to security, why are they still counted in the DAA window",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-12-11T05:44:48.783Z",
      "updated_at": "2024-12-11T05:44:48.783Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/including-transactions-of-red-blocks/62/4",
      "category_id": 9
    },
    {
      "post_id": 322,
      "post_number": 1,
      "topic_id": 193,
      "topic_title": "Atomic Composability and other considerations for L1/L2 support",
      "topic_slug": "atomic-composability-and-other-considerations-for-l1-l2-support",
      "content": "This post is based on extensive conversations and brainstorming with Sutton, Roma, and Ilia@Starkware. Background We are putting efforts into designing Kaspa\u2019s L1 support for L2. Our design follows the principles of zk-based rollups (ZK RUs): all smart contract activity is recorded in L1 as payloads (blobs in Eth jargon), and the state of all logic zones (smart contracts or rollups, I use them deliberately interchangeably, for reasons I\u2019ll explain later or in a separate post) is committed to in block headers. The term rollups originally required on-chain sequencing, but nowadays on-chain sequenced rollups became the exception, not the rule, and are referred to as based rollups. To anchor a new state of a certain logic zone to the base layer, after one or multiple state transitions, some prover node must send to the base layer a transaction that provides a ZKP. The latter is verified in the base consensus through a new op_code, and the (commitment to the) state of the logic zone is thence updated. Crucially, the proofs may be submitted with non-negligible delays, notwithstanding users can get instant (~100 ms) confirmation by any node following this logic zone, executing its (L1-sequenced) transactions, and parsing its state. In short: proof latency does not affect finality latency; it affects the state-sync time of new L2 nodes, the L1-pruning interval, and the computational load of cross-logic zone transactions. Atomic Composability Sync vs Async Composability The best smart contract layers are designed for atomic or sync composability, where a smart contract can allow functions from other smart contracts to call it and interact with its variables (read and write) on the spot, atomically, in the scope of the triggering transactions. This cross-smart contract functionality describes the on-chain activity during Ethereum\u2019s early years and arguably facilitated the incredible growth of its ecosystem and dev community. Unfortunately, Ethereum\u2019s rollup-centric roadmap is working against this atomic composability feature and settles for async composability (which is still much better than TradFi UX, which requires manual composing). In async composability, smart contracts can still interact with and send messages to one another, yet this is done through some layer that carries or bridges these messages \u2013 typically the base layer \u2013 which suffers from some latency. Consequently, read/write actions are not treated in the scope of the originating transaction, atomicity is not guaranteed, and the effect of a composable transaction (the contracts\u2019 variables and the issuer\u2019s account post the transaction) cannot be guaranteed in advance. Note: The unavoidable lack of predictability due to multiple users acting simultaneously is a separate issue; transaction issuers can enforce behaviour through several techniques, e.g., slippage specification, explicit conditions in the transaction, or intent specification. The issue with async composability is that the effect of only parts of the transaction can be fully enforced. Based rollups are atomically composable There are arguments for why async composability is good enough, but I will not get into them here. Let\u2019s just assume we don\u2019t want to settle on that. The good news is that we don\u2019t need to since we are going full zk-based mode: Since all data is on-chain, the state of each logic zone is fully reconstructable from on-chain data (until the pruning point). Consequently, the effect of a multi-logic zone transaction occurs at its very sequencing, with no latency, and conditions on its effect across all logic zones can be simultaneously enforced. Contrast this dynamic with the non-based rollup-centric Ethereum, wherein the semi-off-chain sequencing of logic zone I transactions may be inaccessible to L1 (and to logic zone II provers), hence not provable to it. I reiterate that the sync atomicity of transactions is irrespective of prover latency: Proof arrival frequency does not affect confirmation times in L1 or L2. Logic zones interactions and dependencies Now, consider a composable transaction txn that not only acts on two logic zones but also triggers an interaction between them; e.g., the transaction calls a function inside logic zone I and uses the output of this interaction as an argument for a function call to logic zone II. Observe that to verify the correct execution of logic zone II, the base layer must see a proof of the correct state transition of logic zone II and that of logic zone I, since the output or intermediate state of the latter served as input to the state transition of the former. Similarly, the operators (read: provers) of logic zone II that wish to follow and parse their state must follow and execute logic zone I as well. This dependency seems problematic at first sight\u2014if the existence of composable transactions implies that all provers need to execute all transactions of all logic zones, then the architecture supposedly collapses back to one grand logic zone that suffers from the same scalability hindrances as a computation-oriented L1\u2014each txn consumes the same computation load that serves all other txns. But this is not really the case because: Executing transactions of other logic zones needs to occur only when logic zones interact. Logic zones can define permissions (in their program code) for specific logic zones to interact with them in sync mode and require other logic zones to interact in async mode through the base layer\u2019s messaging protocol. Transaction execution is cheaper than proof generation by 2 or 3 orders of magnitude. Observe that provers of logic zone II need to execute but not to prove the (intermediate) state of logic zone I, which is the computationally intense part. Running a prover needs to be permissionless but not necessarily decentralized in the sense of optimizing for commodity hardware being able to run system-wide provers. Minimizing cross logic-zone dependencies These considerations imply that an ideal ecosystem would minimize the scope of logic zones, which as a byproduct would minimize cross-logic zone dependencies as well as the implications (e.g., the aforementioned execution burden) of dependencies. I strongly encourage L2/SC projects building on Kaspa to follow this design principle and avoid aggregating many separable logic zones (smart contracts) under one overarching logic zone (rollup). Latency in async composability mode It is important to note that when logic zones are not supporting atomic composability and instead use async composability through L1\u2019s messaging feature, they suffer the latency of provers and not (only) the latency of the base layer. Thus, even when Kaspa implements 10 (Sutton, read: 100) BPS, if the prover of logic zone I provides a proof every 10 minutes, then that is the latency that async-composable transactions would suffer; and for many applications, 10 minutes = infinity (which is why Bitcoin can\u2019t realistically serve as a base layer for zk-based rollups). This is why I think we should insist on atomic composability. Ensuring State Commitments A final comment on this construction: Recall that the composable txn described above forces provers of logic zone II to execute the state of logic zone I after txn\u2019s first part. Now, a proof by the former operators can appear on-chain only after the latter submitted one; let\u2019s denote this state by state_I_pre. However, logic zone I provers might submit a proof that batches a series of state transitions of I, of which state_I_pre is only an intermediate member. To allow II\u2019s provers to build their proof utilizing the (chunk) proof of I, we must ensure L1 has access to the intermediate states that have been proven by I\u2019s provers. In other words, we need all proofs to commit to all intermediate states in an accumulator (e.g., Merkle tree), and then II\u2019s provers can add a witness to that commitment alongside the proof of their execution. Reflection At the risk of overselling the feature discussed in this post, I think that the construction I\u2019m proposing here extracts the best of all architectures: Bitcoin, Ethereum (rollup-centric roadmap), Solana\u2014an internet-speed version of Nakamoto base layer (verification-oriented), a zk-based computation layer, and a Solana-like unified defragmented state. L2 throughput regulation and transaction fee market Controlling L2 Throughput How to control the throughput of L2? Let\u2019s denominate computation with the familiar \u201cgas\u201d unit. How should the gas limit be enforced? Since L1\u2019s sequencers\u2014miners, in Kaspa\u2014are the only entity capable of selecting and prioritizing transactions entering the system, the gas regulation mechanism too must be employed at the L1 layer. The most simplistic design is to convert gas units to mass units, and since the latter are capped per block, so will gas per block be. Such a unidimensional restriction pools together resources of different natures\u2014L1 mass (script computation and storage) and L2 proving loads\u2014and this is economically inefficient: It implies, for instance, that a user issuing a gas-heavy, storage-light transaction may be outcompeted by users issuing storage-heavy transactions, despite the fact that she imposes no externality on them and can be co-approved without consuming the same resource. We should thus keep the mass and gas throughput constraints decoupled, namely, to provide a two-dimensional throughput restriction on blocks in the form: mass(block) < mass_limit AND gas(block) < gas_limit. This proposal implies miners will face a two-dimensional knapsack problem when selecting transactions from the mempool. Note: The very same discussion applies to our coupling of L1 script computation mass and KIP-9 storage mass. We opted to couple them nonetheless under the same mass function and accept the theoretical economic inefficiency. Notice that a surge in demand for L2 operations will not translate to higher revenue for provers\u2014miners, the selectors of the L1 club, will collect the profits This profit flow seems okay, since the gas limit implies provers\u2019 load does not increase (beyond the limit) in times of higher demand; need to think about this more. Another proposal that came up is to run a variant of the EIP-1559 mechanism on the gas units, which (i) would flow base fee profits to provers, and which (ii) would remove the complexity of running a dual knapsack, as it provides an in-consensus sufficiently precise price per gas unit. Avg vs peak gas Limit Either way, when setting gas_limit, one consideration to put in mind is the gap between the average and the peak load on a prover: While logic zone I may typically comprise 10% of the gas capacity in blocks, its relative demand can reach 100% in peak demand, when users want to interact with it more than anything else. Here too, an economically-efficient design would restrict the block\u2019s gas per logic zone. This, however, would result in an n-dimensional knapsack problem for constructing a block out of the mempool, and so we are currently opting for a simpler design with one gas_limit per block, acknowledging the economic suboptimality. Funding Schemes for Provers With either of the above mechanisms, L2 projects may conceive of additional funding schemes for provers. From an ecosystem vantage point, it is imperative that these fees be given in KAS, contribute to the flywheel of Kaspa, and align all players. Again, I emphasize that I\u2019ve summarized here ideas and considerations raised by aforementioned co-contributors. Some of the topics discussed here are still WIP and open for discussion, hence the research post format.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2024-11-12T23:32:23.476Z",
      "updated_at": "2024-12-03T08:16:09.382Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/atomic-composability-and-other-considerations-for-l1-l2-support/193/1",
      "category_id": 11
    },
    {
      "post_id": 239,
      "post_number": 1,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "This thread is here to house a discussion about KIP9 and our approach to mitigating UTXO bloat using quadratic costs via local constraints. This is to house discussions both about KIP9 and the research paper (which would be linked here as soon as the preprint is published).",
      "raw_content": "",
      "author": "Deshe2",
      "created_at": "2024-03-29T08:46:56.387Z",
      "updated_at": "2024-03-29T09:10:04.936Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/1",
      "category_id": 1
    },
    {
      "post_id": 247,
      "post_number": 2,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "Hello, Thank you for the great work in (and writeup of) KIP9. The formulas look so \u201csimple\u201d but I am sure a whole lot of thinking and considerations went into this work. However my question is more legalese/practical. It concerns this section of KIP9: \" The downside to this solution is that the merchant must constantly have a hot wallet available and cooperate with the customer to create a mutually signed transaction. In a following KIP, we will specify auto-compounding wallet addresses, where UTXOs owned by such addresses will allow anyone to add to their balance without the owner of the UTXO having to sign it. Among other applications, this mechanism will allow the millionaire to purchase ice cream as described above, using her wallet alone.\" Would the merchant be able to set limits on people (addresses) that can append to their hot wallet? For example, let us say there is an address widely known to be associated with an hacker or some rogue organization. I would imagine that it needs to be possible for a merchant to prevent that hacker/organization from spending their ill-gotten gains at my crypto-based business. Or another case, someone steals 1 million tokens/coins and in a fit of sudden generosity, she/he distributes the tainted cash to as many hot wallets (owned by innocent merchants) as possible. Now those merchants are roped into potential AML, criminal \u2026investigations. I have lost my train of thought on this. I suppose this line of thinking is asking crypto to do more (enforce) than is possible with cash. If I have a tomato stand and a customer comes in, it is not always possible for me to know that they got their cash from an armed robbery, embezzlement or whatever. There are also anti-discrimination laws that could prevent me from denying them service. However,",
      "raw_content": "",
      "author": "dr_g",
      "created_at": "2024-03-30T17:18:18.892Z",
      "updated_at": "2024-03-30T17:18:18.892Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/2",
      "category_id": 1
    },
    {
      "post_id": 248,
      "post_number": 3,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "Like any other cryptocurrency, also in Kaspa, one can send funds to any address without getting any kind of \u201cpermission\u201d from the receiver. This has nothing to do with this suggestion. The new thing here is that the payer can spend a UTXO owned by the payee, as long as the transaction creates a corresponding output UTXO with a larger amount going to the same payee address. The concern with such an approach would be the ability of spammers to interfere with ongoing payments by spending the same UTXOs. The KIP will circumvent this by allowing the address owner to specify a minimum increment value. Other intermediate designs are possible: for instance such addresses can require a semi-sig which requires knowing some secret (which is not the full private key only known to the owner). This way the seller can only share the secret with trusted customers.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-03-30T18:01:46.258Z",
      "updated_at": "2024-03-30T21:26:36.116Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/3",
      "category_id": 1
    },
    {
      "post_id": 249,
      "post_number": 4,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "Thank you, Michael, for this clarification. Really appreciate it.",
      "raw_content": "",
      "author": "dr_g",
      "created_at": "2024-03-30T21:41:28.291Z",
      "updated_at": "2024-03-30T21:41:28.291Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/4",
      "category_id": 1
    },
    {
      "post_id": 251,
      "post_number": 5,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "Hi, Seems like the main point is to reject transactions with a very small but multiple/a lot of outputs And the KIP proposes a formula to figure out what tx are of this type Now the question, The proposed formula naturally rejects tx with very small outputs But since those tx might be legal it also proposes a way to send them anyway, by letting a wallet to find a right path for this Doesn\u2019t the fact that this path exists contradicts the very purpose of blocking those transactions for the first place? i.e. the attacker can prepare the needed inputs using his own wallet and then sends the dust",
      "raw_content": "",
      "author": "Tsirkin_Evgeny",
      "created_at": "2024-03-31T09:37:16.775Z",
      "updated_at": "2024-03-31T09:37:16.775Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/5",
      "category_id": 1
    },
    {
      "post_id": 252,
      "post_number": 6,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "That\u2019s exactly the point, the path exists, but it\u2019s costly. While the cost is tolerable for a single point to point payment, it will become intolerable for an attacker wishing to create thousands or millions of such entries. Additionally, for a real genuine user wishing to make a micropayment we also analyze and suggest the costless mutual transaction method",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-03-31T13:17:11.168Z",
      "updated_at": "2024-03-31T13:17:59.718Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/6",
      "category_id": 1
    },
    {
      "post_id": 253,
      "post_number": 7,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "I would say that the entire principle behind the solution is that transactions are not rejected (In practice they are, for several reasons, e.g. P2P policies or block mass limits, but the validity of the solution does not rely on this limit). Rejecting transactions means setting thresholds, and thresholds are generally bad, because they are arbitrary and usually based on ephemeral metrics such as the current Kaspa price, the current average fee, etc\u2026 The idea here is to allow any transaction, but choosing a smart policy for pricing these transactions. The path exists, and if you want to use it even though it is less than optimal, that\u2019s your prerogative. The pricing is set in such a way that you couldn\u2019t do meaningful damage without spending huge resources. So it isn\u2019t about legal/illegal, it is about having to pay for your decisions.",
      "raw_content": "",
      "author": "Deshe2",
      "created_at": "2024-03-31T18:04:28.614Z",
      "updated_at": "2024-03-31T18:04:28.614Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/7",
      "category_id": 1
    },
    {
      "post_id": 260,
      "post_number": 8,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "Thank you guys for the response I was thinking - the KIP is trying to adjust the price of the tx alone This make sense since it makes the calculation stateless and thus cheaper. However, Why not make it stateful by i.e. counting the number of total \u201cmass\u201d in last period of time and adjust the price/rules based on that. This imitates the real world more precise - if there is a congestion in the network you get higher fees. It is also free from the downside of complicating micropayments in \u201cnormal\u201d circumstances.",
      "raw_content": "",
      "author": "Tsirkin_Evgeny",
      "created_at": "2024-04-03T12:32:43.007Z",
      "updated_at": "2024-04-03T12:32:43.007Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/8",
      "category_id": 1
    },
    {
      "post_id": 274,
      "post_number": 9,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "Using dependence has extremely high costs in terms of the ability of the network to scale, and I am not sure I see any benefit. Obtaining such strong guarantees using only local calculation is a magical property one should have an exceptionally good reason to concede.",
      "raw_content": "",
      "author": "Deshe2",
      "created_at": "2024-04-05T15:29:51.413Z",
      "updated_at": "2024-04-05T15:29:51.413Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/9",
      "category_id": 1
    },
    {
      "post_id": 326,
      "post_number": 10,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "Hi. Maybe i am a little late on this topic, but let me ask something: If I am a very famous merchant of automated microservices that accepts kaspa. My address will have some UTXO with spend script that anyone of my millions clients to use to spend sending me the same ammount plus the service fee. What about the concurrency in this case? Won\u2019t even I be blocked to spend this UTXO competing with my own clients to use it?",
      "raw_content": "",
      "author": "patrickdalla",
      "created_at": "2024-11-13T00:57:05.451Z",
      "updated_at": "2024-11-13T00:57:05.451Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/10",
      "category_id": 1
    },
    {
      "post_id": 329,
      "post_number": 11,
      "topic_id": 159,
      "topic_title": "Quadratic storage mass and KIP9",
      "topic_slug": "quadratic-storage-mass-and-kip9",
      "content": "The final design implemented here is actually more broad and allows designs such as using secondary secrets, limited one time borrows, and so on, see examples. A merchant with such millions of clients still has a natural concurrency limit. For instance 1k clients online making concurrent payments within timeframes of 1-5 seconds. Such a merchant should manage ~1k additive addresses (each with 1 or more UTXOs) which are distributed and managed tightly between the current clients. If tight management is undesired, he can manage a pool of 100k (>> 1k) addresses and rely on randomness to reduce collisions to be negligible. My main point is \u2013 with threshold additive addresses we\u2019re in the realm of engineering creativity.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-11-13T09:56:47.888Z",
      "updated_at": "2024-11-13T09:56:47.888Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/quadratic-storage-mass-and-kip9/159/11",
      "category_id": 1
    },
    {
      "post_id": 255,
      "post_number": 1,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "The following proposal draft is intended for fostering discussion and gathering various conceptual and technical feedback before turning into a well-formed KIP. This is a follow-up on the micropayments discussion initiated by KIP9. The basic idea is to allow reception of coins in an additive way, where a UTXO owned by the recipient can be spent by the payer transaction (without a signature) as long as the same transaction creates a corresponding output UTXO with a larger amount going to the same recipient address. For an informal background see here. Opt-in rather than opt-out Allowing every UTXO to be spent this way is an opening for spam attacks, where a user trying to spend his own UTXOs can be outraced by an attacker spending the same UTXO through the new mechanism (by adding negligible amounts to it). This would essentially remove the underlying assumption of almost any wallet which assumes exclusive access to spending his own UTXOs. Therefor our suggestion is to allow this only for special addresses (opt-in) where such an address can be configured in various ways to protect from concurrent-spending spam attacks. In the context of this document we\u2019ll name such addresses additive addresses. Minimum increment threshold The idea is to have the address (or more concretely, the script_pub_key) define a minimum increment value. Any transaction spending the UTXO and adding to it less than the threshold will be invalid. By the owner specifying this threshold depending on his use case and market value conditions, such spam attacks will become costly and reduced to the point they only benefit the owner. Semi-signatures Another possibility is to allow spending only with a semi-sig which requires knowing some secret (which is not the full private key only known to the owner). This way the payee can share the secret only with trusted senders. Technical aspects Script public key format At the consensus engine level, a Kaspa address is encoded into a ScriptPublicKey. An additive address with a configured increment threshold can be encoded as a ScriptPublicKey in two possible ways: (i) a special version of p2pk with the threshold appended at the end (u64); (ii) using p2sh and hashing the usual spk + threshold to a single hash. The first method has the following main advantage: The new spk will have a known prefix (the version + the usual spk), which will allow easy searching for such addresses by wallets (possibly recreated from a seed w/o additional info). It\u2019s worth noting that Kaspa nodes use key-value based databases which support prefix-key searches, thus allowing for such prefix queries in an efficient way. The second method is more robust and general and fits into existing script engine schemes. We can also support both options. New script-engine opcodes The main missing component in the script engine is the ability to read from the transaction outputs. It seems that the most straightforward way is to add a new opcode which allows reading an output at index (and accessing the amount and recipient spk). This way, the script engine can make the required verification by accessing the spent spk, the recipient spk and the respective amounts. The signature can be used for adding any additional info needed such as providing proof of a semi-sig etc. Additional remarks The transaction input index should be mapped to a known output index to avoid multiple spends being mapped to the same output (perhaps simply i to i, forcing such inputs to be first) It\u2019ll be the first time that validating a sig requires looking at tx outputs (so broader context). This can be crucial. For instance such \u201csignature\u201d validations cannot be cached as valid Besides the aforementioned schemes there can be a type of special address which allows additive mining. Meaning miners can use this UTXO as input to coinbase txs and output the reward + previous balance. This can be seen as a minimum increment threshold where the threshold is dynamic and is equal to the current mining reward. To avoid any kind of attack, the input UTXO will be used only if it\u2019s available at the time the coinbase tx is built (by the merging block), otherwise we fallback to normal behavior. This is an important feature for 10BPS where nearly 1M rewards will be mined per day. This writeup is an intermediate summary of ongoing discussions with @ori, @Deshe2, @hashdag, @biryukovmaxim, @coderofstuff, @ey51",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-04-03T10:34:02.737Z",
      "updated_at": "2024-04-03T10:47:23.023Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/1",
      "category_id": 9
    },
    {
      "post_id": 263,
      "post_number": 2,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "Technical detail that missed: should both ways(P2SH and new script class) be applied to both schemes: threshold, semi-signature? I think P2SH works for both. However Leaking of semi signature can lead to the same attack. P2SH can add additional protection mechanisms like secret+min_threshold+lock_time or whatever logic allowed by engine/opcodes P2pk is only okay for threshold.",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2024-04-03T12:53:51.779Z",
      "updated_at": "2024-04-03T12:53:51.779Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/2",
      "category_id": 9
    },
    {
      "post_id": 265,
      "post_number": 3,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "Why would the semi signature leak the secret? can\u2019t we provide same guarantees as usual sigs? you mean it\u2019s leaked by showing it to the payer? if so, I don\u2019t think that\u2019s an issue, it\u2019s a type of trust model",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-04-03T14:05:52.030Z",
      "updated_at": "2024-04-03T14:08:06.264Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/3",
      "category_id": 9
    },
    {
      "post_id": 266,
      "post_number": 4,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "As mentioned, the minimum increment threshold [MIT, :)] will be readily adjustable. That is good. If i understand correctly, the MIT will be in units of Kaspa. Is this correct? And now a really uneducated question: Could the merchant provide the MIT in units of $ or \u20ac, \u00a3 and the system automatically converts to Kaspa. Cryptocurrencies fluctuate wildly (often within minutes) and widely (intraday moves of up to 100% or more), so it could make sense for some merchants to put a floor in some desired fiat currencies. Obviously the system would need instantaneous knowledge of the fiat value per Kaspa, as well as conversion ratios between variousnfiat currencies. Best.",
      "raw_content": "",
      "author": "dr_g",
      "created_at": "2024-04-03T20:07:35.790Z",
      "updated_at": "2024-04-03T20:07:35.790Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/4",
      "category_id": 9
    },
    {
      "post_id": 269,
      "post_number": 5,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "@biryukovmaxim has mentioned this elsewhere, but I think it\u2019s worth noting here that we\u2019d need to consider the experience for recovering P2SH addresses created for this purpose when recovering wallets. Since normally, we only recover P2PK addresses in those. The opt-out mechanism isn\u2019t discussed above, so I\u2019ll mention it: The owner can always use their regular private key, sign the UTXO and spend it like any other UTXO. When they do, they effectively opt-out of allowing to use that UTXO for further micropayments. I want to explore further the P2PK route. spk + threshold is mentioned above with the intent that threshold is a deterrent for spamming. Consider this scenario where a wallet has a single 200M Kaspa UTXO and the owner uses this KIP10 mechanism to convert (\u201cKIP10\u2019ize\u201d) that into a 200M UTXO that can be used for micropayments by others, with threshold set to 0.00000001 KAS. A dedicated \u201chater\u201d with a budget of 10,001 KAS can deny the use of that 200M UTXO by creating a chain of 100,000,000 transactions each reusing the 200M UTXO one after another after adding a small amount to it (2 utxos, 2 outputs; assume fee = 0.0001, increment = 0.00000001) for a duration of time. At 1 BPS, that should be ~1157 days since chained transactions can\u2019t go in the same block so they\u2019d have to go in (at best) consecutive blocks. The \u201chater\u201d may have paid a hefty amount, but they have denied the use of the UTXO for a significant period of time enough to adversely affect another. My main questions with the above absurd scenario are: Should there be a maximum amount for the UTXO that can be KIP10\u2019ized? Likely the answer should be no, since arbitrary thresholds are bad. But this brings up a UX concern: wallets should warn users about using this mechanism when they try to convert large utxos to the KIP10\u2019ized ones. Maybe ask the owner to create a smaller UTXO then KIP10\u2019ize that. Should there be a limit (set by the user) to the number of times a KIP10\u2019ized UTXO can keep getting reused? Possibly by adding a counter to the SPK. Maybe spk + threshold + limit. Upon use of this UTXO, the limit must also be decreased. When limit reaches 0, it can no longer be reused. The owner then can opt-in again and re-KIP10\u2019ize this UTXO, setting a new limit (or even a new threshold) If the owner attempts to spend a KIP10\u2019ized UTXO (so they\u2019ll have their signature from their private key), should their transaction be prioritized over another that uses the UTXO but doesn\u2019t have a signature?",
      "raw_content": "",
      "author": "coderofstuff",
      "created_at": "2024-04-04T05:22:49.514Z",
      "updated_at": "2024-04-04T05:22:49.514Z",
      "reply_count": 3,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/5",
      "category_id": 9
    },
    {
      "post_id": 270,
      "post_number": 6,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "If the owner attempts to spend a KIP10\u2019ized UTXO (so they\u2019ll have their signature from their private key), should their transaction be prioritized over another that uses the UTXO but doesn\u2019t have a signature? It would be nice to have the priority. Not sure how easy is to implement it on probably mempool level. Should there be a limit (set by the user) to the number of times a KIP10\u2019ized UTXO can keep getting reused? Possibly by adding a counter to the SPK. Maybe spk + threshold + limit. Upon use of this UTXO, the limit must also be decreased. When limit reaches 0, it can no longer be reused. The owner then can opt-in again and re-KIP10\u2019ize this UTXO, setting a new limit (or even a new threshold) Another ideas: frequency ratio, or time_lock/sequence_lock. Frequency in meaning that utxos can be collected once per x blocks from not a signer. Time_lock/sequence_lock - can be collected up to the time/sequence number, then work as regular p2pk",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2024-04-04T05:48:16.593Z",
      "updated_at": "2024-04-04T05:48:16.593Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/6",
      "category_id": 9
    },
    {
      "post_id": 272,
      "post_number": 7,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "Fyi I\u2019m not a Blockchain expert, simply making observations from a general perspective. Feel free to ignore or criticise if there\u2019s a technical detail invalidating me The problem with the threshold model is that it doesn\u2019t solve this problem: For a high tx volume business, they could be outraced by their non-malicious clients for their additive UTXO too. The problem with Semi Sigs is that it does not prevent malicious actors. The problem here essentially stems from the inability to have a Critical Section in a decentralised multi-threaded world. In an ideal world, one would have a RW-Lock or something similar where the owner of the wallet would have their Mutex take precedence. I have some off-the-cuff ideas for exploring this problem: Have a designated UTXO which can be the only source of any micro-payments. Once that UTXO is used in a transaction, the output UTXO becomes the next Exclusive Additive UTXO (EAU). If an account has no balance or no previous EAU, create one with a value of 0(?). When the user wants to have exclusive access to the EAU, they simply create a new one and wait for future additive transactions to point to that one. This also avoids the possibility of chaining attacks as described above, as it allows a user to gain exclusive access to the EAU by rerouting traffic away from it in advance. Any transaction by the user involving an EAU could be composed of 2 sub-transactions: A signal to lock the EAU from any unsigned transactions until the next signed transaction The actual transaction signed by the wallet owner, which unlocks UTXO after",
      "raw_content": "",
      "author": "GGabi",
      "created_at": "2024-04-04T14:13:28.544Z",
      "updated_at": "2024-04-04T15:09:22.923Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/7",
      "category_id": 9
    },
    {
      "post_id": 273,
      "post_number": 8,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "GGabi: a high tx volume business, they could be outraced by their non-malicious clients for their additive UTXO too A high tx volume business can manage a set of additive addresses per some local unit, like a checkout in the supermarket, or a server session, etc. Basically, If malicious users get penalized and all you need is management of resources, it can be managed by the wallet owner in many ways.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-04-04T16:40:39.855Z",
      "updated_at": "2024-04-04T16:40:39.855Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/8",
      "category_id": 9
    },
    {
      "post_id": 276,
      "post_number": 9,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "It sounds to me like both options are are too prone to human error/selfish behavior. Minimum increment threshold: the problem is a merchant wouldn\u2019t care about the security of the network as a whole. She would care about her own security, followed by ease of usage, and the easiest usage, the one that will require the least action for both for her and her customers, would be having as little a threshold as possible. It might be bad for the network as a whole, but it\u2019s good for her as an individual user, she has no incentive to set a high threshold as far as I can tell. Semi-signatures: Similarly, I think leakage of the semi-sig will happen by the merchant herself. After a while of dealing with the back and forth of sending every customer the semi-key, merchants will lose patience and just advertise this on their web page. Why shouldn\u2019t they? they don\u2019t risk anything. Sure, the above will not occur for all the additive addresses, but it can happen with enough addresses, and a spammer could then use those to spam. The users of these addresses wouldn\u2019t be individually effected from the attack, you can only get them to move to a safer address by asking for their good will.",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2024-04-08T01:32:11.949Z",
      "updated_at": "2024-04-08T01:32:11.949Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/9",
      "category_id": 9
    },
    {
      "post_id": 277,
      "post_number": 10,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "FreshAir08: It might be bad for the network as a whole, but it\u2019s good for her as an individual user, she has no incentive to set a high threshold as far as I can tell The network cares nothing about such spam. It simply means the UTXO is spent (and increased) by the spammer rather than by an actual customer. The only one who should care is the merchant which cannot carry on deals",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-04-08T07:38:30.109Z",
      "updated_at": "2024-04-08T07:38:30.109Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/10",
      "category_id": 9
    },
    {
      "post_id": 278,
      "post_number": 11,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "Ok. I completely misunderstood the suggested problem then. I thought the worry was that state bloaters could piggyback on these inputs to artificially bloat the number/budget of input. How exactly are these addresses to be dealt with by the storage mass formula? are they completely ignored? if they are just added as both inputs and outputs, doesn\u2019t that allow a way to artificially increase the budget? taking a naive look at the bound provided in kip9 (\\sum_tx(storage_mass(tx)>=C*growth^2(G)/budget), by substantially increasing the budget you would need less storage mass. Would a more tight analysis apply only to the attacker\u2019s own budget and disregard the budget coming from the additive addresses?",
      "raw_content": "",
      "author": "FreshAir08",
      "created_at": "2024-04-08T09:59:09.573Z",
      "updated_at": "2024-04-08T11:10:10.010Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/11",
      "category_id": 9
    },
    {
      "post_id": 280,
      "post_number": 12,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "FreshAir08: Would a more tight analysis apply only to the attacker\u2019s own budget and not the budget of the additive addresses? Nice observation. Yes, a tighter analysis would show exactly that. In fact, it\u2019s part of a more general phenomena where the budget needs to be equally spread across the growth in order to reach the bound tightly (where in the case of additive addresses they only increase in budget while keeping the same number of entries) The tighter analysis is also required for showing that sufficient budget is locked through time (with correlation to the growth is consumes). The relevant bound is a more complicated expression which is still wip.",
      "raw_content": "",
      "author": "michaelsutton",
      "created_at": "2024-04-08T11:43:34.514Z",
      "updated_at": "2024-04-08T11:43:34.514Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/12",
      "category_id": 9
    },
    {
      "post_id": 282,
      "post_number": 14,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "Not a full answer but here is some back-of-the-envelope math: Say we have an attacker that wants to break one input with b KAS into k inputs with b/k KAS. Then (using C=1) this should cost (k-1)^2/b mass. Your concern is if they use extra budget (from an additive UTXO or otherwise) of a KAS then they break (a+b) kas into k UTXOs with b/k KAS and a single UTXO with a KAS, and the bound only implies that this would cost k^2/(a+b) mass. So if a\\gg b the cost greatly decreases (and additive UTXOs mean they wouldn\u2019t even have to obtain the a KAS themselves). However, we can easily prove that any attack that breaks a+b KAS into k UTXOs of size b/k and a single UTXO of size a actually costs at least k^2/b mass, regardless of a: Say that the attack costs v, then if I have a+nb KAS I can repeat the attack n times to create nk inputs of size b/k and a single input of size a for a cost of nv. However, the original bound tells us that such an attack must cost at least (nk)^2/(a+nb) mass, so it holds that nv\\ge(nk)^2/(a+nb). Rearranging we get n\\left(k^{2}-vb\\right)\\le va and this inequality is obviously violated for a sufficiently large n unless k^2-vb\\le 0.",
      "raw_content": "",
      "author": "Deshe2",
      "created_at": "2024-04-11T15:18:07.798Z",
      "updated_at": "2024-04-15T10:54:00.424Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/14",
      "category_id": 9
    },
    {
      "post_id": 284,
      "post_number": 15,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "you can pick out users that opt into a better peel chain?",
      "raw_content": "",
      "author": "Alcyone",
      "created_at": "2024-04-12T11:28:01.843Z",
      "updated_at": "2024-04-12T11:28:01.843Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/15",
      "category_id": 9
    },
    {
      "post_id": 298,
      "post_number": 16,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "I\u2019m sorry, I didn\u2019t understand the question. What do you mean by a \u201cpeel chain\u201d?",
      "raw_content": "",
      "author": "Deshe2",
      "created_at": "2024-04-15T10:43:44.592Z",
      "updated_at": "2024-04-15T10:43:44.592Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/16",
      "category_id": 9
    },
    {
      "post_id": 313,
      "post_number": 17,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "coderofstuff: f the owner attempts to spend a KIP10\u2019ized UTXO (so they\u2019ll have their signature from their private key), should their transaction be prioritized over another that uses the UTXO but doesn\u2019t have a signature? Currently we have rbf, owner can icrease fee to be prioritized over borrowers or vice versa",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2024-10-15T07:18:06.703Z",
      "updated_at": "2024-10-15T07:18:06.703Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/17",
      "category_id": 9
    },
    {
      "post_id": 314,
      "post_number": 18,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "coderofstuff: Should there be a limit (set by the user) to the number of times a KIP10\u2019ized UTXO can keep getting reused? Possibly by adding a counter to the SPK. Maybe spk + threshold + limit. Upon use of this UTXO, the limit must also be decreased. When limit reaches 0, it can no longer be reused. The owner then can opt-in again and re-KIP10\u2019ize this UTXO, setting a new limit (or even a new threshold) such script/scenario is implemented here: github.com/kaspanet/rusty-kaspa Implement one-time and two-times threshold borrowing scenarios Commit by biryukovmaxim - Add KIP-10 Mutual Transaction Opcodes and Hard Fork Support kaspanet:master \u2190 biryukovmaxim:kip-10-mutual-tx - Add threshold_scenario_limited_one_time function - Add threshold_scenario_limi\u2026ted_2_times function - Create generate_limited_time_script for reusable script generation - Implement nested script structure for two-times borrowing - Update documentation for both scenarios - Add tests for owner spending, borrowing, and invalid attempts in both cases - Ensure consistent error handling and logging across scenarios - Refactor to use more generic script generation approach",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2024-10-19T10:13:44.689Z",
      "updated_at": "2024-10-19T10:13:44.689Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/18",
      "category_id": 9
    },
    {
      "post_id": 316,
      "post_number": 19,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "Btw, I suggest changing the OpOutputSpk, OpOutputAmount behavior to have an index argument, where -1 will refer to the index of the current input (or add OpCurrInputIdx). This will allow more complex applications (e.g. checking that a payment is being divided to two addresses in some ratio). I think that we shouldn\u2019t do the same for the input opcodes, because it\u2019ll break the assumption that you only need the current input utxo to validate a certain input",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2024-10-29T06:15:12.969Z",
      "updated_at": "2024-10-29T06:15:12.969Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/19",
      "category_id": 9
    },
    {
      "post_id": 317,
      "post_number": 20,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "I like the idea in general. I believe that having relative indexes is more flexible and useful. To cover both case Im going to introduce a flag, depending on that the engine will apply absolute/relative logic",
      "raw_content": "",
      "author": "biryukovmaxim",
      "created_at": "2024-10-29T09:56:58.827Z",
      "updated_at": "2024-10-29T09:56:58.827Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/20",
      "category_id": 9
    },
    {
      "post_id": 318,
      "post_number": 21,
      "topic_id": 168,
      "topic_title": "Auto-compounding/Additive addresses \u2014 KIP10 draft",
      "topic_slug": "auto-compounding-additive-addresses-kip10-draft",
      "content": "I think it\u2019s best to use OpCurrInputIdx to support both relative and absolute in a more elegant and composable way",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2024-11-04T17:58:13.146Z",
      "updated_at": "2024-11-04T17:58:13.146Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/auto-compounding-additive-addresses-kip10-draft/168/21",
      "category_id": 9
    },
    {
      "post_id": 309,
      "post_number": 1,
      "topic_id": 189,
      "topic_title": "KIP 6 discussion thread",
      "topic_slug": "kip-6-discussion-thread",
      "content": "This is the discussion thread for KIP 6",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2024-10-07T08:53:02.978Z",
      "updated_at": "2024-10-07T09:19:41.300Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/1",
      "category_id": 1
    },
    {
      "post_id": 310,
      "post_number": 2,
      "topic_id": 189,
      "topic_title": "KIP 6 discussion thread",
      "topic_slug": "kip-6-discussion-thread",
      "content": "Right now calculating the pruning point from block b POV is quite annoying: P is Self::finality_depth() + Self::merge_depth_bound() * 2 + 4 * Self::prev_mergeset_size_limit() * Self::ghostdag_k() as u64 + 2 * Self::ghostdag_k() as u64 + 2 and the difference between two pruning points is at least F, which makes calculations quite annoying. Also, for KIP6 related calculations, we can\u2019t easily calc prev_posterity(b) because each block points to its pruning point, and prev_posterity(b) is somewhere between b and b.pruning_point. So I suggest: Change P to 3F (or the equivalent of 72 hrs) replacing the field b.pruning_point with b.prev_posterity. Optional: set the posterity period to 1 hour This will also simplify the functions are_pruning_points_in_valid_chain, next_pruning_points_and_candidate_by_ghostdag_data and are_pruning_points_violating_finality Another point is that because of the current structure we can\u2019t validate finality violations of less than 2F when replacing a pruning point proof. This will fix it. @michaelsutton @Deshe2",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2024-10-07T09:02:16.728Z",
      "updated_at": "2024-10-07T09:04:27.430Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/2",
      "category_id": 1
    },
    {
      "post_id": 311,
      "post_number": 3,
      "topic_id": 189,
      "topic_title": "KIP 6 discussion thread",
      "topic_slug": "kip-6-discussion-thread",
      "content": "And to avoid the ambiguity of the field b.pruning_point/prev_posterity, 72 hours after the HF we\u2019ll hardcode the new pruning point and tell are_pruning_points_in_valid_chain and are_pruning_points_violating_finality to stop the check there",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2024-10-07T09:05:02.805Z",
      "updated_at": "2024-10-07T09:05:02.805Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/3",
      "category_id": 1
    },
    {
      "post_id": 312,
      "post_number": 4,
      "topic_id": 189,
      "topic_title": "KIP 6 discussion thread",
      "topic_slug": "kip-6-discussion-thread",
      "content": "And unrelated suggestion: I suggest to replace the notion of PoChM Merkle root with an MMR (of all blocks until prev_posterity or even of the whole history). This should simplify the design",
      "raw_content": "",
      "author": "someone235",
      "created_at": "2024-10-07T09:08:08.034Z",
      "updated_at": "2024-10-07T09:08:08.034Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/kip-6-discussion-thread/189/4",
      "category_id": 1
    },
    {
      "post_id": 156,
      "post_number": 1,
      "topic_id": 95,
      "topic_title": "Some of the intuition behind the design of the invalidation rules for pruning",
      "topic_slug": "some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning",
      "content": "(A formal proof of the security of our pruning protocol can be found here, the purpose of the current post is to provide an intuitive exposition) Finality is the practice of not allowing reorgs below some fixed depth, pruning is the practice of removing unnecessary block data. The two are strongly coupled: blocks which weren\u2019t finalized may not be pruned. On the other hand, it is not generally true that finalized blocks may be pruned, as the data they contain might be needed to compute the UTXO set of incoming blocks. More concretely, a pruning block P is a block with the property that any blocks which affect the UTXO set of the virtual block are in its future. Differently put, if when B was discovered it was not in P.Future, then at no point in time will B be in Virtual.Past. In order to make things meaningful in the context of consensus, we apply the usual trick: we define the pruning block P of a block B, B.P, and then define \u201cthe\u201d pruning block to be P=Virtual.P. The way we do this is straightforward: we set a pruning parameter \\pi, and then define B.P to be the \u201cblock at depth \\pi from B \u201d, that is, B.P is the most recent block in the selected chain of B such that B.BlueScore - B.P.BlueScore > \\pi. We similarly define a finality parameter \\phi < \\pi, and define B.F to be the block at depth \\phi, and in particular \u201cthe\u201d finality block is F=Virtual.F. The finality rule becomes very straightforward: any block whose inclusion would cause a reorg after which F is not in the selected chain is considered a finality breaking block. The handling of such a scenario is to alert the community while halting approval of any and all transactions until the community manually resolves the conflict. From the point of view of designing a pruning algorithm, we may assume that this never happens. Our goal is to design a pruning algorithm which is resistant to pruning attacks. A pruning attack is a scenario where an adversary manages to force the network to require the data of an already pruned block. In particular, of a block outside P.Future. Since P is guaranteed to be in the selected chain by our definitions, and since \\phi<\\pi, the only way to achieve this is to force the network to require the data of a block in P.Anticone. Note that no approach can deterministically prevent this scenario. For any block B, a 51% attack can easily make it so that B would be in Virtual.Past at some point, and it follows that any attacker could achieve this feat with positive probability. Our best possible approach is to design the mechanism such that the probability of success decays exponentially fast as a function of the pruning parameter \\pi. This kind of \u201ccompromise\u201d is not unique to our protocol, and is ubiquitous to any kind of security which relies on PoW. Our design approach employs the apparatus of invalidation rules. We set some criteria for when a block is considered invalid, and require that the virtual block is always valid, with the hopes that the rules imply that a pruning attack is impossible for a 49% attacker (or more accurately, that the probability of ever carrying such an attack diminishes exponentially with \\pi for any \\alpha attacker where \\alpha < 1/2 is fixed). Such rules must be in consensus, in the sense that one could read from the structure of the graph (and no other external data such as clocks and time signatures) whether a block is valid. This requirement is utterly crucial, since invalidating blocks with respect to inconsistent data will almost certainly lead to some splitting attack in which some blocks are considered invalid by some honest nodes, but are required to calculate the UTXO sets of some other honest nodes. The first invalidation rule is self evident: if a block is invalid then so is any block pointing to it. Since we discard such blocks, we can\u2019t trust blocks which point at them as the data is unavailable. Note a crucial subtlety: we do not require that Virtual.Past will not include any blocks in P.Anticone. Such a requirement is far too strong and there are many legitimate scenarios in which it is infringed, even when the network is completely honest (in particular, if P'\\in P.Anticone and P,P'\\in Virtual.Past then P' will be in the past of any block pointing having the same parents as Virtual). Our requirement has to do with the time the node learned of B. By appealing to time we are actually breaking the consensus: network delay and network topology makes it so different nodes learn of blocks in different orders, making it possible that B violates this condition for one node but not for the other. This makes it impossible to simply require that \u201cblocks which were in P.Anticone when they arrived are invalid\u201d, since this is not a consensus rule. The solution is to design the system in such a way that even if a block is only accepted by some of the nodes, its data will never actually be required. Intuitively, such blocks diverge from the selected chain in a very deep point: either P itself or a block very close to P (in particular, a block which was created at most twice the roundtrip time of a block since P was discovered by an honest node). This could be used to enforce the policy that such blocks are not needed, but designing a set of rules which achieves this is quite delicate. The high-level idea of the pruning mechanism is to force any attacker who wishes to perform such an attack, in which the node requires data from a block which was in P.Anticone at the time it was discovered, into a block race. That is, making such an attack possible only if the attacker is able to create blocks at a higher rate than the honest network. We introduce how this is achieved from the bottom up. The first thing one might try is to invalidate any block B which directly points at a block in B.P.Anticone. This is a good start, but also obviously not sufficient, as it could be easily sidestepped by using an \u201cintermediate\u201d block C which points to a block in P.Anticone and is also a parent of B. A next reasonable step is to say \u201cOK, so we invalidate B if it has a block P\u2019 in P.Anticone in its past, and there is no block in B.Blueset with P\u2019 in its past. Still, no dice. An attacker can overcome this by having the intermediate block C also point to a block close enough to B so that C be in B.Blueset, but P\u2019 not in C.P.Anticone. It might not be possible to create such a C, but in that case we could just \u201cadd more steps\u201d, that is, create C in B.Blueset and C\u2019 in C.Blueset such that P\u2019 is not in C\u2019.P.Anticone. If this is also not possible, we can add yet another step. The overall number of steps we will require is bound by a low constant (up to negligible probability), making a pruning attack feasible. What we need to do is to somehow verify that the block D in B.Past which is also in B.P.Anticone is not recent, and that it was in the graph long before B. We can\u2019t rely on timestamps to do so. What we can do, is to rely on the protocol. In particular, if D has been known for a while, then there should be honest blocks which know of it. In particular, there should be a block C in B.Blueset such that B.F is in C.SelectedChain (ensuring C is much newer than B.P) and D is in C.Past. We call such a block C a \u201ckosherizing\u201d block. It is a block which is on one hand reliable, and on the other hand familiar with D. Note that there are two possibilities for C: if D is not in C.P.Anticone, then (assuming C was discovered before B, an assumption we revisit later) D could not have been in P.Anticone when it was discovered. Otherwise, C must also admit a kosherizing block, or it is invalid, and the argument continues inductively. This leads us to our second invalidation rule: a block B is invalid if there is a block D in B.Past which is in B.P.Anticone, for which there is no kosherizing block. Are these two rules sufficient to prevent a pruning attack? Not quite, but we are getting there. The problem is that an adversary could still carry out what we call a \u201cclimbing attack\u201d. In this attack the adversary first creates a block K which points both to D and to the finality block F. This block will act as a kosherizing block. It then creates a succession of blocks, the first one, K_1, pointing at K, and at the highest block it can on the selected chain such that K remains in its blue set. The next block, K_2, will point to K_1 and the highest chain block it can point to while keeping K_2 in its selected chain, etc., until creating a block high enough that it remains blue while pointing to it and to the current tip. The amount of blocks needed to create this attack is constant, about f/k. The solution to this problem is to impose a \u201cbounded merge\u201d rule. Recall that the merge set of a block B is defined to be the set of blocks in B.Past which are neither B.SelectedParent nor in B.SelectedParent.Past. The third consensus rule limits the size of this set. That is, we fix some parameter L and impose the rule that a block may not have a merge set of size more than L. We will say something about how L should be chosen in a moment, but for now let us consider the implication of this rule. Remember that the entire goal of the attack is to make it so that a block which was in P.Anticone while it was discovered will never be in Virtual.past. The attacker has the freedom to choose a chain block P\u2019, create a block in P\u2019.Anticone, and withhold it until P\u2019 is the pruning block. All blocks created up to that point must be withheld. Let us first consider an attacker which is restricted to a premining attack. That is, the attack mines some blocks on the side, and then releases them all at once and does not create any more blocks. If the attack was successful, this implies that all the blocks created by the attacker are in the merge set of the new virtual block, and in particular, there are at most L of them. This implies at once that choosing L<f/k makes such attacks impossible. The attacker of course is not bound to this restriction, and may create blocks after the premined blocks were created, in the hope of kosherizing one of the bad blocks further down the line. Now here is the crux: after being published, the honest network will not point at any of the bad blocks, as it will consider all of them red and in particular pointing at them means including in their past a block in P.Anticone with no kosherizing block. The attacker, on the other hand, can not extend the attack to beyond while pointing at chain blocks, since the chain block would be their selected parent, which will make their merge set larger than L. The upshot of all of this is that after publishing the attacker can only make up to L blocks, each of which climbing at most k blocks up the selected chain (as it has to consider the top bad block blue), after which they can not have an honest block be their selected parent. In other words, they are in a block race against the network. At this point the security proof of PHANTOM shows that a <50% attacker will almost certainly lose this race. Of course, this argument seems only relevant to a particular attack vector, but it is provable that any successful attack must create such a succession (so, in particular, this is the optimal attack). So, in summary, the three invalidation rules are that a block B is considered invalid if: B has an invalid parent, B.Mergeset is larger than L, or There is a block C which is both in B.Past and B.P.Anticone such that it has no kosherizing block, where a kosherizing block is a block D in B.Blueset such that B.F is in D.SelectedChain, and such that C is in D.Past The security follows by a theorem (proven by Sutton and myself): under these invalidation rules, and under the assumption that the virtual block must be valid, if the following hold: There is never a reorg of depth more than \\phi, and There is an honest majority, then if C was in P.Anticone when it was discovered, then the probability that at some point in the future C would be in Virtual.Past is exponentially small in p.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-17T10:11:27.452Z",
      "updated_at": "2021-12-01T10:00:40.141Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning/95/1",
      "category_id": 1
    },
    {
      "post_id": 234,
      "post_number": 2,
      "topic_id": 95,
      "topic_title": "Some of the intuition behind the design of the invalidation rules for pruning",
      "topic_slug": "some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning",
      "content": "The great research and what shall I do for my account activate?",
      "raw_content": "",
      "author": "Kalid_Sherefudin",
      "created_at": "2023-10-18T19:41:33.259Z",
      "updated_at": "2023-10-18T19:41:33.259Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/some-of-the-intuition-behind-the-design-of-the-invalidation-rules-for-pruning/95/2",
      "category_id": 1
    },
    {
      "post_id": 185,
      "post_number": 1,
      "topic_id": 122,
      "topic_title": "Miner funds consolidation",
      "topic_slug": "miner-funds-consolidation",
      "content": "Currently the miners of Kaspa create a single UTXO every block they mine. This causes UTXO-set bloat, and creates difficulty for miners to spend their mined coins. This is also a blocker for an increased block rate, 10x block rate will create 10x UTXOs.",
      "raw_content": "",
      "author": "mikez",
      "created_at": "2022-01-03T17:09:01.639Z",
      "updated_at": "2022-01-03T17:09:01.639Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/miner-funds-consolidation/122/1",
      "category_id": 7
    },
    {
      "post_id": 187,
      "post_number": 2,
      "topic_id": 122,
      "topic_title": "Miner funds consolidation",
      "topic_slug": "miner-funds-consolidation",
      "content": "I suggest implementing a special kind of transaction, a compounding one, where source and destination addresses are the same, input UTXOs are strictly coinbase ones, and the output value is slightly greater than the sum of the inputs; this excess is calculated as a certain small coefficient multiplied by a polynomial or exponential function of the number of used coinbase UTXOs and their values in order to encourage the user to combine as many outputs as possible in one transaction, instead of making many small unions that do not give a significant gain to a system. Such transactions are to be launched only from the wallet manually, thus eliminating the need for the mining software to access the user\u2019s password, but it will be rational for the user to automate this process by periodically starting the wallet with the appropriate command, especially for users who will mine for sale and who therefore need the maximum profit at any given time. The command to the wallet can be supplied with parameters, so that it either terminates without executing if the number of unspent outputs is not enough to get the maximum possible benefit (the wallet itself must determine what the limit is, based on the maximum allowed transaction mass), or so that it is executed unconditionally, with the key like \u2018\u2013force\u2019.",
      "raw_content": "",
      "author": "Affiele",
      "created_at": "2022-01-04T20:39:29.313Z",
      "updated_at": "2022-01-04T20:39:29.313Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/miner-funds-consolidation/122/2",
      "category_id": 7
    },
    {
      "post_id": 188,
      "post_number": 3,
      "topic_id": 122,
      "topic_title": "Miner funds consolidation",
      "topic_slug": "miner-funds-consolidation",
      "content": "and the output value is slightly greater than the sum of the inputs But this effects the total emission of coins, and also gives more value to larger miners, which is undesired from decentralization standpoint",
      "raw_content": "",
      "author": "msutton",
      "created_at": "2022-01-05T13:46:09.529Z",
      "updated_at": "2022-01-05T14:00:37.346Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/miner-funds-consolidation/122/3",
      "category_id": 7
    },
    {
      "post_id": 189,
      "post_number": 4,
      "topic_id": 122,
      "topic_title": "Miner funds consolidation",
      "topic_slug": "miner-funds-consolidation",
      "content": "In general, one can think of allowing coinbase transactions to have a UTXO input (thus constantly aggregating), however this introduces the following tradeoff/dillema: Spending a UTXO usually requires a signature, thus complicating miner logic and requiring interaction with miner\u2019s wallet Allowing the usage of a UTXO which belongs to the same mining address without a signature, might have many side-effects which are usually avoided in the UTXO model, such as replay attacks and double-spending on behalf of someone by mining to his address",
      "raw_content": "",
      "author": "msutton",
      "created_at": "2022-01-05T14:39:53.253Z",
      "updated_at": "2022-01-05T14:39:53.253Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/miner-funds-consolidation/122/4",
      "category_id": 7
    },
    {
      "post_id": 190,
      "post_number": 5,
      "topic_id": 122,
      "topic_title": "Miner funds consolidation",
      "topic_slug": "miner-funds-consolidation",
      "content": "| But this effects the total emission of coins Yes it does but in a predictable way and with a calculable upper limit.",
      "raw_content": "",
      "author": "Affiele",
      "created_at": "2022-01-05T16:03:16.486Z",
      "updated_at": "2022-01-05T16:03:31.501Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/miner-funds-consolidation/122/5",
      "category_id": 7
    },
    {
      "post_id": 204,
      "post_number": 6,
      "topic_id": 122,
      "topic_title": "Miner funds consolidation",
      "topic_slug": "miner-funds-consolidation",
      "content": "mikez: and creates difficulty for miners to spend their mined coins. If true, this is already sufficient incentive for miners to consolidate their UTXOs. If indeed true, what\u2019s preventing miners from consolidating their UTXOs today? Is it purely the lack of automatic mechanism to consolidate? If so, I propose a flag in the miner app, that when set, remembers the one consolidated or n consolidated UTXOs, and then adds to the mining template a transaction in the same block that consolidates it with the coinbase or even a special transaction that consolidates it into the coinbase. If always set to run this way, this mechanism would ensure there would be just one large UTXO the miner rolls on like a snowball (good name for the flag). A parameter n can be used to define how many snowball UTXOs to keep (instead of one), and it can round robin between them. A \u201cmigration\u201d would need to happen before, where miners consolidate all their UTXOs into n snowball UTXOs. The con is that the miner would have to call the wallet to sign a transaction when it finds a block. To circumvent this, we could have new blocks invalidate the consolidated coinbase txs and validate the consolidating coinbase tx whenever there is a new block that asks to be paid to the same address as an existing block.",
      "raw_content": "",
      "author": "ey51",
      "created_at": "2022-02-16T00:43:02.745Z",
      "updated_at": "2022-02-16T01:27:31.707Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/miner-funds-consolidation/122/6",
      "category_id": 7
    },
    {
      "post_id": 227,
      "post_number": 7,
      "topic_id": 122,
      "topic_title": "Miner funds consolidation",
      "topic_slug": "miner-funds-consolidation",
      "content": "thanks for the awesome information.",
      "raw_content": "",
      "author": "alexsunny123",
      "created_at": "2022-12-16T06:57:57.264Z",
      "updated_at": "2022-12-16T06:57:58.756Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/miner-funds-consolidation/122/7",
      "category_id": 7
    },
    {
      "post_id": 205,
      "post_number": 1,
      "topic_id": 127,
      "topic_title": "Pruning point heuristic formula",
      "topic_slug": "pruning-point-heuristic-formula",
      "content": "This is not an actual research post, but rather a technical mathematical one for clarifying a point. I\u2019m using this platform mainly due to latex support. I might be using basic notation from the prunality proof. However note that pruning point calculation in the actual Kaspa implementation was modified a bit to be non-sliding, as described below. Definitions Denote P, F to be the pruning and finality depths respectively. Define the finality score of a block to be: FinalityScore(B) := \\lfloor BlueScore(B) / F \\rfloor The formula for calculating the pruning point of a block B is as follows: PruningPoint(B) := \\max C \\in Chain(B) \\text{ s.t. } BlueScore(B) - BlueScore(C) \\ge P AND FinalityScore(C) > FinalityScore(PruningPoint(SelectedParent(B))) In words: we only progress the pruning point of a block if we find a pruning point candidate which has larger finality score than the previous pruning point (i.e. the one we calculated for SelectedParent(B)). The idea is that the usage of finality score makes the pruning point move in steps of F. Heuristic Now, we want to define a heuristic for easily determining that the pruning point did not move, and to do so with out iterating a F\u2013length chain, at least in most cases. Claim: if FinalityScore(BlueScore(B) - P) \\le FinalityScore(PruningPoint(SelectedParent(B))) then PruningPoint(B) = PruningPoint(SelectedParent(B)) Proof: assume the contrary and denote \\pi the pruning point of B. By definition we know that BlueScore(\\pi) \\le BlueScore(B) - P. So FinalityScore(\\pi) \\le FinalityScore(BlueScore(B) - P), contradicting the requirement that FinalityScore progressed. (note: some abuse of notation here by using the block or the blue score of a block or a sum combination in calls to FinalityScore) However, the current codebase seems to have a wrong formula for this heuristic (see here), with the following condition: FinalityScore(BlueScore(B)) \\le FinalityScore(PruningPoint(SelectedParent(B))+P) This is not an equal definition due to the floor function in the definition of FinalityScore. The question for discussion is how safe is it to fix this condition w/o breaking consensus. It seems like there are theoretical cases where the wrong formula returns true while the correct formula returns false, meaning that the current code would skip searching for the new pruning point although it should have performed the search.",
      "raw_content": "",
      "author": "msutton",
      "created_at": "2022-03-03T11:34:35.180Z",
      "updated_at": "2022-03-03T12:19:46.968Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/pruning-point-heuristic-formula/127/1",
      "category_id": 10
    },
    {
      "post_id": 179,
      "post_number": 1,
      "topic_id": 117,
      "topic_title": "Updated survey of the data growth in kaspad",
      "topic_slug": "updated-survey-of-the-data-growth-in-kaspad",
      "content": "All and all we store three components: full header data above the pruning block, the UTXO set of the pruning block, and a proof of correctness for the UTXO set. We have a fancy pruning mechanism that allows us to remove old block data. At full capacity the size of a block payload is bound by 100kB and the size of a block header is bound by 100 +32\\cdot \\log_2(\\mbox{past size}) bytes. In the distant future where we have a trillion blocks in the network (this will take about 30 thousand years of one block per second) we will have that \\log_2(\\mbox{past size})\\le 40, so let us assume henceforth that \\log_2(\\mbox{past size})\\le 40 forever (being mindful that the following analysis is \u201conly\u201d good for the coming 30 thousand years). This means that the header size is bound by (100+32\\cdot40) bytes which is just below 1.5kB. We store three days worth of full block data which, at a rate of one block/second, accumulates to about 26GB (note that this bound assumes that all blocks are at maximum capacity, no assumptions on average number of txns per block). The UTXO correctness proof requires that we keep additional \\log_2(\\mbox{number of blocks in the network}) headers (not full blocks). Using again the assumption \\log_2(\\mbox{past size})\\le 40 this adds about 60kB of data, which is completely negligible. Currently we store all block headers, as it requires some care to remove them without accidentally removing headers required for the proof and our dev team hasn\u2019t got around to this yet, this is a completely technical issue which will be resolved in the near future. (There is another detail I swept under the rug, which is that we also have to store the headers of all pruning blocks. This means one header per day. While this technically grows at a rate O(n\\cdot\\log n) the constant is ridiculously small: it is bound by 1.5kB/day, which are about 570kB a year). The only thing that grows linearly is the pruning block UTXO set itself. It currently requires a field of a fixed size for every unspent output in the network. It is hard to predict how fast this set grows as this heavily depends on user behavior. We intend to resolve this in the future by means of cryptographic accumulators. An accumulator is a way to represent a large set succinctly such that it is impossible to recover the set itself (due to information theoretic compression bounds), but it is possible to verify that an element is in the set given a proof. This means that every user will only need to store the (proofs of) their own unspent outputs, and the nodes will only have to verify this proof against the accumulator, which is much smaller than the actual number of unspent outputs. The sizes of the accumulator and the proofs depends on the exact solution we will choose.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2021-12-01T08:33:45.860Z",
      "updated_at": "2021-12-01T08:40:17.020Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/updated-survey-of-the-data-growth-in-kaspad/117/1",
      "category_id": 1
    },
    {
      "post_id": 160,
      "post_number": 1,
      "topic_id": 98,
      "topic_title": "An atlas of the various constants and dependencies thereof",
      "topic_slug": "an-atlas-of-the-various-constants-and-dependencies-thereof",
      "content": "The purpose of this post is to list and describe some of the various constants which parametrize the system and the relations thereof, provide sources to more in depth discussion when available and fill the gaps when not. Some of the constants described below appear in the code, whereas the others are defined for the sake of theoretic discussion. In the former case I will state how the variable under discussion is used in the codebase. The maximal network delay d_\\max A bound on the time it takes for the entire network to learn of a block since it was broadcasted by its miner. The tolerance parameter \\delta A lot of the structural properties of the DAG are only guaranteed with high probability. Adjusting some parameters allows us to control that probability to make it so desired properties hold \u201cmost of the time\u201d, when we say \u201cmost of the time\u201d we actually mean 1-\\delta of the time. In the entire system the only such hard assumption we make is on the anticone size (cf. k below), and the rest is derived. Hence, \\delta designates a bound on the expected fraction of blocks whose anticone is larger than k. When discussing parameters which exist as variables in the codebase, I will state the name of the variable. All of the descriptions and properties above are assumed in the honest scenario unless stated otherwise. This does not mean we assume that the network is honest, only that some of the variables have meaningful interpretation in the honest settings. All security guarantees are proven in the honest majority setting. The block rate \\lambda Name in codebase: for implementation reasons, it is more convenient to use the reciprocal 1/\\lambda, which represents block delay rather than block rate. This appears in the code as defaultTargetTimePerBlock. Conceptual meaning: The expected number of blocks created in a period of time of length D is D/\\lambda. Operational meaning: the difficulty adjustment algorithm is parametrized to adjust the difficulty such that if H is the global hash rate then the expected number of hashes required to mine a block is H\\lambda. The parent cap C Name in codebase: defaultMaxBlockParents Operational meaning: a cap on the number of blocks a given block could point to. This parameter was introduced as a means to control the size of a block. An unlimited number of parents implies that an attacker could inflate the blocksize by posting many parallel blocks, thus causing the network delay of the block to exceed d_\\max, violating the assumptions on which the security of PHANTOM relies. The effective merge delay d_\\mbox{eff} The effective merge delay is a period of time d_\\mbox{eff} such that if block B' was created at least d_\\mbox{eff} after the block B was reported to the network then B\\in B'.Past with probability at least 1-\\delta. Dependence on other variables: If C\\ge \\lambda d_\\max then we have d_\\mbox{eff} = d_\\max. If C = \\alpha \\lambda d_\\max then, in the worst case in which there are \\lambda d_\\max tips it would take at least 1/\\alpha blocks to merge them, making d_\\mbox{eff} \\ge d_\\max + \\alpha/\\lambda. It is impossible to upper bound d_\\mbox{eff} without further assumptions. Generally speaking, if there are more than C tips it could be that one of them would be indefinitely starved. This could be mitigated in a variety of ways, either by refining the definition of d_\\mbox{eff} to allow starvation of some blocks, by incentivizing miners to point at slightly older blocks by giving (some or all) of the block rewards of red blocks to the merging block, by forcing miners to randomly choose the pointed tips etc\u2026 This is beyond the scope of this post. We assume for now that d_\\mbox{eff} \\le d_\\max + \\eta\\alpha/\\lambda, leaving the specification of \\eta to whoever specifies the chosen approach. The blue anticone size limit k Name in codebase: defaultGHOSTDAGK Conceptual meaning: most of the time, the anticone size should be less than k Operational meaning: Let B be a block, and let C\\in B.BlueSet, then |C.AntiCone \\cap B.BlueSet| \\le k Dependence on other constants: Once d_\\max, \\lambda and \\delta above are decided upon, k is chosen such that, in expectation, a fraction of at least 1-\\delta of the blocks admit anticones of size at most k. For d_\\max = 50, \\lambda = 1/sec and \\delta = 0.05 this evaluates to k=18. The calculation is based on the formula which appears in the PHANTOM paper, subsection 4.2. The analysis therein assumes d_\\mbox{eff} = d_\\max. If the system is parametrized such that the equality does not hold, d_\\mbox{eff} should be used rather than d_\\max. The merge set size limit L Name in codebase: defaultMergeSetSizeLimit Conceptual meaning: A limit of the number of blocks a new block can introduce to the state. Operational meaning: Any block B must satisfy that |B.Past \\setminus B.SelectedParent.Past|\\le L + 1 (the +1 is because the set in the LHS conains B.SelectedParent). The finality depth \\phi and pruning depth \\pi Name in codebase: \\phi is called defaultFinalityDuration, \\pi is not defined but rather calculated in the function PruningDepth(). Conceptual meaning of \\phi: represents a period of time after which the selected chain may not be changed. That is, if a block in the selected chain is old enough, it is promised that it is in the selected chain. Conceptual meaning of \\pi: represents a period of time such that blocks which were created in the anticone of a selected chain block older than this period of time will never affect the state of the UTXO set. Operational meaning of \\phi: \\phi is used to define the pruning block F as the newest block in Virtual.SelectedChain whose blue score is not larger than Virtual.BlueScore - \\phi. In the event where including a new block would cause the selected chain to reorganize such that it does not include the current F, we alert the community that the network is split. The split is then manually resolved. Operational meaning of \\pi: \\pi is used to define the pruning block P as the newest block in Virtual.SelectedChain whose blue score is not larger than Virtual.BlueScore - \\pi. The validation rules, thoroughly discussed here, are designed such that if at the time B was discovered it was in the anticone of P, it would never be in the past of the virtual block (almost certainly, barring a more-than-half attack). Dependence on each other and variables: \\phi is chosen to represent an arbitrary real time length deemed reasonable by external considerations, hence it does not strongly depend on other variables. The only form of weak dependence is that it must be orders of magnitude larger than the expected number of blocks during the effective convergence time, i.e. \\phi \\gg \\lambda d_\\mbox{eff}. The discussion in the linked post describes validation rules, relative to which it was formally proven that the pruning mechanism is secure given that \\pi = 2\\phi + 4Lk + 2k + 2. The dependence of \\pi on k also implies dependence on d_\\mbox{eff} and \\lambda, However, this dependence is accounted for in the calculation of k. The difficulty window size \\mathcal{N}, the timestamp deviation tolerance \\mathcal{F} Name in codebase: \\mathcal{N} is called defaultDifficultyAdjustmentWindowSize, \\mathcal{F} is called defaultTimestampDeviationTolerance. Conceptual meaning: \\mathcal{N} describes how deep into the past of a block we should look into when calculating its difficulty target. \\mathcal{F} describes how tolerant we are to timestamps deviating from the expected. The operational meaning is thoroughly discussed here. The only dependence on other variables lies in the fact that \\mathcal{F} relies on measuring a real time period (currently two minutes) by block count, which depends on \\lambda.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-29T10:55:43.572Z",
      "updated_at": "2021-11-30T12:40:54.769Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/an-atlas-of-the-various-constants-and-dependencies-thereof/98/1",
      "category_id": 1
    },
    {
      "post_id": 168,
      "post_number": 1,
      "topic_id": 106,
      "topic_title": "Visualization of DAGs",
      "topic_slug": "visualization-of-dags",
      "content": "This post describes some wishes I have regarding visualization features for DAG that could bring more intuition and understanding of DAG tadeoff to any new DAGer: A simulated visualizer that is configurable, allowing the user to play with the block rate, the block mass limit(s), the assumed delay in the network (more advanced: the topology of the mining network), etc., and view a DAG growing according to the specified configuration. A rich Rothchild (=Kaspa testnet txgen): The primary effect of the mass limit(s) is on whether the DAG is processing many regular small txns, or few txns with big payload. The latter is more interesting imo, as it is more in line with the current needs of the community (cf. Rollups), and because it keeps the requirement for full node more reasonable, as it only needs to process few (10, say) txns per second. Bottom line is: visualizing the different setups and capabilities requires implementing a txgen with rich features. As a next step, I\u2019d wish to be able to see what the resource consumption would be with such configurations \u2013 some rough estimations on the required CPU, bandwidth, disk space needed to run such a DAG in a real node. The estimation could originate from some extrapolation from several real data points (real performance of Kaspad under different configurations). A separate tool that allows for a step-by-step algorithm visualizer of various DAG protocols, e.g., longest chain. Inclusive-GHOST, SPECTRE, PHANTOM. Similar to https://rosulek.github.io/vamonos/demos/index.html A dashboard for the actual Kaspa (test/main) network, monitoring various metrices. Personally I\u2019m interested in (i) the end-to-end time-to-inclusion, namely, the time that passes from the user pressing \u201csend txn\u201d to the user observing the txn included inside some block in the DAG; and in visualizing (ii) the frequency of block creation by small miners, i.e., how frequent a miner with x% of the hashrate would mine a block, and the resulting time-to-inclusion for a user that sends a txn directly to the miner, without going through the mempool. More motivation on this later. The expected value of these items can be provided with over-the-envelope calculations, but we want to see that they admit to their expected value in the actual network. Other metrics are mentioned in this post. If you have more ideas pls comment below.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2021-02-24T12:16:30.512Z",
      "updated_at": "2021-02-24T12:30:22.348Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/visualization-of-dags/106/1",
      "category_id": 1
    },
    {
      "post_id": 85,
      "post_number": 1,
      "topic_id": 58,
      "topic_title": "Manipulating difficulty to spam the UTXO set is ineffective",
      "topic_slug": "manipulating-difficulty-to-spam-the-utxo-set-is-ineffective",
      "content": "The growth of the UTXO set poses a challenge to the design of any cryptocurrency protocol, as it bloats the state of the clients. An attacker might want to bloat this set by flooding the network with many cheap transactions in order to artificially increase the size of the state. In this post I am not addressing these issues directly, but merely trying to argue that a mining attacker could not abuse the difficulty adjustment protocol to increase the UTXO set. This is in response to the following attack (proposed by @ori and Elichai): create a side chain where you artificially decrease the difficulty, and then create a huge graph with many transactions for cheap. I argue that this attack does not actually allow a < 50% attacker to spam the UTXO set, if we add a simple consensus rule (which was conceived in an entirely different context, which serves my position that it is a natural and reasonable consensus rule, and not a ad-hoc hacky solution). The consensus rule is: For some constant maxDiff (which should be about k), reject any blocks which has two immediate parents whose blue score difference is more than maxDiff. (my \u201cphilosophical\u201d argument for this rule is that the PHANTOM DAG should be thought of as a generalization of Nakamoto chains. In this case, it makes sense to ask what replaces the maximal block in this generalization. The most easy answer is \u201cthe tips\u201d, but I argue that \u201cthe maximal tip, and other tips with similar blue score\u201d is better. The reason the top block in a Nakamoto chain is interesting is because it is supposed to contain the most current data. This could not be said about a week old block, even if it is technically a tip). Now, when calculating the UTXO set of the virtual block, it will simply not include tips with low scores, and their content will not affect the nodes UTXO set. Coming back to the attack above. In order for an attacker to decrease difficulty they must use time stamps which go into the future. While it is true that the attacker may create many blocks this way, they must also wait for the clock to catch up with the futuristic time stamps for them to be included, and by that time the network will have created more blocks. The difficulty adjustment is designed such that at any point in time (up to a small constant) the attacker can not create a lot of blocks which will be accepted. While it is true that the attacker can create many blocks this way, at any point in times, the amount of blocks they created which will be accepted by honest nodes is proportional to their hashrate. This (along with the consensus rule described above) implies that these blocks will not affect the nodes UTXO set.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-03-05T16:26:21.353Z",
      "updated_at": "2020-03-05T16:26:21.353Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/manipulating-difficulty-to-spam-the-utxo-set-is-ineffective/58/1",
      "category_id": 1
    },
    {
      "post_id": 159,
      "post_number": 2,
      "topic_id": 58,
      "topic_title": "Manipulating difficulty to spam the UTXO set is ineffective",
      "topic_slug": "manipulating-difficulty-to-spam-the-utxo-set-is-ineffective",
      "content": "Should be noted that this approach has not proven itself in the long run. tipDiff could be seen as a very early precursor to the more mature idea of a \u201ckosherizing block\u201d which is presented in the pruning algorithm.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-21T16:18:13.011Z",
      "updated_at": "2020-12-21T16:18:13.011Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/manipulating-difficulty-to-spam-the-utxo-set-is-ineffective/58/2",
      "category_id": 1
    },
    {
      "post_id": 158,
      "post_number": 1,
      "topic_id": 97,
      "topic_title": "Handling timestamp manipulations",
      "topic_slug": "handling-timestamp-manipulations",
      "content": "DAA relies on time stamps for retargeting, and this is a problem as timestamps are not reliable, both for adversarial reasons and due to natural clock drift across large networks. This poses an inherent tension: being too lax means giving more power to difficulty attacks, whereas being too strict would cause honest blocks to be dropped. There should be a mechanism for determining and enforcing a leeway for time signature deviation. I describe one here. The mechanism relies on the ability to decide, given two blocks, which one is \u201colder\u201d. This decision should not be based on time stamps as this leads to circular reasoning. From our point of view, we can assume that the design of the difficulty adjustment algorithm already contains a way to decide block chronology, and we only require that the DAA and the timestamp verification use the same way. (At the time this post was written, the selected metric was blue accumulated work, you can read about the reasoning behind this decision in this post.) Say we decide our deviation tolerance should be N\\lambda where \\lambda is the expected block delay. The mechanism acts differently for checking if a block is too much in the past, and for checking if a block is too much into the future. There is also a difference in how we chose to handle both this case. To verify that a block is not too much into the past, we check that its time signature is at least as late as the median timestamp of the last 2\\mathcal{F}-1 blocks in its past. If it is, then the block is considered invalid and is rejected indefinitely. The reason we can be confidant about rejecting the block is that the information required to avoid this situation is completely known to the miner of the block. To verify that a block is not too much into the future, we have to rely on the system clock. If the timestamp on the block is later than t + \\mathcal{F}\\lambda where t is the local clock, the block is dropped. In this case, we do not consider the block rejected, and we trust that if it is an honest block it would be transmitted again (another approach would be to hold on to the block, but delay its inclusion into the DAG until the condition is satisfied). The reason this is crucial is because we rely on system clock, which is not in consensus. In particular, if we invalidated blocks which are too much into the future, an adversary could use well timed blocks to split the network. Also, it would cause nodes with too much of a negative clock drift to reject honest blocks. How \\mathcal{F} should be chosen depends on the particular DAA algorithm. The current algorithm allows a simple bound on the factor by which timestamp manipulation could affect the difficulty adjustment. We quantify this with regards to the particular DAA algorithm at the time this post was written, which is described here and here. In this algorithm, we have a difficulty window size N, the correction factor to the difficulty is the ratio between the following two quantities: The expected time it should take to create N blocks, N\\lambda, and the approximate time it took to create the blocks in the window, which is the difference between the minimal and maximal timestamps in the window, call this quantity R. The adjustment factor is thus \\alpha = R/N\\lambda. The choice of \\mathcal{F} also relies on a bound e on the drift between clocks in the network (which could be a very good approximation to the assumption that clock delay distributes like \\mathcal{N}(0,e^2)). Assuming that in days of peace R\\approx N\\lambda, the worst an attacker can do is to push the least timestamp about \\lambda F into the past, and the latest timestamp exactly \\lambda F into the future, getting at R\\approx N\\lambda + 2\\lambda\\mathcal{F}=\\lambda(N+2\\mathcal{F}. In this case we get that \\alpha \\approx N/(N+2\\mathcal{F}) or \\frac{1}{\\alpha} \\approx 1 + 2\\frac{\\mathcal{F}}{N}. This means we can\u2019t make \\mathcal{F} irresponsibly large, e.g. if \\mathcal{F} = N/2 then an attacker can potentially make the difficulty half as easier than necessary by placing just two blocks. (Of course, this attack will only be as effective for a single block, and will have a declining effect for the next \\mathcal{F} blocks. If we want to limit timestamp attacks to a create a multiplicative error of at most (1+\\delta) we need to have 2\\frac{F}{N}\\le \\delta or 2\\mathcal{F}{\\delta}\\le N. In can be crudely stated that large \\frac{F}{N} protects us from synchronization problems at the expense of allowing some difficulty manipulation, while choosing small \\frac{F}{N} protects us from such manipulations while making the algorithm less responsive. It might be possible to ameliorate this lack of responsiveness by means other than decreasing N, such as choosing different averaging mechanisms. Let \\delta be the maximal timestamp manipulation we allow. Assuming that the DAA works properly then (during times in which the global hash rate does not change too frantically) the block delay will be at least \\lambda/(1+\\delta). If we want to prevent honest nodes from dropping honest blocks we should require that \\mathcal{F} \\ge (1+\\delta)e/\\lambda. In particular, if we allow \\delta=10\\%, and assume e=120 seconds and \\lambda = 1 seconds, we get that we need \\mathcal{F} \\ge (1+\\delta)e/\\lambda=132 and N\\ge 2\\mathcal{F}/\\delta = 2640.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-21T15:00:15.085Z",
      "updated_at": "2020-12-21T15:43:31.909Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/handling-timestamp-manipulations/97/1",
      "category_id": 1
    },
    {
      "post_id": 145,
      "post_number": 1,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "Q1: What quantity acts as a (rough) time measurement in a POW-powered DAG? A1: The number of blocks whose timestamps were considered by the difficulty adjustment algorithm (DAA), when adjusting the difficulty. Q2: Why is time measurement important? A2: It is (and has to be) used for regulating coin minting and for enforcing relative time-lock scripts. ====== A1 is quite tautological but still I want to point out why the blue score, or the pastSize (roughly the number of blocks in the DAG) do not act as such measurements. The problem with blueScore is that an attacker can withhold its blocks, publish them late so that they are not part of the blue set, and manipulate thus time measurement. This would lead to us underestimating the time that has elapsed. The problem with pastSize is that post a network split (which did not last longer than the pruning window) both branches will have already adjusted their block rates (i.e., mine with lower difficulty) which would lead to us overestimating the time that has elapsed, as pointed out by @Deshe. ====== Notes and reminders: Recall that Kaspa DAA uses a sliding window, in the line of algorithms developed and investigated by Zawy12 in https://github.com/zawy12/difficulty-algorithms. Roughly, the idea is to consider the last N blocks (in Kaspa: the N blocks with highest blue score in the DAG) \u2013 aka the DAA window \u2013 and use their average and min and max timestamps to adjust the blockrate, in a manner similar to that done in Bitcoin. Technically, A1 translates to maintaining, for each block, the number of blocks that are members of its merged set and that were used in the DAA (i.e., that were part of its DAA window). I emphasize \u201cblocks that are members of the merged set\u201d because, as @elichai2 points out, we select the highest blue score blocks in a block\u2019s past when calculating the DAA, and every block inherits the blue-red colouring of its selected parent, which in turn implies that a block will not consider in its DAA window blocks in its selected parent\u2019s past that were not already considered by its selected parent. While every block will have its own time measurement \u2013 i.e., its own count of the number of blocks that participated in the DAA windows throughout its past set \u2013 the dictating one is of course the block that won the consensus battlefield, i.e., the selected tip of the virtual.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2020-12-09T11:12:49.703Z",
      "updated_at": "2020-12-09T11:23:35.418Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/1",
      "category_id": 1
    },
    {
      "post_id": 146,
      "post_number": 2,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "I want to emphasize a big difference between the under-estimation in blue score vs. the over-estimation in past size. But before that some context: one of the reasons that we use a clock is to implement statements such as \u201cthe finality block should be a week old\u201d. Currently we use blue score to measure it. That is, we define that the difference in blue score between the finality block and the virtual is the expected increase in blue score during a week of operation. When we discuss a different metric of time, we also implicitly discuss the consequential changes in the implementation of choice of the finality block (and consequently, the pruning block). The under-estimation in the former case is bound by the computational power of the adversary, and in particular could be manipulated at most by a factor of two (that is, an almost 50% adversary can convince us that time passes almost twice as slow as it really does). On the other hand, the over-estimation in the latter case is unbounded*. In particular, if we use past size to measure time it is very cheap to convince us that time passed 1000 times faster than it actually does. Another point is the ramifications of the attack: by increasing our clock speed, an attacker could force us to choose finality (and consequentially, pruning) blocks which were made very late. Technically, they could make so that the finality block is the selected parent of the selected tip! On the other hand, I fail to see any dire consequence of decreasing our clock speed, especially not when it is increased by a factor of at most two, and at a very considerable PoW expanse. *That\u2019s not completely true due to the bounded merge rule. Recall that there is a parameter L which bounds the maximal merge set size of a valid block. This implies that the past size can not increase by more than L. The implications for the attack is that the attacker can\u2019t actually make our clock arbitrarily faster, but they can still increase it by a factor of almost L (a more exact expression is [L - average merge set size in the rest of the network], but since L >> k this is pretty much the same). So maybe an attacker can\u2019t force that the finality block is the selected parent of the selected tip, but for, say, L=1000, it can convince us that a block is a week old when it is actually ten minutes old.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-09T11:40:36.361Z",
      "updated_at": "2020-12-09T11:40:36.361Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/2",
      "category_id": 1
    },
    {
      "post_id": 147,
      "post_number": 3,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "Good point. Do you then suggest we modify the pruning/finality choices according to the time measurement proposed herein? BTW, a bad (though not dire) consequence of decreasing clock speed is the deviation from minting/inflation schedule.",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2020-12-09T17:35:25.452Z",
      "updated_at": "2020-12-09T17:35:25.452Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/3",
      "category_id": 1
    },
    {
      "post_id": 149,
      "post_number": 4,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "I always kind of assumed this is the case. When we outlined the finality/pruning we always used real world time terminology e.g. \u201cthe finality window is a week long\u201d. The way pruning/finality are implemented is under the implicit assumption that blue score is a viable approximation of time. If we decide that other metrics are more accurate and less attackable, then I think we should adjust pruning accordingly.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-10T11:13:24.713Z",
      "updated_at": "2020-12-10T11:13:24.713Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/4",
      "category_id": 1
    },
    {
      "post_id": 150,
      "post_number": 5,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "On a second thought, I am not sure if the implementation complexity doesn\u2019t outweigh the benefit. Are you bothered by the fact that a 33% attacker can make finality blocks 1.5 older than they should be? To an extent which justifies altering the finality/pruning block selection mechanism? I am not fluent enough in the engineering implications to make the call.",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-10T11:20:10.622Z",
      "updated_at": "2020-12-10T11:20:44.862Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/5",
      "category_id": 1
    },
    {
      "post_id": 151,
      "post_number": 6,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "AFAIR in finality/pruning blueScore is used for time approximation as a side effect, the main reason is for the pruning proof which heavily leans on k which is only true under blueScore. IMHO as long as blueScore gives us a lower bound on time then it\u2019s good enough for finality/pruning, but that\u2019s not true for the monetary policy and lock times",
      "raw_content": "",
      "author": "elichai2",
      "created_at": "2020-12-10T11:35:49.954Z",
      "updated_at": "2020-12-10T11:35:49.954Z",
      "reply_count": 2,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/6",
      "category_id": 1
    },
    {
      "post_id": 152,
      "post_number": 7,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "Lower bound is sufficient. If I understand correctly, an attacker can manipulate these points by a factor of 2 at most (assuming an attacker < 50%)",
      "raw_content": "",
      "author": "hashdag",
      "created_at": "2020-12-10T11:44:02.348Z",
      "updated_at": "2020-12-10T11:44:02.348Z",
      "reply_count": 1,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/7",
      "category_id": 1
    },
    {
      "post_id": 153,
      "post_number": 8,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "elichai2: AFAIR in finality/pruning blueScore is used for time approximation as a side effect, the main reason is for the pruning proof which heavily leans on k which is only true under blueScore. It is a bit more nuanced than that. The proof uses k in several different ways. Let w\u2019(B) be the \u201caccumulated difficulty window size\u201d of B. That is, the size of the union of difficulty windows of B and the blocks in its selected chain (equivalent recursive defn: w\u2019(B) = w\u2019(B.SelectedParent) + |B.DifficultyWindow / B.SelectedParent.Past|, w\u2019(Genesis) = 0). It is no longer true that w\u2019(B) - w\u2019(B.SelectedParent) <= k, but this is not a real issue because we can still say that w\u2019(B) - w\u2019(B.SelectedParent) <= diffWindowSize. The other assumption that we make, that there would not be a split below a certain depth, can still be stated in terms of k. The upshot, I think, is that the security proof should carry over but some of the k factors in the pruning block depth would have to be changed to diffWindowSize. @msutton what do you think?",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-10T11:54:08.410Z",
      "updated_at": "2020-12-10T11:54:08.410Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/8",
      "category_id": 1
    },
    {
      "post_id": 154,
      "post_number": 9,
      "topic_id": 93,
      "topic_title": "Difficulty adjustment and time measurement in DAGs",
      "topic_slug": "difficulty-adjustment-and-time-measurement-in-dags",
      "content": "hashdag: Lower bound is sufficient. If I understand correctly, an attacker can manipulate these points by a factor of 2 at most (assuming an attacker < 50%) I am fairly convinced that this is indeed the case. An alpha attacker can slow down the clock by a factor of at most (1 - alpha).",
      "raw_content": "",
      "author": "Deshe",
      "created_at": "2020-12-10T11:58:44.749Z",
      "updated_at": "2020-12-10T11:58:44.749Z",
      "reply_count": 0,
      "url": "https://research.kas.pa/t/difficulty-adjustment-and-time-measurement-in-dags/93/9",
      "category_id": 1
    }
  ],
  "metadata": {
    "forums_processed": 1,
    "total_posts_fetched": 128,
    "credential_status": "configured",
    "processing_mode": "topic_centric"
  }
}